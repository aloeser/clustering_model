{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is configured for TPCH (chunk size 65535) with scale factor 1, 60 seconds runtime, and at most 10 runs per query\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "\n",
    "BENCHMARK = \"TPCH\"\n",
    "CHUNK_SIZE = 65535\n",
    "\n",
    "if BENCHMARK == \"TPCH\":\n",
    "    SCALE_FACTOR = 1\n",
    "    RUNS = 10\n",
    "    TIME = 60\n",
    "    STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-H__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}__ENCODING_DictionaryFSBA\"\n",
    "    STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPCH-BASE\"\n",
    "elif BENCHMARK == \"TPCDS\":\n",
    "    SCALE_FACTOR = 1\n",
    "    RUNS = 1\n",
    "    TIME = 60\n",
    "    STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-DS__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}\"\n",
    "else:\n",
    "    raise Exception(\"Unknown benchmark: \" + BENCHMARK)\n",
    "\n",
    "print(f\"Model is configured for {BENCHMARK} (chunk size {CHUNK_SIZE}) with scale factor {SCALE_FACTOR}, {TIME} seconds runtime, and at most {RUNS} runs per query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ~/Dokumente/repos/example_plugin/TPCH-BASE/table_scans.csv\n"
     ]
    }
   ],
   "source": [
    "# Load table scan statistics\n",
    "\n",
    "path = f\"{STATISTICS_PATH}/table_scans.csv\"\n",
    "scans = pd.read_csv(path, sep='|')\n",
    "EXPECTED_SCAN_COUNT = len(scans)\n",
    "LOADED_CHUNK_SIZE = CHUNK_SIZE\n",
    "LOADED_BENCHMARK = BENCHMARK\n",
    "LOADED_SCALE_FACTOR = SCALE_FACTOR\n",
    "LOADED_RUNS = RUNS\n",
    "LOADED_TIME = TIME\n",
    "\n",
    "print(f\"Successfully loaded {path}\")\n",
    "\n",
    "def assert_correct_statistics_loaded():\n",
    "    assert BENCHMARK == LOADED_BENCHMARK, f\"The model is configured to use {BENCHMARK}, but {LOADED_BENCHMARK} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert SCALE_FACTOR == LOADED_SCALE_FACTOR, f\"The model is configured to use {SCALE_FACTOR} as scale factor, but data for a scale factor of {LOADED_SCALE_FACTOR} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert RUNS == LOADED_RUNS, f\"The model is configured to perform at most {RUNS} runs, but the currently loaded data had at most {LOADED_RUNS} runs.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert TIME == LOADED_TIME, f\"The model is configured to run for {TIME} seconds, but the currently data had a runtime of {LOADED_TIME} seconds.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert CHUNK_SIZE == LOADED_CHUNK_SIZE, f\"The model is configured to use {CHUNK_SIZE} as chunk_size, but data for a chunk size of {LOADED_CHUNK_SIZE} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert EXPECTED_SCAN_COUNT == len(scans), f\"There should be {EXPECTED_SCAN_COUNT} table scans, but there are only {len(scans)}\\nProbably one of the last commands reassigned it unintentionally\"\n",
    "    \n",
    "    assert 'GET_TABLE_HASH' in scans.columns, f\"the statistics in {STATISTICS_PATH} are outdated (column 'GET_TABLE_HASH' in table_scans.csv is missing). Please create them again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - looks like pruning was deactivated while the statistics were created\n"
     ]
    }
   ],
   "source": [
    "# Validate table scans\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "# To make sure pruning was not active,\n",
    "# first fetch table sizes,\n",
    "table_statistics = pd.read_csv(f\"{STATISTICS_PATH}/table_meta_data.csv\", sep='|')\n",
    "table_sizes = dict(zip(table_statistics.TABLE_NAME, table_statistics.ROW_COUNT))\n",
    "\n",
    "# then make sure INPUT_ROW_COUNT == table_size\n",
    "def input_size_matches(row):\n",
    "    #print(row)\n",
    "    \n",
    "    actual_row_count = row['INPUT_ROW_COUNT']\n",
    "    table = row['TABLE_NAME']\n",
    "    expected_row_count = table_sizes[table]\n",
    "    return expected_row_count == actual_row_count\n",
    "\n",
    "data_scans = scans[scans['COLUMN_TYPE'] == 'DATA']\n",
    "input_size_matches = data_scans.apply(input_size_matches, axis=1)\n",
    "all_sizes_match = reduce(np.logical_and, input_size_matches) #input_size_matches.apply()\n",
    "\n",
    "if not all_sizes_match:\n",
    "    #raise Exception(\"The given statistics were probably created while pruning was active\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"OK - looks like pruning was deactivated while the statistics were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for TPCH contain 436 table scans\n",
      "Of those, only 228 are useful for pruning\n",
      "TODO: For now, filtering on scans is deactivated. This is because all scans are needed to recognize OR-Chains. Models have to take care themselves whether a scan can contribute to pruning or not\n"
     ]
    }
   ],
   "source": [
    "# Append additional information to the table scans\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "print(f\"Statistics for {BENCHMARK} contain {len(scans)} table scans\")\n",
    "\n",
    "\n",
    "# Add statistics about selectivity and speed for each operator\n",
    "scans['selectivity'] = scans['OUTPUT_ROW_COUNT'] / scans['INPUT_ROW_COUNT']\n",
    "\n",
    "# TODO: Assumption that reading and writing a row have the same cost\n",
    "scans['time_per_row'] = scans['RUNTIME_NS'] / (scans['INPUT_ROW_COUNT'] + scans['OUTPUT_ROW_COUNT'])\n",
    "scans['time_per_input_row'] = scans['time_per_row']\n",
    "scans['time_per_output_row'] = scans['time_per_row']\n",
    "\n",
    "\n",
    "def determine_or_chains(table_scans):\n",
    "    table_scans['part_of_or_chain'] = False\n",
    "    \n",
    "    single_table_scans = table_scans.groupby(['QUERY_HASH', 'TABLE_NAME', 'GET_TABLE_HASH'])\n",
    "    \n",
    "    for _, scans in single_table_scans:\n",
    "        input_row_frequencies = Counter(scans.INPUT_ROW_COUNT)\n",
    "        or_input_sizes = set([input_size for input_size, frequency in input_row_frequencies.items() if frequency > 1])\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['INPUT_ROW_COUNT'] = scans['INPUT_ROW_COUNT']\n",
    "        df['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "        df['part_of_or_chain'] = scans.apply(lambda row: row['INPUT_ROW_COUNT'] in or_input_sizes, axis=1)\n",
    "\n",
    "        for _ in range(len(scans)):\n",
    "            or_input_sizes |= set(df[df['part_of_or_chain']].OUTPUT_ROW_COUNT.unique())\n",
    "            df['part_of_or_chain'] = df.apply(lambda row: row['INPUT_ROW_COUNT'] in or_input_sizes, axis=1)\n",
    "\n",
    "        or_chains = list(df[df['part_of_or_chain']].index)\n",
    "        #table_scans.iloc[or_chains, table_scans.columns.get_loc('part_of_or_chain')] = True\n",
    "        table_scans.loc[or_chains, 'part_of_or_chain'] = True\n",
    "    \n",
    "    return table_scans\n",
    "\n",
    "# Hyrise does not use scans that are part of an OR-chain for pruning\n",
    "scans = determine_or_chains(scans)\n",
    "\n",
    "\n",
    "# Like scans are not useful if they start with %\n",
    "# TODO what if they dont start with % and contain more than one % ? -> up to first % prunable, but is it used?\n",
    "def benefits_from_sorting(row):    \n",
    "    description = row['DESCRIPTION']\n",
    "    if \"ColumnLike\" in description:\n",
    "        words = description.split('LIKE')\n",
    "        assert len(words) == 2, f\"expected exactly one occurence of LIKE, but got {description}\"\n",
    "        like_criteria = words[1]\n",
    "        assert \"%\" in like_criteria or \"_\" in like_criteria, f\"LIKE operators should have an % or _, but found none in {like_criteria}\"\n",
    "        first_char = like_criteria[2]\n",
    "        assert first_char != ' ' and first_char != \"'\", \"Like check considers the wrong token\"\n",
    "        return first_char != '%' and first_char != '_'\n",
    "    elif \"ExpressionEvaluator\" in description and \" IN \" in description:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "scans['benefits_from_sorting'] = scans.apply(benefits_from_sorting, axis=1)\n",
    "# TODO: valid atm, but feels a bit hacky to assume not benefitting from sorted segments -> not benefitting from pruning\n",
    "scans['useful_for_pruning'] = scans.apply(lambda row: not row['part_of_or_chain'] and row['benefits_from_sorting'] , axis=1)\n",
    "EXPECTED_SCAN_COUNT = len(scans)\n",
    "print(f\"Of those, only {len(scans[scans['useful_for_pruning']])} are useful for pruning\")\n",
    "\n",
    "print(\"TODO: For now, filtering on scans is deactivated. This is because all scans are needed to recognize OR-Chains. Models have to take care themselves whether a scan can contribute to pruning or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test OK\n"
     ]
    }
   ],
   "source": [
    "def test_determine_or_chains():\n",
    "    test = pd.DataFrame()\n",
    "    test['QUERY_HASH'] = pd.Series(['1']*3  + ['2']*4)\n",
    "    test['TABLE_NAME'] = pd.Series(['lineitem']*3  + ['part']*4)\n",
    "    test['GET_TABLE_HASH'] = pd.Series(['0x1'] + ['0x2']*2 + ['0x3']*4)\n",
    "    test['COLUMN_NAME'] = pd.Series(['l_shipdate', 'l_shipdate', 'l_discount', 'p_brand', 'p_type', 'p_type', 'p_size'])\n",
    "    test['INPUT_ROW_COUNT'] = pd.Series( [6001215, 6001215, 200000, 200000, 199000, 199000, 50000])\n",
    "    test['OUTPUT_ROW_COUNT'] = pd.Series([ 400000,  300000, 200000, 199000,      0,  50000, 20000])\n",
    "    test_result = determine_or_chains(test)\n",
    "    assert len(test_result) == 7, \"should not filter out any rows\"    \n",
    "    assert len(test_result[test_result['part_of_or_chain']]) == 3, \"expected 3 scans, got\\n\" + str(test_result)\n",
    "    assert list(test_result['part_of_or_chain']) == [False]*4 + [True]*3\n",
    "    print(\"Test OK\")\n",
    "\n",
    "test_determine_or_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query frequency information\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def get_query_frequencies():\n",
    "    plan_cache = pd.read_csv(f\"{STATISTICS_PATH}/plan_cache.csv\", sep='|')\n",
    "    return dict(zip(plan_cache.QUERY_HASH, plan_cache.EXECUTION_COUNT))\n",
    "\n",
    "#get_query_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load column statistics - especially interesting: number of distinct values, and columns sorted during statistics creation\n",
    "\n",
    "# Returns a 2-level-dictionary: distinct_values[TABLE][COLUMN] = number_of_distinct_values\n",
    "def get_distinct_values_count():        \n",
    "    # Code\n",
    "    column_statistics_df = pd.read_csv(f\"{STATISTICS_PATH}/column_meta_data.csv\", sep='|')\n",
    "    column_statistics_df['DISTINCT_VALUES'] = np.int32(column_statistics_df['DISTINCT_VALUES'])\n",
    "    tables_and_columns = column_statistics_df.groupby('TABLE_NAME')\n",
    "    distinct_values = {table: dict(zip(column_df.COLUMN_NAME, column_df.DISTINCT_VALUES)) for table, column_df in tables_and_columns }\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    num_tables = len(distinct_values)\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        assert num_tables == 8, f\"TPCH has 8 tables, but got {num_tables}\"\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        assert num_tables == 24, f\"TPCDS has 24 tables, but got {num_tables}\"\n",
    "    else:\n",
    "        assert False, \"Insert a benchmark specific check here\"\n",
    "    \n",
    "    return distinct_values\n",
    "\n",
    "# Returns a dictionary: sorted_columns_during_creation[TABLE] = [column1, column2, ...]\n",
    "def get_sorted_columns_during_creation():\n",
    "    # Code\n",
    "    column_statistics_df = pd.read_csv(f\"{STATISTICS_PATH}/column_meta_data.csv\", sep='|')\n",
    "    globally_sorted_columns = column_statistics_df[column_statistics_df['IS_GLOBALLY_SORTED'] == 1]\n",
    "    \n",
    "    tables_and_columns = globally_sorted_columns.groupby('TABLE_NAME')\n",
    "    globally_sorted_columns = {table: list(column_df.COLUMN_NAME) for table, column_df in tables_and_columns }\n",
    "    \n",
    "    return globally_sorted_columns\n",
    "\n",
    "#get_distinct_values_count()\n",
    "#get_sorted_columns_during_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOIN_MODE</th>\n",
       "      <th>LEFT_TABLE_NAME</th>\n",
       "      <th>LEFT_COLUMN_NAME</th>\n",
       "      <th>LEFT_TABLE_ROW_COUNT</th>\n",
       "      <th>RIGHT_TABLE_NAME</th>\n",
       "      <th>RIGHT_COLUMN_NAME</th>\n",
       "      <th>RIGHT_TABLE_ROW_COUNT</th>\n",
       "      <th>OUTPUT_ROW_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Semi</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>5741</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_partkey</td>\n",
       "      <td>192</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Semi</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>800000</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29935</td>\n",
       "      <td>119740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>AntiNullAsTrue</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_suppkey</td>\n",
       "      <td>119740</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>4</td>\n",
       "      <td>119707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Inner</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>119707</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29935</td>\n",
       "      <td>119707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Semi</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>800000</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29437</td>\n",
       "      <td>117748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>AntiNullAsTrue</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_suppkey</td>\n",
       "      <td>117748</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>4</td>\n",
       "      <td>117697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Inner</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>117697</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29437</td>\n",
       "      <td>117697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Inner</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_suppkey</td>\n",
       "      <td>1</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Inner</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_suppkey</td>\n",
       "      <td>1</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Inner</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_suppkey</td>\n",
       "      <td>1</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         JOIN_MODE LEFT_TABLE_NAME LEFT_COLUMN_NAME  LEFT_TABLE_ROW_COUNT  \\\n",
       "50            Semi            part        p_partkey                  5741   \n",
       "51            Semi        partsupp       ps_partkey                800000   \n",
       "52  AntiNullAsTrue        partsupp       ps_suppkey                119740   \n",
       "53           Inner        partsupp       ps_partkey                119707   \n",
       "54            Semi        partsupp       ps_partkey                800000   \n",
       "55  AntiNullAsTrue        partsupp       ps_suppkey                117748   \n",
       "56           Inner        partsupp       ps_partkey                117697   \n",
       "58           Inner        lineitem        l_suppkey                     1   \n",
       "60           Inner        lineitem        l_suppkey                     1   \n",
       "62           Inner        lineitem        l_suppkey                     1   \n",
       "\n",
       "   RIGHT_TABLE_NAME RIGHT_COLUMN_NAME  RIGHT_TABLE_ROW_COUNT  OUTPUT_ROW_COUNT  \n",
       "50         lineitem         l_partkey                    192               489  \n",
       "51             part         p_partkey                  29935            119740  \n",
       "52         supplier         s_suppkey                      4            119707  \n",
       "53             part         p_partkey                  29935            119707  \n",
       "54             part         p_partkey                  29437            117748  \n",
       "55         supplier         s_suppkey                      4            117697  \n",
       "56             part         p_partkey                  29437            117697  \n",
       "58         supplier         s_suppkey                  10000                 1  \n",
       "60         supplier         s_suppkey                  10000                 1  \n",
       "62         supplier         s_suppkey                  10000                 1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### JOINS ###\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def load_join_statistics():\n",
    "    def line_looks_suspicious(row):\n",
    "        right_table_name = row['RIGHT_TABLE_NAME']    \n",
    "        if pd.isnull(right_table_name):\n",
    "            pass\n",
    "        elif row['RIGHT_TABLE_ROW_COUNT'] > table_sizes[row['RIGHT_TABLE_NAME']]:\n",
    "            return True\n",
    "\n",
    "        left_table_name = row['LEFT_TABLE_NAME']\n",
    "        if pd.isnull(left_table_name):\n",
    "            pass\n",
    "        elif row['LEFT_TABLE_ROW_COUNT'] > table_sizes[row['LEFT_TABLE_NAME']]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def validate_joins(joins):\n",
    "        is_suspicious = joins.apply(line_looks_suspicious, axis=1)\n",
    "        suspicious_joins = joins[is_suspicious]\n",
    "        assert len(suspicious_joins) < 3, f\"there are {len(suspicious_joins)} suspicious joins:\\n{suspicious_joins[['JOIN_MODE', 'LEFT_TABLE_NAME', 'LEFT_COLUMN_NAME', 'LEFT_TABLE_ROW_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_COLUMN_NAME', 'RIGHT_TABLE_ROW_COUNT', 'OUTPUT_ROW_COUNT']]}\"\n",
    "    \n",
    "    joins = pd.read_csv(f\"{STATISTICS_PATH}/joins.csv\", sep='|')\n",
    "    joins = joins.dropna()\n",
    "    joins['PROBE_TABLE'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_NAME\"] if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['PROBE_COLUMN'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_COLUMN_NAME\"] if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['PROBE_TABLE_ROW_COUNT'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_ROW_COUNT\"]if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\" , axis=1)\n",
    "    joins['BUILD_TABLE'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_NAME\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['BUILD_COLUMN'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_COLUMN_NAME\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['BUILD_TABLE_ROW_COUNT'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_ROW_COUNT\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    validate_joins(joins)\n",
    "    \n",
    "    \n",
    "    # TODOS\n",
    "    \n",
    "    # code: export which side is probe\n",
    "    # model: add columns for probe and build table size\n",
    "    # model: use more precise values for build and probe materialization\n",
    "        \n",
    "    \n",
    "    \n",
    "    #TODO code: activate higher cache size    \n",
    "                                                                                           \n",
    "    return joins\n",
    "\n",
    "load_join_statistics().iloc[50:60][['JOIN_MODE', 'LEFT_TABLE_NAME', 'LEFT_COLUMN_NAME', 'LEFT_TABLE_ROW_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_COLUMN_NAME', 'RIGHT_TABLE_ROW_COUNT', 'OUTPUT_ROW_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract interesting table names\n",
    "# Currently fixed to columns which are scanned or used in joins\n",
    "\n",
    "def get_table_names(table_scans, joins):\n",
    "    scan_tables = set(table_scans['TABLE_NAME'].unique())\n",
    "    left_join_tables = set(joins['LEFT_TABLE_NAME'].unique())\n",
    "    right_join_tables = set(joins['RIGHT_TABLE_NAME'].unique())    \n",
    "\n",
    "    return scan_tables.union(left_join_tables.union(right_join_tables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel:\n",
    "    \n",
    "    def __init__(self, query_frequencies, table_name, table_scans, correlations={}):\n",
    "        self.query_frequencies = query_frequencies\n",
    "        self.table_name = table_name\n",
    "        self.table_scans = table_scans\n",
    "        self.correlations = correlations\n",
    "        \n",
    "    def query_frequency(self, query_hash):\n",
    "        return self.query_frequencies[query_hash]\n",
    "        \n",
    "    def extract_scan_columns(self):\n",
    "        useful_scans = self.table_scans[self.table_scans['useful_for_pruning']]\n",
    "        interesting_scan_columns = list(useful_scans['COLUMN_NAME'].unique())\n",
    "        \n",
    "        return interesting_scan_columns\n",
    "    \n",
    "    def extract_join_columns(self):\n",
    "        interesting_join_probe_columns = list(self.joins[self.joins['PROBE_TABLE'] == self.table_name]['PROBE_COLUMN'].unique())\n",
    "        interesting_join_build_columns = list(self.joins[self.joins['BUILD_TABLE'] == self.table_name]['BUILD_COLUMN'].unique())        \n",
    "        \n",
    "        return self.uniquify(interesting_join_probe_columns + interesting_join_build_columns)\n",
    "    \n",
    "    def extract_interesting_columns(self):        \n",
    "        return self.uniquify(self.extract_scan_columns() + self.extract_join_columns())\n",
    "    \n",
    "    def round_up_to_next_multiple(self, number_to_round, base_for_multiple):\n",
    "        quotient = number_to_round // base_for_multiple\n",
    "        if number_to_round % base_for_multiple != 0:\n",
    "            quotient += 1\n",
    "        return quotient * base_for_multiple        \n",
    "\n",
    "    def uniquify(self, seq):\n",
    "            seen = set()\n",
    "            return [x for x in seq if not (x in seen or seen.add(x))]    \n",
    "    \n",
    "    # return a list of possible clusterings\n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old partitioner model.\n",
    "# TODO: refactor to inherit from SingleTableMDCModel (further below). Not sure how much effort should go into this model, since it is not designed for the \"main clustering algorithm\"\n",
    "\n",
    "class PartitionerModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, query_frequencies, table_name, table_scans, table_size, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(query_frequencies, table_name, table_scans, correlations)\n",
    "        self.table_size = table_size\n",
    "        self.distinct_values = distinct_values\n",
    "        self.target_chunksize = target_chunksize\n",
    "        self.joins = joins\n",
    "        self.sorted_columns_during_creation = sorted_columns_during_creation\n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        print(interesting_columns)\n",
    "        \n",
    "        clustering_columns = itertools.product(interesting_columns, interesting_columns)\n",
    "        #clustering_columns = itertools.product(interesting_columns, interesting_columns, interesting_columns)\n",
    "        clustering_columns = filter(lambda x: x[0] <= x[1], clustering_columns)\n",
    "        #clustering_columns = filter(lambda x: x[1] <= x[2], clustering_columns)\n",
    "        clustering_columns = [self.uniquify(clustering) for clustering in clustering_columns]\n",
    "        sort_columns = interesting_columns        \n",
    "        clusterings_with_runtimes = reduce(lambda x,y: x+y,[self.estimate_total_runtime(clustering_cols, sort_columns) for clustering_cols in clustering_columns])\n",
    "        clusterings_with_runtimes.sort(key=lambda x: x[2], reverse=False)\n",
    "        \n",
    "        return clusterings_with_runtimes[0:first_k]\n",
    "        \n",
    "    def estimate_table_scan_runtimes(self, clustering_columns, sorting_columns, split_factors, total_runtimes):        \n",
    "        def compute_unprunable_parts(row, split_factors):\n",
    "            def clustering_columns_correlated_to(column):\n",
    "                return [clustering_column for clustering_column in clustering_columns if column in self.correlations.get(clustering_column, {})]\n",
    "            \n",
    "            def correlates_to_clustering_column(column):\n",
    "                return len(clustering_columns_correlated_to(column)) > 0\n",
    "\n",
    "            column_name = row['COLUMN_NAME']\n",
    "\n",
    "            if not row['useful_for_pruning']:\n",
    "                selectivity = 1\n",
    "            elif column_name in clustering_columns:\n",
    "                scan_selectivity = row['selectivity']\n",
    "                split_factor = split_factors[clustering_columns.index(column_name)]\n",
    "                selectivity =  self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor)\n",
    "            elif correlates_to_clustering_column(column_name):\n",
    "                scan_selectivity = row['selectivity']\n",
    "                correlated_clustering_columns = clustering_columns_correlated_to(column_name)\n",
    "                \n",
    "                # ToDo this is hacky, but for now assume there is just one correlated column\n",
    "                assert len(correlated_clustering_columns) == 1, f\"expected just 1 correlated clustering column, but got {len(correlated_clustering_columns)}\"\n",
    "                \n",
    "                split_factor = split_factors[clustering_columns.index(correlated_clustering_columns[0])]\n",
    "                selectivity = min(1, 1.2 * self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor))\n",
    "            else:\n",
    "                selectivity = 1\n",
    "            \n",
    "            return selectivity\n",
    "        \n",
    "        def compute_runtimes(row, sorting_column):\n",
    "            assert row['estimated_input_rows'] > 1, row\n",
    "            assert row['runtime_per_input_row'] > 0, row\n",
    "            assert row['runtime_per_output_row'] > 0, row\n",
    "            input_row_count = row['estimated_input_rows']\n",
    "            \n",
    "            if row['COLUMN_NAME'] == sorting_column and row['benefits_from_sorting']:\n",
    "                # TODO is this the best way to simulate sorted access?\n",
    "                input_row_count = np.log2(input_row_count)\n",
    "\n",
    "            runtime = input_row_count * row['runtime_per_input_row'] + row['OUTPUT_ROW_COUNT'] * row['runtime_per_output_row']\n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        scans_per_query = self.table_scans.sort_values(['INPUT_ROW_COUNT'], ascending=False).groupby(['QUERY_HASH', 'GET_TABLE_HASH'])\n",
    "        for _, scans in scans_per_query:\n",
    "            number_of_scans = len(scans)\n",
    "            assert number_of_scans > 0 and number_of_scans < 25, f\"weird scan length: {number_of_scans}\\nScans:\\n{scans}\"\n",
    "            # TODO: kinda unrealistic assumption: everything not in the table scan result can be pruned\n",
    "                          \n",
    "            unprunable_parts = scans.apply(compute_unprunable_parts, axis=1, args=(split_factors,))\n",
    "            unprunable_part = unprunable_parts.product()\n",
    "            assert unprunable_part > 0, \"no unprunable part\"\n",
    "            \n",
    "            estimated_pruned_table_size = self.round_up_to_next_multiple(unprunable_part * self.table_size, CHUNK_SIZE)\n",
    "            \n",
    "            runtimes = pd.DataFrame()\n",
    "            runtimes['QUERY_HASH'] = scans['QUERY_HASH']\n",
    "            runtimes['runtime_per_input_row'] = scans['time_per_input_row']\n",
    "            runtimes['runtime_per_output_row'] = scans['time_per_output_row']\n",
    "            runtimes['COLUMN_NAME'] = scans['COLUMN_NAME']\n",
    "            runtimes['benefits_from_sorting'] = scans['benefits_from_sorting']\n",
    "            # the pruned table inputs should be reflected in 'estimated_input_rows'\n",
    "            runtimes['estimated_input_rows'] = scans.apply(lambda x: x['INPUT_ROW_COUNT'], axis=1)\n",
    "            runtimes['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "            runtimes.iloc[0, runtimes.columns.get_loc('estimated_input_rows')] = estimated_pruned_table_size                                    \n",
    "            assert runtimes['estimated_input_rows'].iloc[0] == estimated_pruned_table_size, f\"value is {runtimes.iloc[0]['estimated_input_rows']}, but should be {estimated_pruned_table_size}\"\n",
    "            # TODO modify input sizes of subsequent scans\n",
    "            \n",
    "            for sorting_column in sorting_columns:\n",
    "                scan_runtimes = runtimes.apply(compute_runtimes, axis=1, args=(sorting_column,))\n",
    "                total_runtimes[sorting_column] += scan_runtimes.sum()\n",
    "\n",
    "    def estimate_join_runtimes(self, clustering_columns, sorting_columns, total_runtimes):                \n",
    "        def estimate_join_runtime(row, sorting_column):\n",
    "                        \n",
    "            if \"JoinHash\" in row['DESCRIPTION']:\n",
    "                probe_column = row['PROBE_COLUMN']\n",
    "                if row['PROBE_TABLE'] == self.table_name:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    probe_column_is_sorted = row['PROBE_SORTED'] and probe_column == sorting_column\n",
    "                    probe_column_is_clustered = row['PROBE_SORTED'] and probe_column in clustering_columns\n",
    "                else:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(row['PROBE_TABLE'], {})\n",
    "                    probe_column_is_sorted = probe_column_was_sorted\n",
    "                    probe_column_is_clustered = probe_column_was_sorted\n",
    "                    \n",
    "                build_column = row['BUILD_COLUMN']\n",
    "                if row['BUILD_TABLE'] == self.table_name:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    build_column_is_sorted = row['BUILD_SORTED'] and build_column == sorting_column\n",
    "                    build_column_is_clustered = row['BUILD_SORTED'] and build_column in clustering_columns\n",
    "                else:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(row['BUILD_TABLE'], {})\n",
    "                    build_column_is_sorted = build_column_was_sorted\n",
    "                    build_column_is_clustered = build_column_was_sorted\n",
    "\n",
    "                time_materialize = row['BUILD_SIDE_MATERIALIZING_NS'] + row['PROBE_SIDE_MATERIALIZING_NS']\n",
    "                \n",
    "                probe_weight = 2\n",
    "                build_weight = 2\n",
    "                if probe_column_was_sorted:\n",
    "                    probe_weight = 1\n",
    "                if build_column_was_sorted:\n",
    "                    build_weight = 1\n",
    "                \n",
    "                \n",
    "                probe_table_size = row['PROBE_TABLE_ROW_COUNT']\n",
    "                build_table_size = row['BUILD_TABLE_ROW_COUNT']\n",
    "                total_table_size = probe_weight * probe_table_size + build_weight * build_table_size\n",
    "                \n",
    "                time_materialize_probe = time_materialize * (probe_weight * probe_table_size / total_table_size)\n",
    "                time_materialize_build = time_materialize - time_materialize_probe\n",
    "                \n",
    "                \n",
    "                def get_materialize_factor(was_sorted, is_sorted, is_clustered):\n",
    "                    materialize_factor = 1\n",
    "                    if is_sorted and is_clustered:\n",
    "                        if not was_sorted:\n",
    "                            materialize_factor = 0.5\n",
    "                        else:\n",
    "                            materialize_factor = 1\n",
    "                    elif is_sorted or is_clustered:\n",
    "                        if not was_sorted:\n",
    "                            materialize_factor = 0.55\n",
    "                        else:\n",
    "                            materialize_factor = 1.1\n",
    "                    elif was_sorted:\n",
    "                        # probe column is now neither sorted nor clustered\n",
    "                        materialize_factor = 2\n",
    "                    else:\n",
    "                        # default case: was not sorted before, and is neither sorted nor clustered now. No change\n",
    "                        materialize_factor = 1\n",
    "                        \n",
    "                    return materialize_factor\n",
    "                \n",
    "                materialize_probe_factor = get_materialize_factor(probe_column_was_sorted, probe_column_is_sorted, probe_column_is_clustered)\n",
    "                materialize_build_factor = get_materialize_factor(build_column_was_sorted, build_column_is_sorted, build_column_is_clustered)\n",
    "                \n",
    "                time_materialize = time_materialize_probe * materialize_probe_factor + time_materialize_build *  materialize_build_factor\n",
    "                \n",
    "\n",
    "                # unchanged\n",
    "                time_cluster = row['CLUSTERING_NS']\n",
    "                \n",
    "                # unchanged\n",
    "                time_build = row['BUILDING_NS']\n",
    "                \n",
    "                            \n",
    "                time_probe = row['PROBING_NS']\n",
    "                probe_factor = 1\n",
    "                if probe_column_is_sorted and probe_column_is_clustered:\n",
    "                    if not probe_column_was_sorted:\n",
    "                        probe_factor = 0.7\n",
    "                    else:\n",
    "                        probe_factor = 1\n",
    "                elif probe_column_is_sorted or probe_column_is_clustered:\n",
    "                    if not probe_column_was_sorted:\n",
    "                        probe_factor = 0.9\n",
    "                    else:\n",
    "                        probe_factor = 1.1\n",
    "                elif probe_column_was_sorted:\n",
    "                    # probe column is now neither sorted nor clustered\n",
    "                    probe_factor = 1.4\n",
    "                \n",
    "                time_probe *= probe_factor                \n",
    "                \n",
    "                # unchanged\n",
    "                time_write_output = row['OUTPUT_WRITING_NS']\n",
    "                \n",
    "                \n",
    "                \n",
    "                # TODO: how to deal with the difference between RUNTIME_NS and sum(stage_runtimes)?\n",
    "                runtime = time_materialize + time_cluster + time_build + time_probe + time_write_output\n",
    "            else:\n",
    "                runtime = row['RUNTIME_NS']\n",
    "                \n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        for sorting_column in sorting_columns:\n",
    "            join_runtimes = self.joins.apply(estimate_join_runtime, axis=1, args=(sorting_column,))\n",
    "            total_runtimes[sorting_column] += join_runtimes.sum()\n",
    "                \n",
    "    def estimate_total_runtime(self, clustering_columns, sorting_columns):\n",
    "        #print(f\"testing clustering {clustering_columns} with sorting columns {sorting_columns}\")\n",
    "        split_factors = self.determine_split_factors(clustering_columns)            \n",
    "        total_runtimes = {sorting_column: 0 for sorting_column in sorting_columns}\n",
    "        self.estimate_table_scan_runtimes(clustering_columns, sorting_columns, split_factors, total_runtimes)\n",
    "        self.estimate_join_runtimes(clustering_columns, sorting_columns, total_runtimes)\n",
    "        \n",
    "        clusterings = [[list(zip(clustering_columns, split_factors)), sorting_column, np.int64(total_runtimes[sorting_column])] for sorting_column in sorting_columns]\n",
    "        return clusterings\n",
    "    \n",
    "    def determine_split_factors(self, clustering_columns):\n",
    "        approximate_split_factor = self.table_size / self.target_chunksize\n",
    "        individual_distinct_values = [self.distinct_values[column] for column in clustering_columns]        \n",
    "        log_distinct_values = [math.ceil(0.5+np.log2(x)) for x in individual_distinct_values]\n",
    "        log_distinct_values_product = reduce(operator.mul, log_distinct_values, 1)\n",
    "        assert log_distinct_values_product > 0, \"cannot have a distinct value count of 0\"\n",
    "        \n",
    "        global_modification_factor = approximate_split_factor / log_distinct_values_product\n",
    "        num_dimensions = len(clustering_columns)\n",
    "        individual_modification_factor = np.power(global_modification_factor, 1.0 / num_dimensions)    \n",
    "        split_factors = [math.ceil(x * individual_modification_factor) for x in log_distinct_values]\n",
    "        \n",
    "        # testing\n",
    "        actual_split_factor = reduce(operator.mul, split_factors, 1)\n",
    "        assert actual_split_factor > 0, \"there was a split up factor of 0\"\n",
    "        estimated_chunksize = self.table_size / actual_split_factor\n",
    "        assert estimated_chunksize <= self.target_chunksize, \"chunks should be smaller, not larger than target_chunksize\"\n",
    "        allowed_percentage = 0.55\n",
    "        if estimated_chunksize < allowed_percentage * self.target_chunksize:\n",
    "            print(f\"Warning: chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\")\n",
    "        #assert estimated_chunksize >= allowed_percentage * self.target_chunksize, f\"chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\"\n",
    "        \n",
    "        return split_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def extract_probe_side_joins(joins, table_name):\n",
    "    return joins[joins['PROBE_TABLE'] == table_name]\n",
    "\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 92 * SCALE_FACTOR]],\n",
    "            'orders': [['o_orderdate', 23 * SCALE_FACTOR]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "def create_benchmark_configs():\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    clusterings = {\"default\" : default_benchmark_config()}\n",
    "    query_frequencies = get_query_frequencies()\n",
    "    \n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    for table_name in table_names:\n",
    "        start_time_table = datetime.now()\n",
    "        single_table_scans = extract_single_table(scans, table_name)\n",
    "        probe_side_joins = joins#extract_probe_side_joins(joins, table_name)\n",
    "        table_size = table_sizes[table_name]\n",
    "        if table_size <= 3 * CHUNK_SIZE:\n",
    "            print(f\"Not computing clustering for {table_name}, as it has only {table_size} rows\")\n",
    "            continue\n",
    "\n",
    "        model = PartitionerModel(query_frequencies, table_name, single_table_scans, table_size, distinct_values[table_name], CHUNK_SIZE, correlations.get(table_name, {}), probe_side_joins, sorted_columns_during_creation)\n",
    "        table_clusterings = model.suggest_clustering(3)\n",
    "        for table_clustering in table_clusterings:\n",
    "            config = default_benchmark_config()\n",
    "            config[table_name] = format_table_clustering(table_clustering)\n",
    "            config_name = get_config_name(config)\n",
    "            clusterings[config_name] = config\n",
    "        end_time_table = datetime.now()\n",
    "        print(f\"Done computing clustering for {table_name} ({end_time_table - start_time_table})\")\n",
    "\n",
    "            \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Computed all clusterings in {end_time - start_time}\")\n",
    "    \n",
    "    return clusterings\n",
    "\n",
    "#create_benchmark_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTableMdcModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(query_frequencies, table_name, table_scans, correlations)\n",
    "        self.max_dimensions = max_dimensions\n",
    "        self.table_sizes = table_sizes       \n",
    "        self.table_size = table_sizes[table_name]\n",
    "        self.distinct_values = distinct_values\n",
    "        self.target_chunksize = target_chunksize\n",
    "        self.joins = joins\n",
    "        self.sorted_columns_during_creation = sorted_columns_during_creation\n",
    "        \n",
    "        \n",
    "        self.join_column_names = self.extract_join_columns()\n",
    "        self.scan_column_names = self.extract_scan_columns()\n",
    "        self.scan_estimates = pd.DataFrame()\n",
    "        self.scan_estimates['QUERY_HASH'] = self.table_scans['QUERY_HASH']        \n",
    "        self.scan_estimates['DESCRIPTION'] = self.table_scans['DESCRIPTION']\n",
    "        self.scan_estimates['RUNTIME_ESTIMATE'] = np.array([-1] * len(self.table_scans))\n",
    "        self.scan_estimates['RUNTIME_NS'] = self.table_scans['RUNTIME_NS'] # TODO: probably not necessary?\n",
    "        self.scan_estimates['time_per_input_row'] = self.table_scans['time_per_input_row']\n",
    "        #self.scan_estimates.index = self.table_scans.index\n",
    "\n",
    "        \n",
    "    def is_join_column(self, column_name):\n",
    "        return column_name in self.join_column_names\n",
    "    \n",
    "    def is_scan_column(self, column_name):\n",
    "        return column_name in self.scan_column_names\n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        print(interesting_columns)\n",
    "        clustering_columns = itertools.combinations_with_replacement(interesting_columns, self.max_dimensions)\n",
    "        clustering_columns = [self.uniquify(clustering) for clustering in clustering_columns]\n",
    "        sort_columns = interesting_columns        \n",
    "        clusterings_with_runtimes = reduce(lambda x,y: x+y,[self.estimate_total_runtimes(clustering_cols, sort_columns) for clustering_cols in clustering_columns])\n",
    "        clusterings_with_runtimes.sort(key=lambda x: x[2], reverse=False)\n",
    "        \n",
    "        return clusterings_with_runtimes[0:first_k]\n",
    "    \n",
    "    \n",
    "    def estimate_distinct_values_per_chunk(self, column, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        raise NotImplementedError(\"Each model should provide this function\")\n",
    "        \n",
    "    def estimate_distinct_values_per_chunk_at_statistics_time(self, column, table):        \n",
    "        if column in self.sorted_columns_during_creation.get(table, {}):\n",
    "            # Column was globally sorted\n",
    "            average_count_per_distinct_value = self.table_sizes[table] / self.distinct_values[table][column]\n",
    "            return math.ceil(self.target_chunksize / average_count_per_distinct_value)\n",
    "        else:\n",
    "            # Column was not globally sorted\n",
    "            total_distinct_values = self.distinct_values[table][column]\n",
    "            return min(total_distinct_values, self.target_chunksize)        \n",
    "    \n",
    "    def compute_unprunable_parts(self, row, clustering_columns, split_factors):\n",
    "        def clustering_columns_correlated_to(column):\n",
    "            return [clustering_column for clustering_column in clustering_columns if column in self.correlations.get(clustering_column, {})]\n",
    "\n",
    "        def correlates_to_clustering_column(column):\n",
    "            return len(clustering_columns_correlated_to(column)) > 0\n",
    "\n",
    "        column_name = row['COLUMN_NAME']\n",
    "\n",
    "        if not row['useful_for_pruning']:\n",
    "            selectivity = 1\n",
    "        elif column_name in clustering_columns:\n",
    "            scan_selectivity = row['selectivity']\n",
    "            split_factor = split_factors[clustering_columns.index(column_name)]\n",
    "            selectivity =  self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor)\n",
    "        elif correlates_to_clustering_column(column_name):\n",
    "            scan_selectivity = row['selectivity']\n",
    "            correlated_clustering_columns = clustering_columns_correlated_to(column_name)\n",
    "\n",
    "            # ToDo this is hacky, but for now assume there is just one correlated column\n",
    "            assert len(correlated_clustering_columns) == 1, f\"expected just 1 correlated clustering column, but got {len(correlated_clustering_columns)}\"\n",
    "\n",
    "            split_factor = split_factors[clustering_columns.index(correlated_clustering_columns[0])]\n",
    "            selectivity = min(1, 1.2 * self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor))\n",
    "        else:\n",
    "            selectivity = 1\n",
    "\n",
    "        return selectivity\n",
    "    \n",
    "    def estimate_table_scan_runtime(self, clustering_columns, sorting_column, split_factors):                \n",
    "        def compute_tablescan_runtime(row, sorting_column):\n",
    "            assert row['estimated_input_rows'] > 1, row\n",
    "            assert row['runtime_per_input_row'] > 0, row\n",
    "            assert row['runtime_per_output_row'] > 0, row\n",
    "            input_row_count = row['estimated_input_rows']\n",
    "            \n",
    "            if row['COLUMN_NAME'] == sorting_column and row['benefits_from_sorting'] and not row['COLUMN_NAME'] in self.sorted_columns_during_creation.get(self.table_name, {}):\n",
    "                # TODO is this the best way to simulate sorted access?\n",
    "                input_row_count = np.log2(input_row_count)\n",
    "\n",
    "            runtime = input_row_count * row['runtime_per_input_row'] + row['OUTPUT_ROW_COUNT'] * row['runtime_per_output_row']\n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        original_clustering_column = self.sorted_columns_during_creation[self.table_name][0]\n",
    "        \n",
    "        \n",
    "        runtime = 0\n",
    "        scans_per_query = self.table_scans.sort_values(['INPUT_ROW_COUNT'], ascending=False).groupby(['QUERY_HASH', 'GET_TABLE_HASH'])\n",
    "        for _, scans in scans_per_query:\n",
    "            number_of_scans = len(scans)\n",
    "            assert number_of_scans > 0 and number_of_scans < 25, f\"weird scan length: {number_of_scans}\\nScans:\\n{scans}\"\n",
    "            # TODO: kinda unrealistic assumption: everything not in the table scan result can be pruned            \n",
    "\n",
    "            unprunable_parts = scans.apply(self.compute_unprunable_parts, axis=1, args=(clustering_columns, split_factors,))\n",
    "            unprunable_part = unprunable_parts.product()\n",
    "            assert unprunable_part > 0, \"no unprunable part\"\n",
    "            \n",
    "            estimated_pruned_table_size = min(self.table_size, self.round_up_to_next_multiple(unprunable_part * self.table_size, CHUNK_SIZE))\n",
    "            \n",
    "            runtimes = pd.DataFrame()\n",
    "            runtimes['QUERY_HASH'] = scans['QUERY_HASH']\n",
    "            runtimes['runtime_per_input_row'] = scans['time_per_input_row']\n",
    "            runtimes['runtime_per_output_row'] = scans['time_per_output_row']\n",
    "            runtimes['COLUMN_NAME'] = scans['COLUMN_NAME']\n",
    "            runtimes['benefits_from_sorting'] = scans['benefits_from_sorting']\n",
    "            # the pruned table inputs should be reflected in 'estimated_input_rows'\n",
    "            runtimes['estimated_input_rows'] = scans['INPUT_ROW_COUNT']\n",
    "            runtimes['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "            runtimes.iloc[0, runtimes.columns.get_loc('estimated_input_rows')] = estimated_pruned_table_size\n",
    "            assert runtimes['estimated_input_rows'].iloc[0] == estimated_pruned_table_size, f\"value is {runtimes.iloc[0]['estimated_input_rows']}, but should be {estimated_pruned_table_size}\"\n",
    "            # TODO modify input sizes of subsequent scans\n",
    "            \n",
    "            scan_runtimes = runtimes.apply(compute_tablescan_runtime, axis=1, args=(sorting_column,))\n",
    "            #self.table_scans.iloc[:, self.table_scans.columns.get_loc('RUNTIME_ESTIMATE')] = scan_runtimes\n",
    "            self.scan_estimates.loc[scan_runtimes.index, 'RUNTIME_ESTIMATE'] = scan_runtimes\n",
    "            runtime += scan_runtimes.sum()\n",
    "        return runtime\n",
    "\n",
    "    def estimate_chunk_count(self, scans, clustering_columns, dimension_cardinalities):\n",
    "        raise NotImplementedError(\"Subclass responsibility\")\n",
    "    \n",
    "    def get_chunk_count_factor(self, row, side, clustering_columns, dimension_cardinalities):\n",
    "        query_hash = row['QUERY_HASH']\n",
    "        if side == \"PROBE\":\n",
    "            input_rows = row['PROBE_TABLE_ROW_COUNT']\n",
    "            assert row['PROBE_TABLE'] == self.table_name, \"Call this function only for the own table\"\n",
    "            \n",
    "            # When joining tables, the table size might increase (a lot). This makes it hard to estimate the chunk count, so just ignore it\n",
    "            if input_rows > self.table_size:\n",
    "                return 1\n",
    "        elif side == \"BUILD\":\n",
    "            input_rows = row['BUILD_TABLE_ROW_COUNT']\n",
    "            assert row['BUILD_TABLE'] == self.table_name, \"Call this function only for the own table\"\n",
    "            # When joining tables, the table size might increase (a lot). This makes it hard to estimate the chunk count, so just ignore it\n",
    "            if input_rows > self.table_size:\n",
    "                return 1\n",
    "        else:\n",
    "            raise ValueError(\"side must be PROBE or BUILD\")\n",
    "\n",
    "        expected_chunk_count = self.estimate_chunk_count(query_hash, input_rows, clustering_columns, dimension_cardinalities)\n",
    "        min_chunk_count = math.ceil(input_rows / self.target_chunksize)\n",
    "        max_chunk_count = math.ceil(self.table_size / self.target_chunksize)\n",
    "\n",
    "        CHUNK_COUNT_SPEEDUP_LOW = 3\n",
    "        CHUNK_COUNT_SPEEDUP_HIGH = 1\n",
    "\n",
    "        current_speedup = self.interpolate(CHUNK_COUNT_SPEEDUP_LOW, CHUNK_COUNT_SPEEDUP_HIGH, (expected_chunk_count - min_chunk_count) / max_chunk_count)\n",
    "        #print(f\"current speedup: {current_speedup}\")\n",
    "\n",
    "        old_clustering_columns = self.sorted_columns_during_creation[self.table_name]\n",
    "        old_dimension_cardinalities = [self.statistic_time_dimension_cardinalities()] * len(old_clustering_columns)\n",
    "        old_expected_chunk_count = self.estimate_chunk_count(query_hash, input_rows, old_clustering_columns, old_dimension_cardinalities)\n",
    "        old_speedup = self.interpolate(CHUNK_COUNT_SPEEDUP_LOW, CHUNK_COUNT_SPEEDUP_HIGH, (old_expected_chunk_count - min_chunk_count) / max_chunk_count)\n",
    "        #print(f\"old speedup: {old_speedup}\")\n",
    "\n",
    "        return 1 * old_speedup / current_speedup\n",
    "    \n",
    "    def estimate_join_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        def compute_join_runtime(row, sorting_column):\n",
    "            if \"JoinHash\" in row['DESCRIPTION']:\n",
    "                probe_column = row['PROBE_COLUMN']\n",
    "                if row['PROBE_TABLE'] == self.table_name:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    probe_column_is_sorted = row['PROBE_SORTED'] and probe_column == sorting_column\n",
    "                    materialize_probe_factor = self.get_chunk_count_factor(row, \"PROBE\", clustering_columns, dimension_cardinalities)\n",
    "                else:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(row['PROBE_TABLE'], {})\n",
    "                    probe_column_is_sorted = probe_column_was_sorted\n",
    "                    materialize_probe_factor = 1\n",
    "                    \n",
    "                build_column = row['BUILD_COLUMN']\n",
    "                if row['BUILD_TABLE'] == self.table_name:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    build_column_is_sorted = row['BUILD_SORTED'] and build_column == sorting_column\n",
    "                    materialize_build_factor = self.get_chunk_count_factor(row, \"BUILD\", clustering_columns, dimension_cardinalities)\n",
    "                else:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(row['BUILD_TABLE'], {})\n",
    "                    build_column_is_sorted = build_column_was_sorted\n",
    "                    materialize_build_factor = 1\n",
    "\n",
    "                time_materialize_probe = row['PROBE_SIDE_MATERIALIZING_NS']\n",
    "                time_materialize_build = row['BUILD_SIDE_MATERIALIZING_NS']                \n",
    "                \n",
    "                def get_materialize_factor(column, was_globally_sorted, is_sorted, expected_distinct_value_count):\n",
    "                    # Assumption: \"was_sorted\" implies global sortedness, i.e., both clustering and chunkwise sorting\n",
    "                    # This is true when the clustering produced by the table generator is used by the plan cache exporter\n",
    "                    # If the data has been re-clustered before the plan cache exporter runs, there has to be some system inside Hyrise which tracks the current clustering config\n",
    "                    \n",
    "                    # Sortedness seems to yield a speed up of approx. 1.6, regardless of the number of distinct values\n",
    "                    SORT_SPEEDUP = 1.6\n",
    "                    sortedness_factor = 1                    \n",
    "                    was_sorted = was_globally_sorted\n",
    "                    if was_sorted:\n",
    "                        sortedness_factor *= SORT_SPEEDUP\n",
    "                    if is_sorted:\n",
    "                        sortedness_factor /= SORT_SPEEDUP\n",
    "                    \n",
    "\n",
    "                    # The influence of clustering depends on the number of distinct values\n",
    "                    CLUSTERING_SPEEDUP_LOW = 1.84\n",
    "                    CLUSTERING_SPEEDUP_HIGH = 1\n",
    "                    clustering_factor = 1\n",
    "                    statistics_time_distinct_value_count = self.estimate_distinct_values_per_chunk_at_statistics_time(column, self.table_name);\n",
    "                    clustering_factor *= self.interpolate(CLUSTERING_SPEEDUP_LOW, CLUSTERING_SPEEDUP_HIGH, statistics_time_distinct_value_count / self.target_chunksize)\n",
    "                    clustering_factor /= self.interpolate(CLUSTERING_SPEEDUP_LOW, CLUSTERING_SPEEDUP_HIGH, expected_distinct_value_count / self.target_chunksize)\n",
    "                        \n",
    "                    return sortedness_factor * clustering_factor                \n",
    "                \n",
    "                if row['PROBE_TABLE'] == self.table_name and row['PROBE_SORTED']:\n",
    "                    expected_distinct_values_probe = self.estimate_distinct_values_per_chunk(probe_column, clustering_columns, sorting_column, dimension_cardinalities)\n",
    "                    materialize_probe_factor *= get_materialize_factor(probe_column, probe_column_was_sorted, probe_column_is_sorted, expected_distinct_values_probe)\n",
    "                    \n",
    "                if row['BUILD_TABLE'] == self.table_name and row['BUILD_SORTED']:\n",
    "                    expected_distinct_values_build = self.estimate_distinct_values_per_chunk(build_column, clustering_columns, sorting_column, dimension_cardinalities)\n",
    "                    materialize_build_factor *= get_materialize_factor(build_column, build_column_was_sorted, build_column_is_sorted, expected_distinct_values_build)                \n",
    "                \n",
    "                time_materialize = time_materialize_probe * materialize_probe_factor + time_materialize_build *  materialize_build_factor\n",
    "\n",
    "\n",
    "                # unchanged\n",
    "                time_cluster = row['CLUSTERING_NS']\n",
    "                \n",
    "                # unchanged\n",
    "                time_build = row['BUILDING_NS']\n",
    "                \n",
    "                            \n",
    "                time_probe = row['PROBING_NS']\n",
    "                probe_factor = 1\n",
    "                #if probe_column_is_sorted and probe_column_is_clustered:\n",
    "                #    if not probe_column_was_sorted:\n",
    "                #        probe_factor = 0.7\n",
    "                #    else:\n",
    "                #        probe_factor = 1\n",
    "                #elif probe_column_is_sorted or probe_column_is_clustered:\n",
    "                #    if not probe_column_was_sorted:\n",
    "                #        probe_factor = 0.9\n",
    "                #    else:\n",
    "                #        probe_factor = 1.1\n",
    "                #elif probe_column_was_sorted:\n",
    "                #    # probe column is now neither sorted nor clustered\n",
    "                #    probe_factor = 1.4\n",
    "                \n",
    "                time_probe *= probe_factor                \n",
    "                \n",
    "                # unchanged\n",
    "                time_write_output = row['OUTPUT_WRITING_NS']\n",
    "                \n",
    "                \n",
    "                \n",
    "                # TODO: how to deal with the difference between RUNTIME_NS and sum(stage_runtimes)?\n",
    "                runtime = time_materialize + time_cluster + time_build + time_probe + time_write_output\n",
    "            else:\n",
    "                runtime = row['RUNTIME_NS']\n",
    "                \n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        join_runtimes = self.joins.apply(compute_join_runtime, axis=1, args=(sorting_column,))\n",
    "        return join_runtimes.sum()\n",
    "                \n",
    "    def estimate_total_runtimes(self, clustering_columns, sorting_columns):\n",
    "        #print(f\"testing clustering {clustering_columns} with sorting columns {sorting_columns}\")\n",
    "        dimension_cardinalities = self.get_dimension_cardinalities(clustering_columns)\n",
    "        total_runtimes = {sorting_column: 0 for sorting_column in sorting_columns}\n",
    "        clusterings = []\n",
    "        for sorting_column in sorting_columns:\n",
    "            runtime = self.estimate_total_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "            clusterings.append([list(zip(clustering_columns, dimension_cardinalities)), sorting_column, runtime])\n",
    "        \n",
    "        #clusterings = [[list(zip(clustering_columns, dimension_cardinalities)), sorting_column, np.int64(total_runtimes[sorting_column])] for sorting_column in sorting_columns]\n",
    "        return clusterings\n",
    "    \n",
    "    def estimate_total_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        runtime = 0\n",
    "        runtime += self.estimate_table_scan_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "        runtime += self.estimate_join_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "        \n",
    "        return runtime\n",
    "    \n",
    "    def get_dimension_cardinalities(self, clustering_columns):\n",
    "        raise NotImplementedError(\"Subclasses must override this function\")\n",
    "        \n",
    "    def statistic_time_dimension_cardinalities(self):\n",
    "        raise NotImplementedError(\"Subclasses must override this function\")\n",
    "        \n",
    "    def interpolate(self, low, high, percentage):\n",
    "        assert percentage >= 0 and percentage <= 1, f\"percentage must between 0 and 1, but is {percentage}\"\n",
    "        return (1 - percentage) * low + (percentage * high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisjointClustersModel(SingleTableMdcModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation)\n",
    "        \n",
    "    # This function should only be called for the own table, not for others\n",
    "    def estimate_distinct_values_per_chunk(self, column, clustering_columns, chunk_sorting_column, dimension_cardinalities):\n",
    "        total_distinct_values = self.distinct_values[self.table_name][column]\n",
    "        \n",
    "        if column in clustering_columns:\n",
    "            index = clustering_columns.index(column)\n",
    "            clusters_for_column = dimension_cardinalities[index]\n",
    "            return min(self.target_chunksize, total_distinct_values / clusters_for_column)\n",
    "        else:\n",
    "            # TODO cluster wise sorting\n",
    "            return min(self.target_chunksize, total_distinct_values)\n",
    "        \n",
    "    # This function should only be called for the own table, not for others\n",
    "    def estimate_chunk_count(self, query_hash, join_input_rows, clustering_columns, dimension_cardinalities):        \n",
    "        # TODO include or exclude scans that do not benefit from pruning? Excluded for now        \n",
    "        table_scans = self.table_scans[self.table_scans['QUERY_HASH'] == query_hash]        \n",
    "        table_scans = table_scans[table_scans['useful_for_pruning']]\n",
    "        \n",
    "        if len(table_scans) > 0:\n",
    "            #print(f\"For query hash {query_hash}, there are {len(table_scans)} scans on {self.table_name}\")\n",
    "            def get_denseness_factor(row):\n",
    "                column = row['COLUMN_NAME']\n",
    "                if column in clustering_columns:\n",
    "                    # TODO more precise estimate\n",
    "                    denseness_factor = 1\n",
    "                else:\n",
    "                    denseness_factor = row['selectivity']\n",
    "\n",
    "                return denseness_factor\n",
    "\n",
    "            denseness_factors = table_scans.apply(get_denseness_factor, axis=1)\n",
    "            denseness_factor = denseness_factors.product()\n",
    "            #print(f\"denseness factor is {denseness_factor}\")\n",
    "        else:\n",
    "            denseness_factor = 1        \n",
    "        \n",
    "        chunk_count = math.ceil(join_input_rows / (self.target_chunksize * denseness_factor))\n",
    "        max_chunks = math.ceil(self.table_size / self.target_chunksize)\n",
    "        #if chunk_count > max_chunks:\n",
    "        #    print(f\"WARNING: estimated {chunk_count} chunks, but {self.table_name} got only {max_chunks}\\nDenseness: {denseness_factor}\")\n",
    "        \n",
    "        \n",
    "        return min(chunk_count, max_chunks)\n",
    "    \n",
    "    def statistic_time_dimension_cardinalities(self):\n",
    "        return math.ceil(self.table_size / self.target_chunksize)\n",
    "            \n",
    "    def get_dimension_cardinalities(self, clustering_columns):\n",
    "        # ToDo what if we aim at less than number of chunks clusters, i.e. multiple chunks per cluster?\n",
    "        target_cluster_count = math.ceil(1.1 * self.table_size / self.target_chunksize)\n",
    "        # idea: fixed size for join columns, variable amount for scan columns\n",
    "        \n",
    "        join_columns = list(filter(lambda x: self.is_join_column(x), clustering_columns))\n",
    "        scan_columns = list(filter(lambda x: self.is_scan_column(x), clustering_columns))\n",
    "        intersecting_columns = set(join_columns).intersection(set(scan_columns))\n",
    "        assert len(intersecting_columns) == 0, f\"The following columns are used as both join and scan column: {intersecting_columns}\"\n",
    "        \n",
    "        if len(scan_columns) == 0:\n",
    "            CLUSTERS_PER_JOIN_COLUMN = math.ceil(math.pow(target_cluster_count, 1/len(join_columns)))\n",
    "        else: \n",
    "            CLUSTERS_PER_JOIN_COLUMN = 3;\n",
    "        # Assumption: uniform distribution (in the sense that every cluster actually exists)\n",
    "        num_join_clusters = math.pow(CLUSTERS_PER_JOIN_COLUMN, len(join_columns))\n",
    "        assert num_join_clusters <= 2 * target_cluster_count, f\"Would get {num_join_clusters} clusters for join columns, but aimed at at most {target_cluster_count} clusters\"\n",
    "    \n",
    "        # only applies to scan columns\n",
    "        desired_scan_clusters_count = math.ceil(target_cluster_count / num_join_clusters)\n",
    "        individual_distinct_values = [self.distinct_values[self.table_name][column] for column in scan_columns]\n",
    "        log_distinct_values = [math.ceil(0.5+np.log2(x)) for x in individual_distinct_values]\n",
    "        log_distinct_values_product = reduce(operator.mul, log_distinct_values, 1)\n",
    "        assert log_distinct_values_product > 0, \"cannot have a distinct value count of 0\"\n",
    "\n",
    "        global_modification_factor = desired_scan_clusters_count / log_distinct_values_product\n",
    "        num_scan_dimensions = len(scan_columns)\n",
    "        individual_modification_factor = np.power(global_modification_factor, 1.0 / max(1, num_scan_dimensions))\n",
    "        \n",
    "        join_column_cluster_counts = [CLUSTERS_PER_JOIN_COLUMN] * len(join_columns)\n",
    "        scan_column_cluster_counts = [math.ceil(x * individual_modification_factor) for x in log_distinct_values]\n",
    "        \n",
    "        \n",
    "        # Merge join and scan columns\n",
    "        join_index = 0\n",
    "        scan_index = 0\n",
    "        cluster_counts = []\n",
    "        for clustering_column in clustering_columns:\n",
    "            if clustering_column in join_columns:\n",
    "                cluster_counts.append(join_column_cluster_counts[join_index])\n",
    "                join_index += 1\n",
    "            elif clustering_column in scan_columns:\n",
    "                cluster_counts.append(scan_column_cluster_counts[scan_index])\n",
    "                scan_index += 1\n",
    "        assert join_index == len(join_columns), f\"Processed the wrong number of join columns: {join_index} instead of {len(join_column_cluster_counts)}\"\n",
    "        assert scan_index == len(scan_columns), f\"Processed the wrong number of scan columns: {scan_index} instead of {len(scan_column_cluster_counts)}\"\n",
    "        assert len(cluster_counts) == len(clustering_columns), f\"Expected {len(clustering_columns)} cluster counts, but got {len(cluster_counts)}\"\n",
    "        \n",
    "        # testing\n",
    "        actual_cluster_count = reduce(operator.mul, cluster_counts, 1)\n",
    "        assert actual_cluster_count > 0, \"there was a split up factor of 0\"\n",
    "        assert actual_cluster_count <= 2 * target_cluster_count, f\"Wanted at most {target_cluster_count} clusters, but got {actual_cluster_count}\\nConfig: {clustering_columns}\\nCluster sizes: {cluster_counts}\"\n",
    "        estimated_chunksize = self.table_size / actual_cluster_count\n",
    "\n",
    "        assert estimated_chunksize <= self.target_chunksize, f\"chunks should be smaller, not larger than target_chunksize. Estimated chunk size is {estimated_chunksize}\"\n",
    "        allowed_percentage = 0.55\n",
    "        if estimated_chunksize < allowed_percentage * self.target_chunksize:\n",
    "            print(f\"Warning: chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\")\n",
    "        #assert estimated_chunksize >= allowed_percentage * self.target_chunksize, f\"chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\"\n",
    "        \n",
    "        return cluster_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l_receiptdate', 'l_shipdate', 'l_quantity', 'l_discount', 'l_partkey', 'l_suppkey', 'l_orderkey']\n",
      "Execution time: 0:07:04.770711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[('l_orderkey', 101)], 'l_orderkey', 27161151387.71346],\n",
       " [[('l_partkey', 101)], 'l_orderkey', 27367222280.32986],\n",
       " [[('l_orderkey', 101)], 'l_partkey', 27448430733.391533],\n",
       " [[('l_partkey', 11), ('l_suppkey', 11)], 'l_orderkey', 27537095104.021976],\n",
       " [[('l_partkey', 11), ('l_orderkey', 11)], 'l_orderkey', 27541506644.4774],\n",
       " [[('l_orderkey', 101)], 'l_shipdate', 27762147281.09371],\n",
       " [[('l_shipdate', 18), ('l_discount', 6)], 'l_orderkey', 27903350769.79986],\n",
       " [[('l_shipdate', 101)], 'l_orderkey', 27908526217.40806],\n",
       " [[('l_shipdate', 34), ('l_suppkey', 3)], 'l_orderkey', 27917327212.01194],\n",
       " [[('l_shipdate', 34), ('l_partkey', 3)], 'l_orderkey', 27920618273.537914],\n",
       " [[('l_shipdate', 34), ('l_orderkey', 3)], 'l_orderkey', 27920618273.537914],\n",
       " [[('l_shipdate', 14), ('l_quantity', 8)], 'l_orderkey', 27921596258.80122],\n",
       " [[('l_receiptdate', 11), ('l_shipdate', 11)],\n",
       "  'l_orderkey',\n",
       "  27924182749.003033],\n",
       " [[('l_receiptdate', 18), ('l_discount', 6)],\n",
       "  'l_orderkey',\n",
       "  27967642252.054817],\n",
       " [[('l_receiptdate', 101)], 'l_orderkey', 27977529740.669933],\n",
       " [[('l_receiptdate', 34), ('l_suppkey', 3)], 'l_orderkey', 27985108080.91984],\n",
       " [[('l_receiptdate', 14), ('l_quantity', 8)],\n",
       "  'l_orderkey',\n",
       "  27986298442.890854],\n",
       " [[('l_receiptdate', 34), ('l_partkey', 3)], 'l_orderkey', 27988399142.445816],\n",
       " [[('l_receiptdate', 34), ('l_orderkey', 3)],\n",
       "  'l_orderkey',\n",
       "  27988399142.445816],\n",
       " [[('l_orderkey', 101)], 'l_receiptdate', 28222802037.73813]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo delete\n",
    "start_time = datetime.now()\n",
    "clusterings = model.suggest_clustering(20)\n",
    "end_time = datetime.now()\n",
    "print(f\"Execution time: {end_time - start_time}\")\n",
    "clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QUERY_HASH</th>\n",
       "      <th>DESCRIPTION1</th>\n",
       "      <th>RUNTIME_BASE</th>\n",
       "      <th>RUNTIME_ESTIMATE</th>\n",
       "      <th>RUNTIME_CLUSTERED</th>\n",
       "      <th>rti</th>\n",
       "      <th>TOTAL_ERROR</th>\n",
       "      <th>RELATIVE_ERROR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129f61e2f3ecf0ef</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12028701</td>\n",
       "      <td>444624</td>\n",
       "      <td>458607</td>\n",
       "      <td>1.930294</td>\n",
       "      <td>13983</td>\n",
       "      <td>1.031449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1a48bd3a39e20462</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>37786567</td>\n",
       "      <td>18797941</td>\n",
       "      <td>11216317</td>\n",
       "      <td>3.164142</td>\n",
       "      <td>-7581624</td>\n",
       "      <td>0.596678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239bb106372cd9ad</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11934549</td>\n",
       "      <td>149966</td>\n",
       "      <td>176640</td>\n",
       "      <td>1.963705</td>\n",
       "      <td>26674</td>\n",
       "      <td>1.177867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33d198b26945d669</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11613677</td>\n",
       "      <td>417356</td>\n",
       "      <td>438003</td>\n",
       "      <td>1.865681</td>\n",
       "      <td>20647</td>\n",
       "      <td>1.049471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3898d8ff5bfa1046</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>52674334</td>\n",
       "      <td>26254001</td>\n",
       "      <td>287319586</td>\n",
       "      <td>4.402514</td>\n",
       "      <td>261065585</td>\n",
       "      <td>10.943840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>394be8faae0702a1</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>24426745</td>\n",
       "      <td>3227037</td>\n",
       "      <td>2083079</td>\n",
       "      <td>3.532581</td>\n",
       "      <td>-1143958</td>\n",
       "      <td>0.645508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45337615a2f78d4e</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18327812</td>\n",
       "      <td>2410330</td>\n",
       "      <td>339800</td>\n",
       "      <td>2.652385</td>\n",
       "      <td>-2070530</td>\n",
       "      <td>0.140977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4582f099547f7754</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12092679</td>\n",
       "      <td>447065</td>\n",
       "      <td>476348</td>\n",
       "      <td>1.940548</td>\n",
       "      <td>29283</td>\n",
       "      <td>1.065501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>497648c7e308ba08</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18513811</td>\n",
       "      <td>2449303</td>\n",
       "      <td>272887</td>\n",
       "      <td>2.676885</td>\n",
       "      <td>-2176416</td>\n",
       "      <td>0.111414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4e4cb5b90ba5de03</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38786200</td>\n",
       "      <td>19151260</td>\n",
       "      <td>12796926</td>\n",
       "      <td>3.271840</td>\n",
       "      <td>-6354334</td>\n",
       "      <td>0.668203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>522ef8893e703a3a</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18687657</td>\n",
       "      <td>2459378</td>\n",
       "      <td>310045</td>\n",
       "      <td>2.704174</td>\n",
       "      <td>-2149333</td>\n",
       "      <td>0.126066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>548b15e67b35ba6b</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11694837</td>\n",
       "      <td>430115</td>\n",
       "      <td>472506</td>\n",
       "      <td>1.877079</td>\n",
       "      <td>42391</td>\n",
       "      <td>1.098557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>554a5f5419b5d610</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18558792</td>\n",
       "      <td>2451817</td>\n",
       "      <td>391695</td>\n",
       "      <td>2.683961</td>\n",
       "      <td>-2060122</td>\n",
       "      <td>0.159757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>57444d10d8dbc8b9</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11251526</td>\n",
       "      <td>142907</td>\n",
       "      <td>234891</td>\n",
       "      <td>1.851067</td>\n",
       "      <td>91984</td>\n",
       "      <td>1.643663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5aa537e0d658225a</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36139185</td>\n",
       "      <td>17863080</td>\n",
       "      <td>12738605</td>\n",
       "      <td>3.045412</td>\n",
       "      <td>-5124475</td>\n",
       "      <td>0.713125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6081c4fbd55e16ea</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36619654</td>\n",
       "      <td>18196664</td>\n",
       "      <td>11240833</td>\n",
       "      <td>3.069888</td>\n",
       "      <td>-6955831</td>\n",
       "      <td>0.617741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>60b9fcce6fd0723e</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18165491</td>\n",
       "      <td>2388983</td>\n",
       "      <td>351953</td>\n",
       "      <td>2.628894</td>\n",
       "      <td>-2037030</td>\n",
       "      <td>0.147323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6cf267cd7600d268</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18563192</td>\n",
       "      <td>2452398</td>\n",
       "      <td>242505</td>\n",
       "      <td>2.684598</td>\n",
       "      <td>-2209893</td>\n",
       "      <td>0.098885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6fb3f7e54d76ceed</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38502277</td>\n",
       "      <td>19179854</td>\n",
       "      <td>17753460</td>\n",
       "      <td>3.219764</td>\n",
       "      <td>-1426394</td>\n",
       "      <td>0.925631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>716441263d3331d0</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11284564</td>\n",
       "      <td>144645</td>\n",
       "      <td>181669</td>\n",
       "      <td>1.856282</td>\n",
       "      <td>37024</td>\n",
       "      <td>1.255965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>76a116def48395a8</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12326802</td>\n",
       "      <td>454640</td>\n",
       "      <td>462309</td>\n",
       "      <td>1.978299</td>\n",
       "      <td>7669</td>\n",
       "      <td>1.016868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7994976cdc25c131</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18343457</td>\n",
       "      <td>2414080</td>\n",
       "      <td>282247</td>\n",
       "      <td>2.654367</td>\n",
       "      <td>-2131833</td>\n",
       "      <td>0.116917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7a589f8ab18426d6</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>35558487</td>\n",
       "      <td>17700761</td>\n",
       "      <td>12930122</td>\n",
       "      <td>2.975696</td>\n",
       "      <td>-4770639</td>\n",
       "      <td>0.730484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8368ab7c633a1ab2</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18188679</td>\n",
       "      <td>2392033</td>\n",
       "      <td>296565</td>\n",
       "      <td>2.632250</td>\n",
       "      <td>-2095468</td>\n",
       "      <td>0.123980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8d5b7952e19e73c</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11854924</td>\n",
       "      <td>426611</td>\n",
       "      <td>432485</td>\n",
       "      <td>1.904339</td>\n",
       "      <td>5874</td>\n",
       "      <td>1.013769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8f1f30dfee59c323</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11256556</td>\n",
       "      <td>142575</td>\n",
       "      <td>180621</td>\n",
       "      <td>1.851960</td>\n",
       "      <td>38046</td>\n",
       "      <td>1.266849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>96d8fcdffa88f938</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12555267</td>\n",
       "      <td>155603</td>\n",
       "      <td>170214</td>\n",
       "      <td>2.066198</td>\n",
       "      <td>14611</td>\n",
       "      <td>1.093899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>9762c3a887e47469</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11372150</td>\n",
       "      <td>144674</td>\n",
       "      <td>211912</td>\n",
       "      <td>1.870872</td>\n",
       "      <td>67238</td>\n",
       "      <td>1.464755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>9a1adec8eeee6ec1</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>37681799</td>\n",
       "      <td>18688739</td>\n",
       "      <td>12862162</td>\n",
       "      <td>3.164881</td>\n",
       "      <td>-5826577</td>\n",
       "      <td>0.688231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>9ba244c4795319f2</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12505467</td>\n",
       "      <td>450022</td>\n",
       "      <td>473345</td>\n",
       "      <td>2.008840</td>\n",
       "      <td>23323</td>\n",
       "      <td>1.051826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>9f3c5fc752625649</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10853266</td>\n",
       "      <td>138813</td>\n",
       "      <td>198292</td>\n",
       "      <td>1.785386</td>\n",
       "      <td>59479</td>\n",
       "      <td>1.428483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>a23672782a2a93af</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11735348</td>\n",
       "      <td>149085</td>\n",
       "      <td>181110</td>\n",
       "      <td>1.930658</td>\n",
       "      <td>32025</td>\n",
       "      <td>1.214810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>c074ef7db223ee78</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18377123</td>\n",
       "      <td>2427816</td>\n",
       "      <td>223855</td>\n",
       "      <td>2.657688</td>\n",
       "      <td>-2203961</td>\n",
       "      <td>0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>c285de8ddda592f2</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12138149</td>\n",
       "      <td>446262</td>\n",
       "      <td>441565</td>\n",
       "      <td>1.948259</td>\n",
       "      <td>-4697</td>\n",
       "      <td>0.989475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>cc8ca2d4264204b1</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12227621</td>\n",
       "      <td>449315</td>\n",
       "      <td>499047</td>\n",
       "      <td>1.962659</td>\n",
       "      <td>49732</td>\n",
       "      <td>1.110684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>cca164d8b97d6685</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12499671</td>\n",
       "      <td>462032</td>\n",
       "      <td>493739</td>\n",
       "      <td>2.005873</td>\n",
       "      <td>31707</td>\n",
       "      <td>1.068625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>d4f9e149097d68f9</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36305278</td>\n",
       "      <td>17977159</td>\n",
       "      <td>11036743</td>\n",
       "      <td>3.054079</td>\n",
       "      <td>-6940416</td>\n",
       "      <td>0.613931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>dab074f4f1f6b006</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11907379</td>\n",
       "      <td>146112</td>\n",
       "      <td>173567</td>\n",
       "      <td>1.959820</td>\n",
       "      <td>27455</td>\n",
       "      <td>1.187904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>df58c685a959c044</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10882741</td>\n",
       "      <td>134687</td>\n",
       "      <td>324294</td>\n",
       "      <td>1.790985</td>\n",
       "      <td>189607</td>\n",
       "      <td>2.407760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>f16984c46aa94970</td>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>33085946</td>\n",
       "      <td>16422993</td>\n",
       "      <td>11196342</td>\n",
       "      <td>2.776607</td>\n",
       "      <td>-5226651</td>\n",
       "      <td>0.681748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>f407b6343e686343</td>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>18293308</td>\n",
       "      <td>2407480</td>\n",
       "      <td>271909</td>\n",
       "      <td>2.647111</td>\n",
       "      <td>-2135571</td>\n",
       "      <td>0.112943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          QUERY_HASH                                       DESCRIPTION1  \\\n",
       "0   129f61e2f3ecf0ef  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "1   1a48bd3a39e20462  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "2   239bb106372cd9ad  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "3   33d198b26945d669  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "4   3898d8ff5bfa1046  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "5   394be8faae0702a1  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "10  45337615a2f78d4e  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "12  4582f099547f7754  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "14  497648c7e308ba08  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "16  4e4cb5b90ba5de03  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "18  522ef8893e703a3a  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "23  548b15e67b35ba6b  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "25  554a5f5419b5d610  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "27  57444d10d8dbc8b9  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "28  5aa537e0d658225a  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "32  6081c4fbd55e16ea  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "34  60b9fcce6fd0723e  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "37  6cf267cd7600d268  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "39  6fb3f7e54d76ceed  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "40  716441263d3331d0  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "41  76a116def48395a8  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "43  7994976cdc25c131  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "45  7a589f8ab18426d6  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "47  8368ab7c633a1ab2  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "52   8d5b7952e19e73c  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "53  8f1f30dfee59c323  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "54  96d8fcdffa88f938  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "55  9762c3a887e47469  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "56  9a1adec8eeee6ec1  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "57  9ba244c4795319f2  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "58  9f3c5fc752625649  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "59  a23672782a2a93af  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "64  c074ef7db223ee78  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "66  c285de8ddda592f2  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "67  cc8ca2d4264204b1  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "68  cca164d8b97d6685  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "69  d4f9e149097d68f9  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "70  dab074f4f1f6b006  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "71  df58c685a959c044  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "81  f16984c46aa94970  TableScan Impl: ColumnVsValue l_shipdate <= '1...   \n",
       "86  f407b6343e686343  TableScan Impl: ColumnBetween l_shipdate BETWE...   \n",
       "\n",
       "    RUNTIME_BASE  RUNTIME_ESTIMATE  RUNTIME_CLUSTERED       rti  TOTAL_ERROR  \\\n",
       "0       12028701            444624             458607  1.930294        13983   \n",
       "1       37786567          18797941           11216317  3.164142     -7581624   \n",
       "2       11934549            149966             176640  1.963705        26674   \n",
       "3       11613677            417356             438003  1.865681        20647   \n",
       "4       52674334          26254001          287319586  4.402514    261065585   \n",
       "5       24426745           3227037            2083079  3.532581     -1143958   \n",
       "10      18327812           2410330             339800  2.652385     -2070530   \n",
       "12      12092679            447065             476348  1.940548        29283   \n",
       "14      18513811           2449303             272887  2.676885     -2176416   \n",
       "16      38786200          19151260           12796926  3.271840     -6354334   \n",
       "18      18687657           2459378             310045  2.704174     -2149333   \n",
       "23      11694837            430115             472506  1.877079        42391   \n",
       "25      18558792           2451817             391695  2.683961     -2060122   \n",
       "27      11251526            142907             234891  1.851067        91984   \n",
       "28      36139185          17863080           12738605  3.045412     -5124475   \n",
       "32      36619654          18196664           11240833  3.069888     -6955831   \n",
       "34      18165491           2388983             351953  2.628894     -2037030   \n",
       "37      18563192           2452398             242505  2.684598     -2209893   \n",
       "39      38502277          19179854           17753460  3.219764     -1426394   \n",
       "40      11284564            144645             181669  1.856282        37024   \n",
       "41      12326802            454640             462309  1.978299         7669   \n",
       "43      18343457           2414080             282247  2.654367     -2131833   \n",
       "45      35558487          17700761           12930122  2.975696     -4770639   \n",
       "47      18188679           2392033             296565  2.632250     -2095468   \n",
       "52      11854924            426611             432485  1.904339         5874   \n",
       "53      11256556            142575             180621  1.851960        38046   \n",
       "54      12555267            155603             170214  2.066198        14611   \n",
       "55      11372150            144674             211912  1.870872        67238   \n",
       "56      37681799          18688739           12862162  3.164881     -5826577   \n",
       "57      12505467            450022             473345  2.008840        23323   \n",
       "58      10853266            138813             198292  1.785386        59479   \n",
       "59      11735348            149085             181110  1.930658        32025   \n",
       "64      18377123           2427816             223855  2.657688     -2203961   \n",
       "66      12138149            446262             441565  1.948259        -4697   \n",
       "67      12227621            449315             499047  1.962659        49732   \n",
       "68      12499671            462032             493739  2.005873        31707   \n",
       "69      36305278          17977159           11036743  3.054079     -6940416   \n",
       "70      11907379            146112             173567  1.959820        27455   \n",
       "71      10882741            134687             324294  1.790985       189607   \n",
       "81      33085946          16422993           11196342  2.776607     -5226651   \n",
       "86      18293308           2407480             271909  2.647111     -2135571   \n",
       "\n",
       "    RELATIVE_ERROR  \n",
       "0         1.031449  \n",
       "1         0.596678  \n",
       "2         1.177867  \n",
       "3         1.049471  \n",
       "4        10.943840  \n",
       "5         0.645508  \n",
       "10        0.140977  \n",
       "12        1.065501  \n",
       "14        0.111414  \n",
       "16        0.668203  \n",
       "18        0.126066  \n",
       "23        1.098557  \n",
       "25        0.159757  \n",
       "27        1.643663  \n",
       "28        0.713125  \n",
       "32        0.617741  \n",
       "34        0.147323  \n",
       "37        0.098885  \n",
       "39        0.925631  \n",
       "40        1.255965  \n",
       "41        1.016868  \n",
       "43        0.116917  \n",
       "45        0.730484  \n",
       "47        0.123980  \n",
       "52        1.013769  \n",
       "53        1.266849  \n",
       "54        1.093899  \n",
       "55        1.464755  \n",
       "56        0.688231  \n",
       "57        1.051826  \n",
       "58        1.428483  \n",
       "59        1.214810  \n",
       "64        0.092204  \n",
       "66        0.989475  \n",
       "67        1.110684  \n",
       "68        1.068625  \n",
       "69        0.613931  \n",
       "70        1.187904  \n",
       "71        2.407760  \n",
       "81        0.681748  \n",
       "86        0.112943  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_scans(table_name):\n",
    "    m = create_model(table_name, 2)\n",
    "    m.estimate_total_runtime([CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "    assert len(m.scan_estimates[m.scan_estimates['RUNTIME_ESTIMATE'] < 0]) == 0, \"not all runtimes computed\"\n",
    "\n",
    "    CLUSTERED_STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPCH-SHIPDATE-PRUNED\"\n",
    "    path = f\"{CLUSTERED_STATISTICS_PATH}/table_scans.csv\"\n",
    "    clustered_scans = pd.read_csv(path, sep='|')\n",
    "    clustered_scans = clustered_scans[clustered_scans['TABLE_NAME'] == table_name]\n",
    "    clustered_scans = clustered_scans.sort_values(['QUERY_HASH', 'DESCRIPTION'])\n",
    "    \n",
    "    m.scan_estimates.sort_values(['QUERY_HASH', 'DESCRIPTION'], inplace=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['QUERY_HASH'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['rti'] = np.array(m.scan_estimates['time_per_input_row'])\n",
    "    result['COLUMN_NAME'] = np.array(clustered_scans['COLUMN_NAME'])\n",
    "    result['DESCRIPTION1'] = np.array(m.scan_estimates['DESCRIPTION'])\n",
    "    result['DESCRIPTION2'] = np.array(clustered_scans['DESCRIPTION'])\n",
    "    result['QUERY_HASH1'] = np.array(m.scan_estimates['QUERY_HASH'])\n",
    "    result['QUERY_HASH2'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['RUNTIME_BASE'] = np.array(m.scan_estimates['RUNTIME_NS'])\n",
    "    result['RUNTIME_ESTIMATE'] = np.array(m.scan_estimates.apply(lambda row: row['RUNTIME_ESTIMATE'] / m.query_frequency(row['QUERY_HASH']), axis=1), dtype=np.int64)\n",
    "    result['RUNTIME_CLUSTERED'] = np.array(clustered_scans['RUNTIME_NS'])\n",
    "    \n",
    "    # make sure we match all operators\n",
    "    matches = result.apply(lambda row: row['DESCRIPTION1'] == row['DESCRIPTION2'] and row['QUERY_HASH1'] == row['QUERY_HASH2'], axis=1)\n",
    "    assert matches.all(), \"not all rows match\"\n",
    "    \n",
    "    result['TOTAL_ERROR'] = result['RUNTIME_CLUSTERED'] - result['RUNTIME_ESTIMATE']\n",
    "    result['RELATIVE_ERROR'] = result['RUNTIME_CLUSTERED'] / result['RUNTIME_ESTIMATE']\n",
    "    \n",
    "    return result\n",
    "    \n",
    "CLUSTERING_COLUMN = \"l_shipdate\"\n",
    "results = eval_scans(\"lineitem\")\n",
    "results = results[CLUSTERING_COLUMN == results['COLUMN_NAME']]\n",
    "#results = results[\"l_quantity\" == results['COLUMN_NAME']]\n",
    "#results = results[results['COLUMN_NAME'] != 'l_receiptdate']\n",
    "results[['QUERY_HASH', 'DESCRIPTION1', 'RUNTIME_BASE', 'RUNTIME_ESTIMATE', 'RUNTIME_CLUSTERED', 'rti', 'TOTAL_ERROR', 'RELATIVE_ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 2]],\n",
    "            'orders': [['o_orderdate', 2]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "def create_model(table_name, max_dimensions=2):    \n",
    "    query_frequencies = get_query_frequencies()\n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    start_time_table = datetime.now()\n",
    "    single_table_scans = extract_single_table(scans, table_name)\n",
    "\n",
    "    model = DisjointClustersModel(max_dimensions, query_frequencies, table_name, single_table_scans, table_sizes, distinct_values, CHUNK_SIZE, correlations.get(table_name, {}), joins, sorted_columns_during_creation)\n",
    "    return model\n",
    "\n",
    "model = create_model(\"lineitem\", 2)\n",
    "#model.table_scans['original_unprunable_part'] = np.array(model.table_scans['original_unprunable_part'], dtype=np.int32)\n",
    "#model.table_scans[model.table_scans['COLUMN_NAME'] == 'o_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_joins(table_name):\n",
    "    m = create_model(table_name, 2)\n",
    "    m.estimate_total_runtime([\"l_shipdate\"], \"l_shipdate\", [100])\n",
    "    assert len(m.join_estimates[m.join_estimates['ESTIMATE_MATERIALIZE_BUILD'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    assert len(m.join_estimates[m.join_estimates['ESTIMATE_MATERIALIZE_PROBE'] < 0]) == 0, \"not all runtimes computed\"\n",
    "\n",
    "    CLUSTERED_STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/tpch_shipdate\"\n",
    "    path = f\"{CLUSTERED_STATISTICS_PATH}/joins.csv\"\n",
    "    clustered_joins = pd.read_csv(path, sep='|')\n",
    "    clustered_joins = clustered_joins[clustered_joins['TABLE_NAME'] == table_name]\n",
    "    clustered_joins = clustered_joins.sort_values(['QUERY_HASH', 'DESCRIPTION'])\n",
    "    \n",
    "    m.join_estimates.sort_values(['QUERY_HASH', 'DESCRIPTION'], inplace=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['COLUMN_NAME'] = np.array(clustered_scans['COLUMN_NAME'])\n",
    "    result['DESCRIPTION1'] = np.array(m.scan_estimates['DESCRIPTION'])\n",
    "    result['DESCRIPTION2'] = np.array(clustered_scans['DESCRIPTION'])\n",
    "    result['QUERY_HASH1'] = np.array(m.scan_estimates['QUERY_HASH'])\n",
    "    result['QUERY_HASH2'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['RUNTIME_BASE'] = np.array(m.scan_estimates['RUNTIME_NS'])\n",
    "    result['RUNTIME_ESTIMATE'] = np.array(m.scan_estimates.apply(lambda row: row['RUNTIME_ESTIMATE'] / m.query_frequency(row['QUERY_HASH']), axis=1), dtype=np.int64)\n",
    "    result['RUNTIME_CLUSTERED'] = np.array(clustered_scans['RUNTIME_NS'])\n",
    "    \n",
    "    # make sure we match all operators\n",
    "    matches = result.apply(lambda row: row['DESCRIPTION1'] == row['DESCRIPTION2'] and row['QUERY_HASH1'] == row['QUERY_HASH2'], axis=1)\n",
    "    assert matches.all(), \"not all rows match\"\n",
    "    \n",
    "    result['TOTAL_ERROR'] = result['RUNTIME_CLUSTERED'] - result['RUNTIME_ESTIMATE']\n",
    "    result['RELATIVE_ERROR'] = result['RUNTIME_CLUSTERED'] / result['RUNTIME_ESTIMATE']\n",
    "    \n",
    "    return result\n",
    "    \n",
    "CLUSTERING_COLUMN = \"l_shipdate\"\n",
    "results = eval_scans(\"lineitem\")\n",
    "results = results[CLUSTERING_COLUMN == results['COLUMN_NAME']]\n",
    "#results = results[results['COLUMN_NAME'] != 'l_receiptdate']\n",
    "results[['DESCRIPTION1', 'RUNTIME_BASE', 'RUNTIME_ESTIMATE', 'RUNTIME_CLUSTERED', 'TOTAL_ERROR', 'RELATIVE_ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.suggest_clustering(20)\n",
    "#model.estimate_distinct_values_per_chunk_at_statistics_time(\"l_orderkey\", \"lineitem\")\n",
    "#print(model.estimate_distinct_values_per_chunk(\"l_orderkey\", [\"l_orderkey\", \"l_partkey\"], \"l_orderkey\", [20,5]))\n",
    "\n",
    "#model.estimate_chunk_count(\"9762c3a887e47469\", 77313, [\"l_orderkey\"], [92])\n",
    "#row = model.joins.iloc[140]\n",
    "#model.get_chunk_count_factor(row, \"BUILD\", [\"l_orderkey\", \"l_shipdate\"], [5, 20])\n",
    "\n",
    "if False or BENCHMARK == \"TPCH\":\n",
    "    time0 = model.estimate_total_runtime([\"l_orderkey\"], \"l_orderkey\", [100])\n",
    "    time1 = model.estimate_total_runtime([\"l_shipdate\", \"l_orderkey\"], \"l_orderkey\", [20, 5])\n",
    "    print(f\"nosort: {time0}\")\n",
    "    print(f\"best: {time1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering configs for all relevant tables\n",
    "# Currently deactivated, but working\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 2]],\n",
    "            'orders': [['o_orderdate', 2]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "def create_benchmark_configs():    \n",
    "    start_time = datetime.now()\n",
    "    clusterings = {\"default\" : default_benchmark_config()}\n",
    "    query_frequencies = get_query_frequencies()\n",
    "    \n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    print(table_names)\n",
    "    for table_name in table_names:\n",
    "        start_time_table = datetime.now()\n",
    "        single_table_scans = extract_single_table(scans, table_name)\n",
    "        table_size = table_sizes[table_name]\n",
    "        if table_size <= 3 * CHUNK_SIZE:\n",
    "            print(f\"Not computing clustering for {table_name}, as it has only {table_size} rows\")\n",
    "            continue\n",
    "\n",
    "        max_dimensions = 2\n",
    "        model = DisjointClustersModel(max_dimensions, query_frequencies, table_name, single_table_scans, table_sizes, distinct_values, CHUNK_SIZE, correlations.get(table_name, {}), joins, sorted_columns_during_creation)\n",
    "        table_clusterings = model.suggest_clustering(3)\n",
    "        for table_clustering in table_clusterings:\n",
    "            config = default_benchmark_config()\n",
    "            config[table_name] = format_table_clustering(table_clustering)\n",
    "            config_name = get_config_name(config)\n",
    "            clusterings[config_name] = config\n",
    "        end_time_table = datetime.now()\n",
    "        print(f\"Done computing clustering for {table_name} ({end_time_table - start_time_table})\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Computed all clusterings in {end_time - start_time}\")\n",
    "    \n",
    "    return clusterings\n",
    "\n",
    "create_benchmark_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdated code fragments (older model versions) are kept below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"This assertion failure only serves to stop execution here when clicking Cells->Run all. You can safely ignore it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, table_scans, correlations = {}):\n",
    "        super().__init__(table_scans, correlations)        \n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        pairs = itertools.product(interesting_columns, interesting_columns)                \n",
    "        total_runtimes = [self.estimate_total_runtime(self.table_scans, clustering_columns) for clustering_columns in pairs]\n",
    "        total_runtimes.sort(key=lambda x: x[1], reverse=False)\n",
    "        \n",
    "        return total_runtimes[0:first_k]\n",
    "        \n",
    "    \n",
    "    def estimate_total_runtime(self, single_table, clustering_columns):\n",
    "        total_runtime = 0\n",
    "        \n",
    "        pruning_col = clustering_columns[0]\n",
    "        sorted_col = clustering_columns[1]\n",
    "        def compute_runtime(row):\n",
    "            col_name = row['COLUMN_NAME']\n",
    "            if pruning_col == sorted_col:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_log_runtime']\n",
    "                else:\n",
    "                    if col_name in self.correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "\n",
    "            else:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_runtime']\n",
    "                elif col_name == sorted_col:\n",
    "                    # TODO: should this be affected by correlation?\n",
    "                    # we will get less chunks, so a linear scan should be close to optimal_runtime,\n",
    "                    # but log time should beat it anyway\n",
    "                    return row['log_runtime']\n",
    "                else:\n",
    "                    if col_name in self.correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "                    \n",
    "        effective_runtime = single_table.apply(compute_runtime, axis=1)\n",
    "        return [clustering_columns, effective_runtime.sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store additional statistics\n",
    "# TODO keep?\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def round_up_to_chunksize(row):\n",
    "    if row['OUTPUT_ROW_COUNT'] % CHUNK_SIZE == 0:\n",
    "        return row['OUTPUT_ROW_COUNT']\n",
    "    else:\n",
    "        return row['OUTPUT_ROW_COUNT'] + (CHUNK_SIZE - (row['OUTPUT_ROW_COUNT'] % CHUNK_SIZE))\n",
    "\n",
    "scans['pruned_minimum_input_rows'] = scans.apply(round_up_to_chunksize, axis=1)\n",
    "\n",
    "scans['selectivity'] = scans['OUTPUT_ROW_COUNT'] / scans['INPUT_ROW_COUNT']\n",
    "scans['actual_selectivity'] = scans['SINGLE_OUTPUT_ROWS'] / scans['SINGLE_INPUT_ROWS']\n",
    "\n",
    "scans['time_per_ir'] = scans['RUNTIME_NS'] / scans['INPUT_ROW_COUNT']\n",
    "scans['time_per_or'] = scans['RUNTIME_NS'] / scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "# optimal runtime assuming perfect pruning, but not sortedness\n",
    "scans['optimal_runtime'] = scans['time_per_ir'] * scans['pruned_minimum_input_rows']\n",
    "scans['runtime_gain'] = scans['RUNTIME_NS'] - scans['optimal_runtime']\n",
    "\n",
    "\n",
    "# log runtime for sorted columns\n",
    "scans['log_runtime'] = np.log2(scans['RUNTIME_NS'])\n",
    "scans['optimal_log_runtime'] = np.log2(1+scans['optimal_runtime'])\n",
    "scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAIN_COLUMN = 'runtime_gain'\n",
    "\n",
    "scans_groupby_columnname = scans.groupby(['TABLE_NAME', 'COLUMN_NAME'])\n",
    "sum_of_gains = pd.DataFrame(scans_groupby_columnname[GAIN_COLUMN].sum())\n",
    "sum_of_gains.sort_values(by=['TABLE_NAME', GAIN_COLUMN], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "if BENCHMARK == \"TPCH\":\n",
    "    TABLE = \"lineitem\"\n",
    "else:    \n",
    "    TABLE = \"customer_demographics\"\n",
    "\n",
    "import itertools\n",
    "\n",
    "def extract_single_table(table_name):\n",
    "    return scans[scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def extract_interesting_columns(df):\n",
    "    return list(df['COLUMN_NAME'].unique())\n",
    "\n",
    "\n",
    "correlations = {\n",
    "    'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "    'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "    'l_commitdate': ['l_receiptdate', 'l_shipdate']\n",
    "}\n",
    "#correlations = {}\n",
    "def table_sorting_options(table_name):\n",
    "    single_table = extract_single_table(table_name)\n",
    "    interesting_cols = extract_interesting_columns(single_table)\n",
    "    pairs = itertools.product(interesting_cols, interesting_cols)\n",
    "    \n",
    "    total_times = []\n",
    "    for pair in pairs:\n",
    "        pruning_col = pair[0]\n",
    "        sorted_col = pair[1]\n",
    "\n",
    "        def compute_runtime(row):\n",
    "            col_name = row['COLUMN_NAME']\n",
    "            if pruning_col == sorted_col:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_log_runtime']\n",
    "                else:\n",
    "                    if col_name in correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "\n",
    "            else:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_runtime']\n",
    "                elif col_name == sorted_col:\n",
    "                    # TODO: should this be affected by correlation?\n",
    "                    # we will get less chunks, so a linear scan should be close to optimal_runtime,\n",
    "                    # but log time should beat it anyway\n",
    "                    return row['log_runtime']\n",
    "                else:\n",
    "                    if col_name in correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "\n",
    "        effective_runtime = single_table.apply(compute_runtime, axis=1)\n",
    "        total_times.append([pair, effective_runtime.sum()])    \n",
    "    total_times = pd.DataFrame(total_times, columns=['columns', 'time'])    \n",
    "    return total_times\n",
    "\n",
    "options = table_sorting_options(TABLE)\n",
    "options.sort_values(by=['time'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = pd.read_csv(f\"{STATISTICS_PATH}/aggregates.csv\", sep=',')\n",
    "\n",
    "# it looks like column names are mixed up.\n",
    "# COLUMN_NAME -> actually GROUP_BY_COLUMN_COUNT\n",
    "# GROUP_BY_COLUMN_COUNT -> actually AGGREGATE_COLUMN_COUNT\n",
    "# AGGREGATE_COLUMN_COUNT -> actually COLUMN_NAME\n",
    "\n",
    "COL_NAME = 'AGGREGATE_COLUMN_COUNT'\n",
    "GROUPBY_COL = 'COLUMN_NAME'\n",
    "AGG_COL = 'GROUP_BY_COLUMN_COUNT'\n",
    "\n",
    "# All aggregates have to read the entire table, so we cannot skip chunks.\n",
    "# But getting all groups consecutive could provide a speedup\n",
    "# As a result, we care only about aggregates with group by columns\n",
    "\n",
    "interesting_aggregates = aggregates[aggregates[GROUPBY_COL] > 0]\n",
    "stats = interesting_aggregates.groupby(['TABLE_NAME', COL_NAME])\n",
    "out_columns = pd.DataFrame(stats['OUTPUT_ROW_COUNT'].max())\n",
    "out_columns.sort_values(by=['TABLE_NAME', 'OUTPUT_ROW_COUNT'], ascending=[True, False])\n",
    "aggregates[aggregates['COLUMN_TYPE'] == 'DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_time_per_column = scans.groupby(['COLUMN_NAME'])\n",
    "accumulated_scan_times = pd.DataFrame(scan_time_per_column['RUNTIME_NS'].sum())\n",
    "total_scan_runtime = accumulated_scan_times['RUNTIME_NS'].sum()\n",
    "assert total_scan_runtime == scans['RUNTIME_NS'].sum(), f\"{total_scan_runtime}, {scans['RUNTIME_NS'].sum()}\"\n",
    "print(f\"total scan runtime: {total_scan_runtime}\")\n",
    "\n",
    "scan_time_per_column_prunable = scans[scans['useful_for_pruning']].groupby(['COLUMN_NAME'])\n",
    "accumulated_prunable_scan_times = pd.DataFrame(scan_time_per_column_prunable['RUNTIME_NS'].sum())\n",
    "total_prunable_scan_runtime = accumulated_prunable_scan_times['RUNTIME_NS'].sum()\n",
    "print(f\"total prunable scan runtime: {total_prunable_scan_runtime}\")\n",
    "print(f\"{100*total_prunable_scan_runtime/total_scan_runtime}% of scan runtime amount to prunable scans\")\n",
    "\n",
    "accumulated_scan_times.sort_values(['RUNTIME_NS'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "joins = load_join_statistics()\n",
    "\n",
    "print(joins['PROBE_COLUMN'].unique())\n",
    "\n",
    "join_time_per_column = joins.groupby(['PROBE_COLUMN'])\n",
    "\n",
    "accumulated_join_times = pd.DataFrame(join_time_per_column['RUNTIME_NS'].sum())\n",
    "print(len(accumulated_join_times))\n",
    "total_join_runtime = accumulated_join_times['RUNTIME_NS'].sum()\n",
    "#assert total_join_runtime == joins['RUNTIME_NS'].sum(), f\"{total_join_runtime},{joins['RUNTIME_NS'].sum()}\"\n",
    "print(f\"total join runtime: {total_join_runtime}\")\n",
    "\n",
    "joins[joins.apply(lambda x : x['PROBE_COLUMN'] not in ['o_custkey' ,'n_nationkey' ,'s_nationkey' ,'l_suppkey', 's_suppkey',\n",
    " 'l_orderkey', 'o_orderkey', 'p_partkey' ,'l_partkey' ,'ps_suppkey',\n",
    " 'c_nationkey' ,'r_regionkey' ,'c_custkey' ,'ps_partkey'] ,axis=1)]\n",
    "\n",
    "print(f\"for {BENCHMARK}, joins take about {total_join_runtime / total_scan_runtime} times longer than table scans\")\n",
    "accumulated_join_times.sort_values(['RUNTIME_NS'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
