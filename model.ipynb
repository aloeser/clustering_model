{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is configured for TPCH (chunk size 65535) with scale factor 1, 60 seconds runtime, and at most 10 runs per query\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "\n",
    "BENCHMARK = \"TPCH\"\n",
    "CHUNK_SIZE = 65535\n",
    "\n",
    "if BENCHMARK == \"TPCH\":\n",
    "    SCALE_FACTOR = 1\n",
    "    RUNS = 10\n",
    "    TIME = 60\n",
    "    #STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-H__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}\"\n",
    "    STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-H__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}__ENCODING_DictionaryFSBA\"\n",
    "elif BENCHMARK == \"TPCDS\":\n",
    "    SCALE_FACTOR = 1\n",
    "    RUNS = 1\n",
    "    TIME = 60\n",
    "    STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-DS__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}\"\n",
    "else:\n",
    "    raise Exception(\"Unknown benchmark: \" + BENCHMARK)\n",
    "\n",
    "print(f\"Model is configured for {BENCHMARK} (chunk size {CHUNK_SIZE}) with scale factor {SCALE_FACTOR}, {TIME} seconds runtime, and at most {RUNS} runs per query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ~/Dokumente/repos/example_plugin/TPC-H__SF_1.000000__RUNS_10__TIME_60__ENCODING_DictionaryFSBA/table_scans.csv\n"
     ]
    }
   ],
   "source": [
    "# Load table scan statistics\n",
    "\n",
    "path = f\"{STATISTICS_PATH}/table_scans.csv\"\n",
    "scans = pd.read_csv(path, sep='|')\n",
    "EXPECTED_SCAN_COUNT = len(scans)\n",
    "LOADED_CHUNK_SIZE = CHUNK_SIZE\n",
    "LOADED_BENCHMARK = BENCHMARK\n",
    "LOADED_SCALE_FACTOR = SCALE_FACTOR\n",
    "LOADED_RUNS = RUNS\n",
    "LOADED_TIME = TIME\n",
    "\n",
    "print(f\"Successfully loaded {path}\")\n",
    "\n",
    "def assert_correct_statistics_loaded():\n",
    "    assert BENCHMARK == LOADED_BENCHMARK, f\"The model is configured to use {BENCHMARK}, but {LOADED_BENCHMARK} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert SCALE_FACTOR == LOADED_SCALE_FACTOR, f\"The model is configured to use {SCALE_FACTOR} as scale factor, but data for a scale factor of {LOADED_SCALE_FACTOR} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert RUNS == LOADED_RUNS, f\"The model is configured to perform at most {RUNS} runs, but the currently loaded data had at most {LOADED_RUNS} runs.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert TIME == LOADED_TIME, f\"The model is configured to run for {TIME} seconds, but the currently data had a runtime of {LOADED_TIME} seconds.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert CHUNK_SIZE == LOADED_CHUNK_SIZE, f\"The model is configured to use {CHUNK_SIZE} as chunk_size, but data for a chunk size of {LOADED_CHUNK_SIZE} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert EXPECTED_SCAN_COUNT == len(scans), f\"There should be {EXPECTED_SCAN_COUNT} table scans, but there are only {len(scans)}\\nProbably one of the last commands reassigned it unintentionally\"\n",
    "    \n",
    "    assert 'GET_TABLE_HASH' in scans.columns, f\"the statistics in {STATISTICS_PATH} are outdated (column 'GET_TABLE_HASH' in table_scans.csv is missing). Please create them again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - looks like pruning was deactivated while the statistics were created\n"
     ]
    }
   ],
   "source": [
    "# Validate table scans\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "# To make sure pruning was not active,\n",
    "# first fetch table sizes,\n",
    "table_statistics = pd.read_csv(f\"{STATISTICS_PATH}/table_meta_data.csv\", sep='|')\n",
    "table_sizes = dict(zip(table_statistics.TABLE_NAME, table_statistics.ROW_COUNT))\n",
    "\n",
    "# then make sure INPUT_ROW_COUNT == table_size\n",
    "def input_size_matches(row):\n",
    "    #print(row)\n",
    "    \n",
    "    actual_row_count = row['INPUT_ROW_COUNT']\n",
    "    table = row['TABLE_NAME']\n",
    "    expected_row_count = table_sizes[table]\n",
    "    return expected_row_count == actual_row_count\n",
    "\n",
    "data_scans = scans[scans['COLUMN_TYPE'] == 'DATA']\n",
    "input_size_matches = data_scans.apply(input_size_matches, axis=1)\n",
    "all_sizes_match = reduce(np.logical_and, input_size_matches) #input_size_matches.apply()\n",
    "\n",
    "if not all_sizes_match:\n",
    "    #raise Exception(\"The given statistics were probably created while pruning was active\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"OK - looks like pruning was deactivated while the statistics were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for TPCH contain 436 table scans\n",
      "Of those, only 266 are useful for pruning\n",
      "TODO: For now, filtering on scans is deactivated. This is because all scans are needed to recognize OR-Chains. Models have to take care themselves whether a scan can contribute to pruning or not\n"
     ]
    }
   ],
   "source": [
    "# Append additional information to the table scans\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "print(f\"Statistics for {BENCHMARK} contain {len(scans)} table scans\")\n",
    "\n",
    "\n",
    "# Add statistics about selectivity and speed for each operator\n",
    "scans['selectivity'] = scans['OUTPUT_ROW_COUNT'] / scans['INPUT_ROW_COUNT']\n",
    "\n",
    "# TODO: Assumption that reading and writing a row have the same cost\n",
    "scans['time_per_row'] = scans['RUNTIME_NS'] / (scans['INPUT_ROW_COUNT'] + scans['OUTPUT_ROW_COUNT'])\n",
    "scans['time_per_input_row'] = scans['time_per_row']\n",
    "scans['time_per_output_row'] = scans['time_per_row']\n",
    "\n",
    "\n",
    "def determine_or_chains(table_scans):\n",
    "    table_scans['part_of_or_chain'] = False\n",
    "    \n",
    "    single_table_scans = table_scans.groupby(['QUERY_HASH', 'TABLE_NAME', 'GET_TABLE_HASH'])\n",
    "    \n",
    "    for _, scans in single_table_scans:\n",
    "        input_row_frequencies = Counter(scans.INPUT_ROW_COUNT)\n",
    "        or_input_sizes = set([input_size for input_size, frequency in input_row_frequencies.items() if frequency > 1])\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['INPUT_ROW_COUNT'] = scans['INPUT_ROW_COUNT']\n",
    "        df['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "        df['part_of_or_chain'] = scans.apply(lambda row: row['INPUT_ROW_COUNT'] in or_input_sizes, axis=1)\n",
    "\n",
    "        for _ in range(len(scans)):\n",
    "            or_input_sizes |= set(df[df['part_of_or_chain']].OUTPUT_ROW_COUNT.unique())\n",
    "            df['part_of_or_chain'] = df.apply(lambda row: row['INPUT_ROW_COUNT'] in or_input_sizes, axis=1)\n",
    "\n",
    "        or_chains = list(df[df['part_of_or_chain']].index)\n",
    "        #table_scans.iloc[or_chains, table_scans.columns.get_loc('part_of_or_chain')] = True\n",
    "        table_scans.loc[or_chains, 'part_of_or_chain'] = True\n",
    "    \n",
    "    return table_scans\n",
    "\n",
    "# Hyrise does not use scans that are part of an OR-chain for pruning\n",
    "scans = determine_or_chains(scans)\n",
    "\n",
    "\n",
    "# Like scans are not useful if they start with %\n",
    "# TODO what if they dont start with % and contain more than one % ? -> up to first % prunable, but is it used?\n",
    "def benefits_from_sorting(row):    \n",
    "    description = row['DESCRIPTION']\n",
    "    if \"ColumnLike\" in description:\n",
    "        words = description.split('LIKE')\n",
    "        assert len(words) == 2, f\"expected exactly one occurence of LIKE, but got {description}\"\n",
    "        like_criteria = words[1]\n",
    "        assert \"%\" in like_criteria or \"_\" in like_criteria, f\"LIKE operators should have an % or _, but found none in {like_criteria}\"\n",
    "        return like_criteria[1] != '%' and like_criteria[1] != '_'\n",
    "    elif \"ExpressionEvaluator\" in description and \" IN \" in description:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "scans['benefits_from_sorting'] = scans.apply(benefits_from_sorting, axis=1)\n",
    "# TODO: valid atm, but feels a bit hacky to assume not benefitting from sorted segments -> not benefitting from pruning\n",
    "scans['useful_for_pruning'] = scans.apply(lambda row: not row['part_of_or_chain'] and row['benefits_from_sorting'] , axis=1)\n",
    "EXPECTED_SCAN_COUNT = len(scans)\n",
    "print(f\"Of those, only {len(scans[scans['useful_for_pruning']])} are useful for pruning\")\n",
    "\n",
    "print(\"TODO: For now, filtering on scans is deactivated. This is because all scans are needed to recognize OR-Chains. Models have to take care themselves whether a scan can contribute to pruning or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test OK\n"
     ]
    }
   ],
   "source": [
    "def test_determine_or_chains():\n",
    "    test = pd.DataFrame()\n",
    "    test['QUERY_HASH'] = pd.Series(['1']*3  + ['2']*4)\n",
    "    test['TABLE_NAME'] = pd.Series(['lineitem']*3  + ['part']*4)\n",
    "    test['GET_TABLE_HASH'] = pd.Series(['0x1'] + ['0x2']*2 + ['0x3']*4)\n",
    "    test['COLUMN_NAME'] = pd.Series(['l_shipdate', 'l_shipdate', 'l_discount', 'p_brand', 'p_type', 'p_type', 'p_size'])\n",
    "    test['INPUT_ROW_COUNT'] = pd.Series( [6001215, 6001215, 200000, 200000, 199000, 199000, 50000])\n",
    "    test['OUTPUT_ROW_COUNT'] = pd.Series([ 400000,  300000, 200000, 199000,      0,  50000, 20000])\n",
    "    test_result = determine_or_chains(test)\n",
    "    assert len(test_result) == 7, \"should not filter out any rows\"    \n",
    "    assert len(test_result[test_result['part_of_or_chain']]) == 3, \"expected 3 scans, got\\n\" + str(test_result)\n",
    "    assert list(test_result['part_of_or_chain']) == [False]*4 + [True]*3\n",
    "    print(\"Test OK\")\n",
    "\n",
    "test_determine_or_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query frequency information\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def get_query_frequencies():\n",
    "    plan_cache = pd.read_csv(f\"{STATISTICS_PATH}/plan_cache.csv\", sep='|')\n",
    "    return dict(zip(plan_cache.QUERY_HASH, plan_cache.EXECUTION_COUNT))\n",
    "\n",
    "#get_query_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load column statistics - especially interesting: number of distinct values, and columns sorted during statistics creation\n",
    "\n",
    "# Returns a 2-level-dictionary: distinct_values[TABLE][COLUMN] = number_of_distinct_values\n",
    "def get_distinct_values_count():        \n",
    "    # Code\n",
    "    column_statistics_df = pd.read_csv(f\"{STATISTICS_PATH}/column_meta_data.csv\", sep='|')\n",
    "    column_statistics_df['DISTINCT_VALUES'] = np.int32(column_statistics_df['DISTINCT_VALUES'])\n",
    "    tables_and_columns = column_statistics_df.groupby('TABLE_NAME')\n",
    "    distinct_values = {table: dict(zip(column_df.COLUMN_NAME, column_df.DISTINCT_VALUES)) for table, column_df in tables_and_columns }\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    num_tables = len(distinct_values)\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        assert num_tables == 8, f\"TPCH has 8 tables, but got {num_tables}\"\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        assert num_tables == 24, f\"TPCDS has 24 tables, but got {num_tables}\"\n",
    "    else:\n",
    "        assert False, \"Insert a benchmark specific check here\"\n",
    "    \n",
    "    return distinct_values\n",
    "\n",
    "# Returns a dictionary: sorted_columns_during_creation[TABLE] = [column1, column2, ...]\n",
    "def get_sorted_columns_during_creation():\n",
    "    # Code\n",
    "    column_statistics_df = pd.read_csv(f\"{STATISTICS_PATH}/column_meta_data.csv\", sep='|')\n",
    "    globally_sorted_columns = column_statistics_df[column_statistics_df['IS_GLOBALLY_SORTED'] == 1]\n",
    "    \n",
    "    tables_and_columns = globally_sorted_columns.groupby('TABLE_NAME')\n",
    "    globally_sorted_columns = {table: list(column_df.COLUMN_NAME) for table, column_df in tables_and_columns }\n",
    "    \n",
    "    return globally_sorted_columns\n",
    "\n",
    "#get_distinct_values_count()\n",
    "#get_sorted_columns_during_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOIN_MODE</th>\n",
       "      <th>LEFT_TABLE_NAME</th>\n",
       "      <th>LEFT_COLUMN_NAME</th>\n",
       "      <th>LEFT_TABLE_ROW_COUNT</th>\n",
       "      <th>RIGHT_TABLE_NAME</th>\n",
       "      <th>RIGHT_COLUMN_NAME</th>\n",
       "      <th>RIGHT_TABLE_ROW_COUNT</th>\n",
       "      <th>OUTPUT_ROW_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Semi</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>5741</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_partkey</td>\n",
       "      <td>192</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Semi</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>800000</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29935</td>\n",
       "      <td>119740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>AntiNullAsTrue</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_suppkey</td>\n",
       "      <td>119740</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>4</td>\n",
       "      <td>119707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Inner</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>119707</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29935</td>\n",
       "      <td>119707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Semi</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>800000</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29437</td>\n",
       "      <td>117748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>AntiNullAsTrue</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_suppkey</td>\n",
       "      <td>117748</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>4</td>\n",
       "      <td>117697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Inner</td>\n",
       "      <td>partsupp</td>\n",
       "      <td>ps_partkey</td>\n",
       "      <td>117697</td>\n",
       "      <td>part</td>\n",
       "      <td>p_partkey</td>\n",
       "      <td>29437</td>\n",
       "      <td>117697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Inner</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_suppkey</td>\n",
       "      <td>1</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Inner</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_suppkey</td>\n",
       "      <td>1</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Inner</td>\n",
       "      <td>lineitem</td>\n",
       "      <td>l_suppkey</td>\n",
       "      <td>1</td>\n",
       "      <td>supplier</td>\n",
       "      <td>s_suppkey</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         JOIN_MODE LEFT_TABLE_NAME LEFT_COLUMN_NAME  LEFT_TABLE_ROW_COUNT  \\\n",
       "50            Semi            part        p_partkey                  5741   \n",
       "51            Semi        partsupp       ps_partkey                800000   \n",
       "52  AntiNullAsTrue        partsupp       ps_suppkey                119740   \n",
       "53           Inner        partsupp       ps_partkey                119707   \n",
       "54            Semi        partsupp       ps_partkey                800000   \n",
       "55  AntiNullAsTrue        partsupp       ps_suppkey                117748   \n",
       "56           Inner        partsupp       ps_partkey                117697   \n",
       "58           Inner        lineitem        l_suppkey                     1   \n",
       "60           Inner        lineitem        l_suppkey                     1   \n",
       "62           Inner        lineitem        l_suppkey                     1   \n",
       "\n",
       "   RIGHT_TABLE_NAME RIGHT_COLUMN_NAME  RIGHT_TABLE_ROW_COUNT  OUTPUT_ROW_COUNT  \n",
       "50         lineitem         l_partkey                    192               489  \n",
       "51             part         p_partkey                  29935            119740  \n",
       "52         supplier         s_suppkey                      4            119707  \n",
       "53             part         p_partkey                  29935            119707  \n",
       "54             part         p_partkey                  29437            117748  \n",
       "55         supplier         s_suppkey                      4            117697  \n",
       "56             part         p_partkey                  29437            117697  \n",
       "58         supplier         s_suppkey                  10000                 1  \n",
       "60         supplier         s_suppkey                  10000                 1  \n",
       "62         supplier         s_suppkey                  10000                 1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### JOINS ###\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def load_join_statistics():\n",
    "    def line_looks_suspicious(row):\n",
    "        right_table_name = row['RIGHT_TABLE_NAME']    \n",
    "        if pd.isnull(right_table_name):\n",
    "            pass\n",
    "        elif row['RIGHT_TABLE_ROW_COUNT'] > table_sizes[row['RIGHT_TABLE_NAME']]:\n",
    "            return True\n",
    "\n",
    "        left_table_name = row['LEFT_TABLE_NAME']\n",
    "        if pd.isnull(left_table_name):\n",
    "            pass\n",
    "        elif row['LEFT_TABLE_ROW_COUNT'] > table_sizes[row['LEFT_TABLE_NAME']]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def validate_joins(joins):\n",
    "        is_suspicious = joins.apply(line_looks_suspicious, axis=1)\n",
    "        suspicious_joins = joins[is_suspicious]\n",
    "        assert len(suspicious_joins) < 3, f\"there are {len(suspicious_joins)} suspicious joins:\\n{suspicious_joins[['JOIN_MODE', 'LEFT_TABLE_NAME', 'LEFT_COLUMN_NAME', 'LEFT_TABLE_ROW_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_COLUMN_NAME', 'RIGHT_TABLE_ROW_COUNT', 'OUTPUT_ROW_COUNT']]}\"\n",
    "    \n",
    "    joins = pd.read_csv(f\"{STATISTICS_PATH}/joins.csv\", sep='|')\n",
    "    joins = joins.dropna()\n",
    "    joins['PROBE_TABLE'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_NAME\"] if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['PROBE_COLUMN'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_COLUMN_NAME\"] if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['PROBE_TABLE_ROW_COUNT'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_ROW_COUNT\"]if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\" , axis=1)\n",
    "    joins['BUILD_TABLE'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_NAME\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['BUILD_COLUMN'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_COLUMN_NAME\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['BUILD_TABLE_ROW_COUNT'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_ROW_COUNT\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    validate_joins(joins)\n",
    "    \n",
    "    \n",
    "    # TODOS\n",
    "    \n",
    "    # code: export which side is probe\n",
    "    # model: add columns for probe and build table size\n",
    "    # model: use more precise values for build and probe materialization\n",
    "        \n",
    "    \n",
    "    \n",
    "    #TODO code: activate higher cache size    \n",
    "                                                                                           \n",
    "    return joins\n",
    "\n",
    "load_join_statistics().iloc[50:60][['JOIN_MODE', 'LEFT_TABLE_NAME', 'LEFT_COLUMN_NAME', 'LEFT_TABLE_ROW_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_COLUMN_NAME', 'RIGHT_TABLE_ROW_COUNT', 'OUTPUT_ROW_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract interesting table names\n",
    "# Currently fixed to columns which are scanned or used in joins\n",
    "\n",
    "def get_table_names(table_scans, joins):\n",
    "    scan_tables = set(table_scans['TABLE_NAME'].unique())\n",
    "    left_join_tables = set(joins['LEFT_TABLE_NAME'].unique())\n",
    "    right_join_tables = set(joins['RIGHT_TABLE_NAME'].unique())    \n",
    "\n",
    "    return scan_tables.union(left_join_tables.union(right_join_tables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel:\n",
    "    \n",
    "    def __init__(self, query_frequencies, table_name, table_scans, correlations={}):\n",
    "        self.query_frequencies = query_frequencies\n",
    "        self.table_name = table_name\n",
    "        self.table_scans = table_scans\n",
    "        self.correlations = correlations\n",
    "        \n",
    "    def query_frequency(self, query_hash):\n",
    "        return self.query_frequencies[query_hash]\n",
    "        \n",
    "    def extract_scan_columns(self):\n",
    "        useful_scans = self.table_scans[self.table_scans['useful_for_pruning']]\n",
    "        interesting_scan_columns = list(useful_scans['COLUMN_NAME'].unique())\n",
    "        \n",
    "        return interesting_scan_columns\n",
    "    \n",
    "    def extract_join_columns(self):\n",
    "        interesting_join_probe_columns = list(self.joins[self.joins['PROBE_TABLE'] == self.table_name]['PROBE_COLUMN'].unique())\n",
    "        interesting_join_build_columns = list(self.joins[self.joins['BUILD_TABLE'] == self.table_name]['BUILD_COLUMN'].unique())        \n",
    "        \n",
    "        return self.uniquify(interesting_join_probe_columns + interesting_join_build_columns)\n",
    "    \n",
    "    def extract_interesting_columns(self):        \n",
    "        return self.uniquify(self.extract_scan_columns() + self.extract_join_columns())\n",
    "    \n",
    "    def round_up_to_next_multiple(self, number_to_round, base_for_multiple):\n",
    "        quotient = number_to_round // base_for_multiple\n",
    "        if number_to_round % base_for_multiple != 0:\n",
    "            quotient += 1\n",
    "        return quotient * base_for_multiple        \n",
    "\n",
    "    def uniquify(self, seq):\n",
    "            seen = set()\n",
    "            return [x for x in seq if not (x in seen or seen.add(x))]    \n",
    "    \n",
    "    # return a list of possible clusterings\n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old partitioner model.\n",
    "# TODO: refactor to inherit from SingleTableMDCModel (further below). Not sure how much effort should go into this model, since it is not designed for the \"main clustering algorithm\"\n",
    "\n",
    "class PartitionerModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, query_frequencies, table_name, table_scans, table_size, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(query_frequencies, table_name, table_scans, correlations)\n",
    "        self.table_size = table_size\n",
    "        self.distinct_values = distinct_values\n",
    "        self.target_chunksize = target_chunksize\n",
    "        self.joins = joins\n",
    "        self.sorted_columns_during_creation = sorted_columns_during_creation\n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        print(interesting_columns)\n",
    "        \n",
    "        clustering_columns = itertools.product(interesting_columns, interesting_columns)\n",
    "        #clustering_columns = itertools.product(interesting_columns, interesting_columns, interesting_columns)\n",
    "        clustering_columns = filter(lambda x: x[0] <= x[1], clustering_columns)\n",
    "        #clustering_columns = filter(lambda x: x[1] <= x[2], clustering_columns)\n",
    "        clustering_columns = [self.uniquify(clustering) for clustering in clustering_columns]\n",
    "        sort_columns = interesting_columns        \n",
    "        clusterings_with_runtimes = reduce(lambda x,y: x+y,[self.estimate_total_runtime(clustering_cols, sort_columns) for clustering_cols in clustering_columns])\n",
    "        clusterings_with_runtimes.sort(key=lambda x: x[2], reverse=False)\n",
    "        \n",
    "        return clusterings_with_runtimes[0:first_k]\n",
    "        \n",
    "    def estimate_table_scan_runtimes(self, clustering_columns, sorting_columns, split_factors, total_runtimes):        \n",
    "        def compute_unprunable_parts(row, split_factors):\n",
    "            def clustering_columns_correlated_to(column):\n",
    "                return [clustering_column for clustering_column in clustering_columns if column in self.correlations.get(clustering_column, {})]\n",
    "            \n",
    "            def correlates_to_clustering_column(column):\n",
    "                return len(clustering_columns_correlated_to(column)) > 0\n",
    "\n",
    "            column_name = row['COLUMN_NAME']\n",
    "\n",
    "            if not row['useful_for_pruning']:\n",
    "                selectivity = 1\n",
    "            elif column_name in clustering_columns:\n",
    "                scan_selectivity = row['selectivity']\n",
    "                split_factor = split_factors[clustering_columns.index(column_name)]\n",
    "                selectivity =  self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor)\n",
    "            elif correlates_to_clustering_column(column_name):\n",
    "                scan_selectivity = row['selectivity']\n",
    "                correlated_clustering_columns = clustering_columns_correlated_to(column_name)\n",
    "                \n",
    "                # ToDo this is hacky, but for now assume there is just one correlated column\n",
    "                assert len(correlated_clustering_columns) == 1, f\"expected just 1 correlated clustering column, but got {len(correlated_clustering_columns)}\"\n",
    "                \n",
    "                split_factor = split_factors[clustering_columns.index(correlated_clustering_columns[0])]\n",
    "                selectivity = min(1, 1.2 * self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor))\n",
    "            else:\n",
    "                selectivity = 1\n",
    "            \n",
    "            return selectivity\n",
    "        \n",
    "        def compute_runtimes(row, sorting_column):\n",
    "            assert row['estimated_input_rows'] > 1, row\n",
    "            assert row['runtime_per_input_row'] > 0, row\n",
    "            assert row['runtime_per_output_row'] > 0, row\n",
    "            input_row_count = row['estimated_input_rows']\n",
    "            \n",
    "            if row['COLUMN_NAME'] == sorting_column and row['benefits_from_sorting']:\n",
    "                # TODO is this the best way to simulate sorted access?\n",
    "                input_row_count = np.log2(input_row_count)\n",
    "\n",
    "            runtime = input_row_count * row['runtime_per_input_row'] + row['OUTPUT_ROW_COUNT'] * row['runtime_per_output_row']\n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        scans_per_query = self.table_scans.sort_values(['INPUT_ROW_COUNT'], ascending=False).groupby(['QUERY_HASH', 'GET_TABLE_HASH'])\n",
    "        for _, scans in scans_per_query:\n",
    "            number_of_scans = len(scans)\n",
    "            assert number_of_scans > 0 and number_of_scans < 25, f\"weird scan length: {number_of_scans}\\nScans:\\n{scans}\"\n",
    "            # TODO: kinda unrealistic assumption: everything not in the table scan result can be pruned\n",
    "                          \n",
    "            unprunable_parts = scans.apply(compute_unprunable_parts, axis=1, args=(split_factors,))\n",
    "            unprunable_part = unprunable_parts.product()\n",
    "            assert unprunable_part > 0, \"no unprunable part\"\n",
    "            \n",
    "            estimated_pruned_table_size = self.round_up_to_next_multiple(unprunable_part * self.table_size, CHUNK_SIZE)\n",
    "            \n",
    "            runtimes = pd.DataFrame()\n",
    "            runtimes['QUERY_HASH'] = scans['QUERY_HASH']\n",
    "            runtimes['runtime_per_input_row'] = scans['time_per_input_row']\n",
    "            runtimes['runtime_per_output_row'] = scans['time_per_output_row']\n",
    "            runtimes['COLUMN_NAME'] = scans['COLUMN_NAME']\n",
    "            runtimes['benefits_from_sorting'] = scans['benefits_from_sorting']\n",
    "            # the pruned table inputs should be reflected in 'estimated_input_rows'\n",
    "            runtimes['estimated_input_rows'] = scans.apply(lambda x: x['INPUT_ROW_COUNT'], axis=1)\n",
    "            runtimes['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "            runtimes.iloc[0, runtimes.columns.get_loc('estimated_input_rows')] = estimated_pruned_table_size                                    \n",
    "            assert runtimes['estimated_input_rows'].iloc[0] == estimated_pruned_table_size, f\"value is {runtimes.iloc[0]['estimated_input_rows']}, but should be {estimated_pruned_table_size}\"\n",
    "            # TODO modify input sizes of subsequent scans\n",
    "            \n",
    "            for sorting_column in sorting_columns:\n",
    "                scan_runtimes = runtimes.apply(compute_runtimes, axis=1, args=(sorting_column,))\n",
    "                total_runtimes[sorting_column] += scan_runtimes.sum()\n",
    "\n",
    "    def estimate_join_runtimes(self, clustering_columns, sorting_columns, total_runtimes):                \n",
    "        def estimate_join_runtime(row, sorting_column):\n",
    "                        \n",
    "            if \"JoinHash\" in row['DESCRIPTION']:\n",
    "                probe_column = row['PROBE_COLUMN']\n",
    "                if row['PROBE_TABLE'] == self.table_name:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    probe_column_is_sorted = row['PROBE_SORTED'] and probe_column == sorting_column\n",
    "                    probe_column_is_clustered = row['PROBE_SORTED'] and probe_column in clustering_columns\n",
    "                else:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(row['PROBE_TABLE'], {})\n",
    "                    probe_column_is_sorted = probe_column_was_sorted\n",
    "                    probe_column_is_clustered = probe_column_was_sorted\n",
    "                    \n",
    "                build_column = row['BUILD_COLUMN']\n",
    "                if row['BUILD_TABLE'] == self.table_name:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    build_column_is_sorted = row['BUILD_SORTED'] and build_column == sorting_column\n",
    "                    build_column_is_clustered = row['BUILD_SORTED'] and build_column in clustering_columns\n",
    "                else:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(row['BUILD_TABLE'], {})\n",
    "                    build_column_is_sorted = build_column_was_sorted\n",
    "                    build_column_is_clustered = build_column_was_sorted\n",
    "\n",
    "                time_materialize = row['BUILD_SIDE_MATERIALIZING_NS'] + row['PROBE_SIDE_MATERIALIZING_NS']\n",
    "                \n",
    "                probe_weight = 2\n",
    "                build_weight = 2\n",
    "                if probe_column_was_sorted:\n",
    "                    probe_weight = 1\n",
    "                if build_column_was_sorted:\n",
    "                    build_weight = 1\n",
    "                \n",
    "                \n",
    "                probe_table_size = row['PROBE_TABLE_ROW_COUNT']\n",
    "                build_table_size = row['BUILD_TABLE_ROW_COUNT']\n",
    "                total_table_size = probe_weight * probe_table_size + build_weight * build_table_size\n",
    "                \n",
    "                time_materialize_probe = time_materialize * (probe_weight * probe_table_size / total_table_size)\n",
    "                time_materialize_build = time_materialize - time_materialize_probe\n",
    "                \n",
    "                \n",
    "                def get_materialize_factor(was_sorted, is_sorted, is_clustered):\n",
    "                    materialize_factor = 1\n",
    "                    if is_sorted and is_clustered:\n",
    "                        if not was_sorted:\n",
    "                            materialize_factor = 0.5\n",
    "                        else:\n",
    "                            materialize_factor = 1\n",
    "                    elif is_sorted or is_clustered:\n",
    "                        if not was_sorted:\n",
    "                            materialize_factor = 0.55\n",
    "                        else:\n",
    "                            materialize_factor = 1.1\n",
    "                    elif was_sorted:\n",
    "                        # probe column is now neither sorted nor clustered\n",
    "                        materialize_factor = 2\n",
    "                    else:\n",
    "                        # default case: was not sorted before, and is neither sorted nor clustered now. No change\n",
    "                        materialize_factor = 1\n",
    "                        \n",
    "                    return materialize_factor\n",
    "                \n",
    "                materialize_probe_factor = get_materialize_factor(probe_column_was_sorted, probe_column_is_sorted, probe_column_is_clustered)\n",
    "                materialize_build_factor = get_materialize_factor(build_column_was_sorted, build_column_is_sorted, build_column_is_clustered)\n",
    "                \n",
    "                time_materialize = time_materialize_probe * materialize_probe_factor + time_materialize_build *  materialize_build_factor\n",
    "                \n",
    "\n",
    "                # unchanged\n",
    "                time_cluster = row['CLUSTERING_NS']\n",
    "                \n",
    "                # unchanged\n",
    "                time_build = row['BUILDING_NS']\n",
    "                \n",
    "                            \n",
    "                time_probe = row['PROBING_NS']\n",
    "                probe_factor = 1\n",
    "                if probe_column_is_sorted and probe_column_is_clustered:\n",
    "                    if not probe_column_was_sorted:\n",
    "                        probe_factor = 0.7\n",
    "                    else:\n",
    "                        probe_factor = 1\n",
    "                elif probe_column_is_sorted or probe_column_is_clustered:\n",
    "                    if not probe_column_was_sorted:\n",
    "                        probe_factor = 0.9\n",
    "                    else:\n",
    "                        probe_factor = 1.1\n",
    "                elif probe_column_was_sorted:\n",
    "                    # probe column is now neither sorted nor clustered\n",
    "                    probe_factor = 1.4\n",
    "                \n",
    "                time_probe *= probe_factor                \n",
    "                \n",
    "                # unchanged\n",
    "                time_write_output = row['OUTPUT_WRITING_NS']\n",
    "                \n",
    "                \n",
    "                \n",
    "                # TODO: how to deal with the difference between RUNTIME_NS and sum(stage_runtimes)?\n",
    "                runtime = time_materialize + time_cluster + time_build + time_probe + time_write_output\n",
    "            else:\n",
    "                runtime = row['RUNTIME_NS']\n",
    "                \n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        for sorting_column in sorting_columns:\n",
    "            join_runtimes = self.joins.apply(estimate_join_runtime, axis=1, args=(sorting_column,))\n",
    "            total_runtimes[sorting_column] += join_runtimes.sum()\n",
    "                \n",
    "    def estimate_total_runtime(self, clustering_columns, sorting_columns):\n",
    "        #print(f\"testing clustering {clustering_columns} with sorting columns {sorting_columns}\")\n",
    "        split_factors = self.determine_split_factors(clustering_columns)            \n",
    "        total_runtimes = {sorting_column: 0 for sorting_column in sorting_columns}\n",
    "        self.estimate_table_scan_runtimes(clustering_columns, sorting_columns, split_factors, total_runtimes)\n",
    "        self.estimate_join_runtimes(clustering_columns, sorting_columns, total_runtimes)\n",
    "        \n",
    "        clusterings = [[list(zip(clustering_columns, split_factors)), sorting_column, np.int64(total_runtimes[sorting_column])] for sorting_column in sorting_columns]\n",
    "        return clusterings\n",
    "    \n",
    "    def determine_split_factors(self, clustering_columns):\n",
    "        approximate_split_factor = self.table_size / self.target_chunksize\n",
    "        individual_distinct_values = [self.distinct_values[column] for column in clustering_columns]        \n",
    "        log_distinct_values = [math.ceil(0.5+np.log2(x)) for x in individual_distinct_values]\n",
    "        log_distinct_values_product = reduce(operator.mul, log_distinct_values, 1)\n",
    "        assert log_distinct_values_product > 0, \"cannot have a distinct value count of 0\"\n",
    "        \n",
    "        global_modification_factor = approximate_split_factor / log_distinct_values_product\n",
    "        num_dimensions = len(clustering_columns)\n",
    "        individual_modification_factor = np.power(global_modification_factor, 1.0 / num_dimensions)    \n",
    "        split_factors = [math.ceil(x * individual_modification_factor) for x in log_distinct_values]\n",
    "        \n",
    "        # testing\n",
    "        actual_split_factor = reduce(operator.mul, split_factors, 1)\n",
    "        assert actual_split_factor > 0, \"there was a split up factor of 0\"\n",
    "        estimated_chunksize = self.table_size / actual_split_factor\n",
    "        assert estimated_chunksize <= self.target_chunksize, \"chunks should be smaller, not larger than target_chunksize\"\n",
    "        allowed_percentage = 0.55\n",
    "        if estimated_chunksize < allowed_percentage * self.target_chunksize:\n",
    "            print(f\"Warning: chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\")\n",
    "        #assert estimated_chunksize >= allowed_percentage * self.target_chunksize, f\"chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\"\n",
    "        \n",
    "        return split_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def extract_probe_side_joins(joins, table_name):\n",
    "    return joins[joins['PROBE_TABLE'] == table_name]\n",
    "\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 92 * SCALE_FACTOR]],\n",
    "            'orders': [['o_orderdate', 23 * SCALE_FACTOR]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "def create_benchmark_configs():\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    clusterings = {\"default\" : default_benchmark_config()}\n",
    "    query_frequencies = get_query_frequencies()\n",
    "    \n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    for table_name in table_names:\n",
    "        start_time_table = datetime.now()\n",
    "        single_table_scans = extract_single_table(scans, table_name)\n",
    "        probe_side_joins = joins#extract_probe_side_joins(joins, table_name)\n",
    "        table_size = table_sizes[table_name]\n",
    "        if table_size <= 3 * CHUNK_SIZE:\n",
    "            print(f\"Not computing clustering for {table_name}, as it has only {table_size} rows\")\n",
    "            continue\n",
    "\n",
    "        model = PartitionerModel(query_frequencies, table_name, single_table_scans, table_size, distinct_values[table_name], CHUNK_SIZE, correlations.get(table_name, {}), probe_side_joins, sorted_columns_during_creation)\n",
    "        table_clusterings = model.suggest_clustering(3)\n",
    "        for table_clustering in table_clusterings:\n",
    "            config = default_benchmark_config()\n",
    "            config[table_name] = format_table_clustering(table_clustering)\n",
    "            config_name = get_config_name(config)\n",
    "            clusterings[config_name] = config\n",
    "        end_time_table = datetime.now()\n",
    "        print(f\"Done computing clustering for {table_name} ({end_time_table - start_time_table})\")\n",
    "\n",
    "            \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Computed all clusterings in {end_time - start_time}\")\n",
    "    \n",
    "    return clusterings\n",
    "\n",
    "#create_benchmark_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTableMdcModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(query_frequencies, table_name, table_scans, correlations)\n",
    "        self.max_dimensions = max_dimensions\n",
    "        self.table_sizes = table_sizes       \n",
    "        self.table_size = table_sizes[table_name]\n",
    "        self.distinct_values = distinct_values\n",
    "        self.target_chunksize = target_chunksize\n",
    "        self.joins = joins\n",
    "        self.sorted_columns_during_creation = sorted_columns_during_creation\n",
    "        \n",
    "        self.join_column_names = self.extract_join_columns()\n",
    "        self.scan_column_names = self.extract_scan_columns()\n",
    "        self.scan_estimates = pd.DataFrame()\n",
    "        self.scan_estimates['QUERY_HASH'] = self.table_scans['QUERY_HASH']        \n",
    "        self.scan_estimates['DESCRIPTION'] = self.table_scans['DESCRIPTION']\n",
    "        self.scan_estimates['RUNTIME_ESTIMATE'] = np.array([-1] * len(self.table_scans))\n",
    "        self.scan_estimates['RUNTIME_NS'] = self.table_scans['RUNTIME_NS'] # TODO: probably not necessary?\n",
    "        #self.scan_estimates.index = self.table_scans.index\n",
    "        \n",
    "    def is_join_column(self, column_name):\n",
    "        return column_name in self.join_column_names\n",
    "    \n",
    "    def is_scan_column(self, column_name):\n",
    "        return column_name in self.scan_column_names\n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        print(interesting_columns)\n",
    "        clustering_columns = itertools.combinations_with_replacement(interesting_columns, self.max_dimensions)\n",
    "        clustering_columns = [self.uniquify(clustering) for clustering in clustering_columns]\n",
    "        sort_columns = interesting_columns        \n",
    "        clusterings_with_runtimes = reduce(lambda x,y: x+y,[self.estimate_total_runtimes(clustering_cols, sort_columns) for clustering_cols in clustering_columns])\n",
    "        clusterings_with_runtimes.sort(key=lambda x: x[2], reverse=False)\n",
    "        \n",
    "        return clusterings_with_runtimes[0:first_k]\n",
    "    \n",
    "    \n",
    "    def estimate_distinct_values_per_chunk(self, column, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        raise NotImplementedError(\"Each model should provide this function\")\n",
    "        \n",
    "    def estimate_distinct_values_per_chunk_at_statistics_time(self, column, table):        \n",
    "        if column in self.sorted_columns_during_creation.get(table, {}):\n",
    "            # Column was globally sorted\n",
    "            average_count_per_distinct_value = self.table_sizes[table] / self.distinct_values[table][column]\n",
    "            return math.ceil(self.target_chunksize / average_count_per_distinct_value)\n",
    "        else:\n",
    "            # Column was not globally sorted\n",
    "            total_distinct_values = self.distinct_values[table][column]\n",
    "            return min(total_distinct_values, self.target_chunksize)        \n",
    "    \n",
    "    def estimate_table_scan_runtime(self, clustering_columns, sorting_column, split_factors):\n",
    "        def compute_unprunable_parts(row, split_factors):\n",
    "            def clustering_columns_correlated_to(column):\n",
    "                return [clustering_column for clustering_column in clustering_columns if column in self.correlations.get(clustering_column, {})]\n",
    "            \n",
    "            def correlates_to_clustering_column(column):\n",
    "                return len(clustering_columns_correlated_to(column)) > 0\n",
    "\n",
    "            column_name = row['COLUMN_NAME']\n",
    "\n",
    "            if not row['useful_for_pruning']:\n",
    "                selectivity = 1\n",
    "            elif column_name in clustering_columns:\n",
    "                scan_selectivity = row['selectivity']\n",
    "                split_factor = split_factors[clustering_columns.index(column_name)]\n",
    "                selectivity =  self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor)\n",
    "            elif correlates_to_clustering_column(column_name):\n",
    "                scan_selectivity = row['selectivity']\n",
    "                correlated_clustering_columns = clustering_columns_correlated_to(column_name)\n",
    "                \n",
    "                # ToDo this is hacky, but for now assume there is just one correlated column\n",
    "                assert len(correlated_clustering_columns) == 1, f\"expected just 1 correlated clustering column, but got {len(correlated_clustering_columns)}\"\n",
    "                \n",
    "                split_factor = split_factors[clustering_columns.index(correlated_clustering_columns[0])]\n",
    "                selectivity = min(1, 1.2 * self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor))\n",
    "            else:\n",
    "                selectivity = 1\n",
    "            \n",
    "            return selectivity\n",
    "        \n",
    "        def compute_tablescan_runtime(row, sorting_column):\n",
    "            assert row['estimated_input_rows'] > 1, row\n",
    "            assert row['runtime_per_input_row'] > 0, row\n",
    "            assert row['runtime_per_output_row'] > 0, row\n",
    "            input_row_count = row['estimated_input_rows']\n",
    "            \n",
    "            if row['COLUMN_NAME'] == sorting_column and row['benefits_from_sorting']:\n",
    "                # TODO is this the best way to simulate sorted access?\n",
    "                input_row_count = np.log2(input_row_count)\n",
    "\n",
    "            runtime = input_row_count * row['runtime_per_input_row'] + row['OUTPUT_ROW_COUNT'] * row['runtime_per_output_row']\n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        runtime = 0\n",
    "        scans_per_query = self.table_scans.sort_values(['INPUT_ROW_COUNT'], ascending=False).groupby(['QUERY_HASH', 'GET_TABLE_HASH'])\n",
    "        for _, scans in scans_per_query:\n",
    "            number_of_scans = len(scans)\n",
    "            assert number_of_scans > 0 and number_of_scans < 25, f\"weird scan length: {number_of_scans}\\nScans:\\n{scans}\"\n",
    "            # TODO: kinda unrealistic assumption: everything not in the table scan result can be pruned\n",
    "                          \n",
    "            unprunable_parts = scans.apply(compute_unprunable_parts, axis=1, args=(split_factors,))            \n",
    "            unprunable_part = unprunable_parts.product()\n",
    "            assert unprunable_part > 0, \"no unprunable part\"\n",
    "            \n",
    "            estimated_pruned_table_size = self.round_up_to_next_multiple(unprunable_part * self.table_size, CHUNK_SIZE)\n",
    "            \n",
    "            runtimes = pd.DataFrame()\n",
    "            runtimes['QUERY_HASH'] = scans['QUERY_HASH']\n",
    "            runtimes['runtime_per_input_row'] = scans['time_per_input_row']\n",
    "            runtimes['runtime_per_output_row'] = scans['time_per_output_row']\n",
    "            runtimes['COLUMN_NAME'] = scans['COLUMN_NAME']\n",
    "            runtimes['benefits_from_sorting'] = scans['benefits_from_sorting']\n",
    "            # the pruned table inputs should be reflected in 'estimated_input_rows'\n",
    "            runtimes['estimated_input_rows'] = scans['INPUT_ROW_COUNT']\n",
    "            runtimes['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "            runtimes.iloc[0, runtimes.columns.get_loc('estimated_input_rows')] = estimated_pruned_table_size                                    \n",
    "            assert runtimes['estimated_input_rows'].iloc[0] == estimated_pruned_table_size, f\"value is {runtimes.iloc[0]['estimated_input_rows']}, but should be {estimated_pruned_table_size}\"\n",
    "            # TODO modify input sizes of subsequent scans\n",
    "            \n",
    "            scan_runtimes = runtimes.apply(compute_tablescan_runtime, axis=1, args=(sorting_column,))\n",
    "            #self.table_scans.iloc[:, self.table_scans.columns.get_loc('RUNTIME_ESTIMATE')] = scan_runtimes\n",
    "            self.scan_estimates.loc[scan_runtimes.index, 'RUNTIME_ESTIMATE'] = scan_runtimes\n",
    "            runtime += scan_runtimes.sum()\n",
    "        return runtime\n",
    "\n",
    "    def estimate_chunk_count(self, scans, clustering_columns, dimension_cardinalities):\n",
    "        raise NotImplementedError(\"Subclass responsibility\")\n",
    "    \n",
    "    def get_chunk_count_factor(self, row, side, clustering_columns, dimension_cardinalities):\n",
    "        query_hash = row['QUERY_HASH']\n",
    "        if side == \"PROBE\":\n",
    "            input_rows = row['PROBE_TABLE_ROW_COUNT']\n",
    "            assert row['PROBE_TABLE'] == self.table_name, \"Call this function only for the own table\"\n",
    "            \n",
    "            # When joining tables, the table size might increase (a lot). This makes it hard to estimate the chunk count, so just ignore it\n",
    "            if input_rows > self.table_size:\n",
    "                return 1\n",
    "        elif side == \"BUILD\":\n",
    "            input_rows = row['BUILD_TABLE_ROW_COUNT']\n",
    "            assert row['BUILD_TABLE'] == self.table_name, \"Call this function only for the own table\"\n",
    "            # When joining tables, the table size might increase (a lot). This makes it hard to estimate the chunk count, so just ignore it\n",
    "            if input_rows > self.table_size:\n",
    "                return 1\n",
    "        else:\n",
    "            raise ValueError(\"side must be PROBE or BUILD\")\n",
    "\n",
    "        expected_chunk_count = self.estimate_chunk_count(query_hash, input_rows, clustering_columns, dimension_cardinalities)\n",
    "        min_chunk_count = math.ceil(input_rows / self.target_chunksize)\n",
    "        max_chunk_count = math.ceil(self.table_size / self.target_chunksize)\n",
    "\n",
    "        CHUNK_COUNT_SPEEDUP_LOW = 3\n",
    "        CHUNK_COUNT_SPEEDUP_HIGH = 1\n",
    "\n",
    "        current_speedup = self.interpolate(CHUNK_COUNT_SPEEDUP_LOW, CHUNK_COUNT_SPEEDUP_HIGH, (expected_chunk_count - min_chunk_count) / max_chunk_count)\n",
    "        #print(f\"current speedup: {current_speedup}\")\n",
    "\n",
    "        old_clustering_columns = self.sorted_columns_during_creation[self.table_name]\n",
    "        old_dimension_cardinalities = [self.statistic_time_dimension_cardinalities()] * len(old_clustering_columns)\n",
    "        old_expected_chunk_count = self.estimate_chunk_count(query_hash, input_rows, old_clustering_columns, old_dimension_cardinalities)\n",
    "        old_speedup = self.interpolate(CHUNK_COUNT_SPEEDUP_LOW, CHUNK_COUNT_SPEEDUP_HIGH, (old_expected_chunk_count - min_chunk_count) / max_chunk_count)\n",
    "        #print(f\"old speedup: {old_speedup}\")\n",
    "\n",
    "        return 1 * old_speedup / current_speedup\n",
    "    \n",
    "    def estimate_join_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        def compute_join_runtime(row, sorting_column):\n",
    "            if \"JoinHash\" in row['DESCRIPTION']:\n",
    "                probe_column = row['PROBE_COLUMN']\n",
    "                if row['PROBE_TABLE'] == self.table_name:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    probe_column_is_sorted = row['PROBE_SORTED'] and probe_column == sorting_column\n",
    "                    materialize_probe_factor = self.get_chunk_count_factor(row, \"PROBE\", clustering_columns, dimension_cardinalities)\n",
    "                else:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(row['PROBE_TABLE'], {})\n",
    "                    probe_column_is_sorted = probe_column_was_sorted\n",
    "                    materialize_probe_factor = 1\n",
    "                    \n",
    "                build_column = row['BUILD_COLUMN']\n",
    "                if row['BUILD_TABLE'] == self.table_name:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    build_column_is_sorted = row['BUILD_SORTED'] and build_column == sorting_column\n",
    "                    materialize_build_factor = self.get_chunk_count_factor(row, \"BUILD\", clustering_columns, dimension_cardinalities)\n",
    "                else:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(row['BUILD_TABLE'], {})\n",
    "                    build_column_is_sorted = build_column_was_sorted\n",
    "                    materialize_build_factor = 1\n",
    "\n",
    "                time_materialize_probe = row['PROBE_SIDE_MATERIALIZING_NS']\n",
    "                time_materialize_build = row['BUILD_SIDE_MATERIALIZING_NS']                \n",
    "                \n",
    "                def get_materialize_factor(column, was_globally_sorted, is_sorted, expected_distinct_value_count):\n",
    "                    # Assumption: \"was_sorted\" implies global sortedness, i.e., both clustering and chunkwise sorting\n",
    "                    # This is true when the clustering produced by the table generator is used by the plan cache exporter\n",
    "                    # If the data has been re-clustered before the plan cache exporter runs, there has to be some system inside Hyrise which tracks the current clustering config\n",
    "                    \n",
    "                    # Sortedness seems to yield a speed up of approx. 1.6, regardless of the number of distinct values\n",
    "                    SORT_SPEEDUP = 1.6\n",
    "                    sortedness_factor = 1                    \n",
    "                    was_sorted = was_globally_sorted\n",
    "                    if was_sorted:\n",
    "                        sortedness_factor *= SORT_SPEEDUP\n",
    "                    if is_sorted:\n",
    "                        sortedness_factor /= SORT_SPEEDUP\n",
    "                    \n",
    "\n",
    "                    # The influence of clustering depends on the number of distinct values\n",
    "                    CLUSTERING_SPEEDUP_LOW = 1.84\n",
    "                    CLUSTERING_SPEEDUP_HIGH = 1\n",
    "                    clustering_factor = 1\n",
    "                    statistics_time_distinct_value_count = self.estimate_distinct_values_per_chunk_at_statistics_time(column, self.table_name);\n",
    "                    clustering_factor *= self.interpolate(CLUSTERING_SPEEDUP_LOW, CLUSTERING_SPEEDUP_HIGH, statistics_time_distinct_value_count / self.target_chunksize)\n",
    "                    clustering_factor /= self.interpolate(CLUSTERING_SPEEDUP_LOW, CLUSTERING_SPEEDUP_HIGH, expected_distinct_value_count / self.target_chunksize)\n",
    "                        \n",
    "                    return sortedness_factor * clustering_factor                \n",
    "                \n",
    "                if row['PROBE_TABLE'] == self.table_name and row['PROBE_SORTED']:\n",
    "                    expected_distinct_values_probe = self.estimate_distinct_values_per_chunk(probe_column, clustering_columns, sorting_column, dimension_cardinalities)\n",
    "                    materialize_probe_factor *= get_materialize_factor(probe_column, probe_column_was_sorted, probe_column_is_sorted, expected_distinct_values_probe)\n",
    "                    \n",
    "                if row['BUILD_TABLE'] == self.table_name and row['BUILD_SORTED']:\n",
    "                    expected_distinct_values_build = self.estimate_distinct_values_per_chunk(build_column, clustering_columns, sorting_column, dimension_cardinalities)\n",
    "                    materialize_build_factor *= get_materialize_factor(build_column, build_column_was_sorted, build_column_is_sorted, expected_distinct_values_build)                \n",
    "                \n",
    "                time_materialize = time_materialize_probe * materialize_probe_factor + time_materialize_build *  materialize_build_factor\n",
    "\n",
    "\n",
    "                # unchanged\n",
    "                time_cluster = row['CLUSTERING_NS']\n",
    "                \n",
    "                # unchanged\n",
    "                time_build = row['BUILDING_NS']\n",
    "                \n",
    "                            \n",
    "                time_probe = row['PROBING_NS']\n",
    "                probe_factor = 1\n",
    "                #if probe_column_is_sorted and probe_column_is_clustered:\n",
    "                #    if not probe_column_was_sorted:\n",
    "                #        probe_factor = 0.7\n",
    "                #    else:\n",
    "                #        probe_factor = 1\n",
    "                #elif probe_column_is_sorted or probe_column_is_clustered:\n",
    "                #    if not probe_column_was_sorted:\n",
    "                #        probe_factor = 0.9\n",
    "                #    else:\n",
    "                #        probe_factor = 1.1\n",
    "                #elif probe_column_was_sorted:\n",
    "                #    # probe column is now neither sorted nor clustered\n",
    "                #    probe_factor = 1.4\n",
    "                \n",
    "                time_probe *= probe_factor                \n",
    "                \n",
    "                # unchanged\n",
    "                time_write_output = row['OUTPUT_WRITING_NS']\n",
    "                \n",
    "                \n",
    "                \n",
    "                # TODO: how to deal with the difference between RUNTIME_NS and sum(stage_runtimes)?\n",
    "                runtime = time_materialize + time_cluster + time_build + time_probe + time_write_output\n",
    "            else:\n",
    "                runtime = row['RUNTIME_NS']\n",
    "                \n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        join_runtimes = self.joins.apply(compute_join_runtime, axis=1, args=(sorting_column,))\n",
    "        return join_runtimes.sum()\n",
    "                \n",
    "    def estimate_total_runtimes(self, clustering_columns, sorting_columns):\n",
    "        #print(f\"testing clustering {clustering_columns} with sorting columns {sorting_columns}\")\n",
    "        dimension_cardinalities = self.get_dimension_cardinalities(clustering_columns)\n",
    "        total_runtimes = {sorting_column: 0 for sorting_column in sorting_columns}\n",
    "        clusterings = []\n",
    "        for sorting_column in sorting_columns:\n",
    "            runtime = self.estimate_total_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "            clusterings.append([list(zip(clustering_columns, dimension_cardinalities)), sorting_column, runtime])\n",
    "        \n",
    "        #clusterings = [[list(zip(clustering_columns, dimension_cardinalities)), sorting_column, np.int64(total_runtimes[sorting_column])] for sorting_column in sorting_columns]\n",
    "        return clusterings\n",
    "    \n",
    "    def estimate_total_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        runtime = 0\n",
    "        runtime += self.estimate_table_scan_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "        runtime += self.estimate_join_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "        \n",
    "        return runtime\n",
    "    \n",
    "    def get_dimension_cardinalities(self, clustering_columns):\n",
    "        raise NotImplementedError(\"Subclasses must override this function\")\n",
    "        \n",
    "    def statistic_time_dimension_cardinalities(self):\n",
    "        raise NotImplementedError(\"Subclasses must override this function\")\n",
    "        \n",
    "    def interpolate(self, low, high, percentage):\n",
    "        assert percentage >= 0 and percentage <= 1, f\"percentage must between 0 and 1, but is {percentage}\"\n",
    "        return (1 - percentage) * low + (percentage * high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisjointClustersModel(SingleTableMdcModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation)\n",
    "        \n",
    "    # This function should only be called for the own table, not for others\n",
    "    def estimate_distinct_values_per_chunk(self, column, clustering_columns, chunk_sorting_column, dimension_cardinalities):\n",
    "        total_distinct_values = self.distinct_values[self.table_name][column]\n",
    "        \n",
    "        if column in clustering_columns:\n",
    "            index = clustering_columns.index(column)\n",
    "            clusters_for_column = dimension_cardinalities[index]\n",
    "            return min(self.target_chunksize, total_distinct_values / clusters_for_column)\n",
    "        else:\n",
    "            # TODO cluster wise sorting\n",
    "            return min(self.target_chunksize, total_distinct_values)\n",
    "        \n",
    "    # This function should only be called for the own table, not for others\n",
    "    def estimate_chunk_count(self, query_hash, join_input_rows, clustering_columns, dimension_cardinalities):        \n",
    "        # TODO include or exclude scans that do not benefit from pruning? Excluded for now        \n",
    "        table_scans = self.table_scans[self.table_scans['QUERY_HASH'] == query_hash]        \n",
    "        table_scans = table_scans[table_scans['useful_for_pruning']]\n",
    "        \n",
    "        if len(table_scans) > 0:\n",
    "            #print(f\"For query hash {query_hash}, there are {len(table_scans)} scans on {self.table_name}\")\n",
    "            def get_denseness_factor(row):\n",
    "                column = row['COLUMN_NAME']\n",
    "                if column in clustering_columns:\n",
    "                    # TODO more precise estimate\n",
    "                    denseness_factor = 1\n",
    "                else:\n",
    "                    denseness_factor = row['selectivity']\n",
    "\n",
    "                return denseness_factor\n",
    "\n",
    "            denseness_factors = table_scans.apply(get_denseness_factor, axis=1)\n",
    "            denseness_factor = denseness_factors.product()\n",
    "            #print(f\"denseness factor is {denseness_factor}\")\n",
    "        else:\n",
    "            denseness_factor = 1        \n",
    "        \n",
    "        chunk_count = math.ceil(join_input_rows / (self.target_chunksize * denseness_factor))\n",
    "        max_chunks = math.ceil(self.table_size / self.target_chunksize)\n",
    "        #if chunk_count > max_chunks:\n",
    "        #    print(f\"WARNING: estimated {chunk_count} chunks, but {self.table_name} got only {max_chunks}\\nDenseness: {denseness_factor}\")\n",
    "        \n",
    "        \n",
    "        return min(chunk_count, max_chunks)\n",
    "    \n",
    "    def statistic_time_dimension_cardinalities(self):\n",
    "        return math.ceil(self.table_size / self.target_chunksize)\n",
    "            \n",
    "    def get_dimension_cardinalities(self, clustering_columns):\n",
    "        # ToDo what if we aim at less than number of chunks clusters, i.e. multiple chunks per cluster?\n",
    "        target_cluster_count = math.ceil(1.1 * self.table_size / self.target_chunksize)\n",
    "        # idea: fixed size for join columns, variable amount for scan columns\n",
    "        \n",
    "        join_columns = list(filter(lambda x: self.is_join_column(x), clustering_columns))\n",
    "        scan_columns = list(filter(lambda x: self.is_scan_column(x), clustering_columns))\n",
    "        intersecting_columns = set(join_columns).intersection(set(scan_columns))\n",
    "        assert len(intersecting_columns) == 0, f\"The following columns are used as both join and scan column: {intersecting_columns}\"\n",
    "        \n",
    "        if len(scan_columns) == 0:\n",
    "            CLUSTERS_PER_JOIN_COLUMN = math.ceil(math.pow(target_cluster_count, 1/len(join_columns)))\n",
    "        else: \n",
    "            CLUSTERS_PER_JOIN_COLUMN = 3;\n",
    "        # Assumption: uniform distribution (in the sense that every cluster actually exists)\n",
    "        num_join_clusters = math.pow(CLUSTERS_PER_JOIN_COLUMN, len(join_columns))\n",
    "        assert num_join_clusters <= 2 * target_cluster_count, f\"Would get {num_join_clusters} clusters for join columns, but aimed at at most {target_cluster_count} clusters\"\n",
    "    \n",
    "        # only applies to scan columns\n",
    "        desired_scan_clusters_count = math.ceil(target_cluster_count / num_join_clusters)\n",
    "        individual_distinct_values = [self.distinct_values[self.table_name][column] for column in scan_columns]\n",
    "        log_distinct_values = [math.ceil(0.5+np.log2(x)) for x in individual_distinct_values]\n",
    "        log_distinct_values_product = reduce(operator.mul, log_distinct_values, 1)\n",
    "        assert log_distinct_values_product > 0, \"cannot have a distinct value count of 0\"\n",
    "\n",
    "        global_modification_factor = desired_scan_clusters_count / log_distinct_values_product\n",
    "        num_scan_dimensions = len(scan_columns)\n",
    "        individual_modification_factor = np.power(global_modification_factor, 1.0 / max(1, num_scan_dimensions))\n",
    "        \n",
    "        join_column_cluster_counts = [CLUSTERS_PER_JOIN_COLUMN] * len(join_columns)\n",
    "        scan_column_cluster_counts = [math.ceil(x * individual_modification_factor) for x in log_distinct_values]\n",
    "        \n",
    "        \n",
    "        # Merge join and scan columns\n",
    "        join_index = 0\n",
    "        scan_index = 0\n",
    "        cluster_counts = []\n",
    "        for clustering_column in clustering_columns:\n",
    "            if clustering_column in join_columns:\n",
    "                cluster_counts.append(join_column_cluster_counts[join_index])\n",
    "                join_index += 1\n",
    "            elif clustering_column in scan_columns:\n",
    "                cluster_counts.append(scan_column_cluster_counts[scan_index])\n",
    "                scan_index += 1\n",
    "        assert join_index == len(join_columns), f\"Processed the wrong number of join columns: {join_index} instead of {len(join_column_cluster_counts)}\"\n",
    "        assert scan_index == len(scan_columns), f\"Processed the wrong number of scan columns: {scan_index} instead of {len(scan_column_cluster_counts)}\"\n",
    "        assert len(cluster_counts) == len(clustering_columns), f\"Expected {len(clustering_columns)} cluster counts, but got {len(cluster_counts)}\"\n",
    "        \n",
    "        # testing\n",
    "        actual_cluster_count = reduce(operator.mul, cluster_counts, 1)\n",
    "        assert actual_cluster_count > 0, \"there was a split up factor of 0\"\n",
    "        assert actual_cluster_count <= 2 * target_cluster_count, f\"Wanted at most {target_cluster_count} clusters, but got {actual_cluster_count}\\nConfig: {clustering_columns}\\nCluster sizes: {cluster_counts}\"\n",
    "        estimated_chunksize = self.table_size / actual_cluster_count\n",
    "\n",
    "        assert estimated_chunksize <= self.target_chunksize, f\"chunks should be smaller, not larger than target_chunksize. Estimated chunk size is {estimated_chunksize}\"\n",
    "        allowed_percentage = 0.55\n",
    "        if estimated_chunksize < allowed_percentage * self.target_chunksize:\n",
    "            print(f\"Warning: chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\")\n",
    "        #assert estimated_chunksize >= allowed_percentage * self.target_chunksize, f\"chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\"\n",
    "        \n",
    "        return cluster_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESCRIPTION1</th>\n",
       "      <th>RUNTIME_BASE</th>\n",
       "      <th>RUNTIME_ESTIMATE</th>\n",
       "      <th>RUNTIME_CLUSTERED</th>\n",
       "      <th>TOTAL_ERROR</th>\n",
       "      <th>RELATIVE_ERROR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12181169</td>\n",
       "      <td>450259</td>\n",
       "      <td>488082</td>\n",
       "      <td>37823</td>\n",
       "      <td>1.084003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36181844</td>\n",
       "      <td>17999629</td>\n",
       "      <td>19569801</td>\n",
       "      <td>1570172</td>\n",
       "      <td>1.087234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10313214</td>\n",
       "      <td>129592</td>\n",
       "      <td>173497</td>\n",
       "      <td>43905</td>\n",
       "      <td>1.338794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11927000</td>\n",
       "      <td>428616</td>\n",
       "      <td>614364</td>\n",
       "      <td>185748</td>\n",
       "      <td>1.433367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>52224714</td>\n",
       "      <td>26029901</td>\n",
       "      <td>28978023</td>\n",
       "      <td>2948122</td>\n",
       "      <td>1.113259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>23031426</td>\n",
       "      <td>3042700</td>\n",
       "      <td>10369509</td>\n",
       "      <td>7326809</td>\n",
       "      <td>3.407996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16812640</td>\n",
       "      <td>2211067</td>\n",
       "      <td>1154341</td>\n",
       "      <td>-1056726</td>\n",
       "      <td>0.522074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12127177</td>\n",
       "      <td>448340</td>\n",
       "      <td>592361</td>\n",
       "      <td>144021</td>\n",
       "      <td>1.321232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16936267</td>\n",
       "      <td>2240600</td>\n",
       "      <td>1000611</td>\n",
       "      <td>-1239989</td>\n",
       "      <td>0.446582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38590056</td>\n",
       "      <td>19054411</td>\n",
       "      <td>19413835</td>\n",
       "      <td>359424</td>\n",
       "      <td>1.018863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16781279</td>\n",
       "      <td>2208490</td>\n",
       "      <td>1039654</td>\n",
       "      <td>-1168836</td>\n",
       "      <td>0.470753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12282515</td>\n",
       "      <td>451729</td>\n",
       "      <td>596204</td>\n",
       "      <td>144475</td>\n",
       "      <td>1.319827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16465437</td>\n",
       "      <td>2175262</td>\n",
       "      <td>1437007</td>\n",
       "      <td>-738255</td>\n",
       "      <td>0.660613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10291142</td>\n",
       "      <td>130709</td>\n",
       "      <td>183834</td>\n",
       "      <td>53125</td>\n",
       "      <td>1.406437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>37270390</td>\n",
       "      <td>18422218</td>\n",
       "      <td>19517417</td>\n",
       "      <td>1095199</td>\n",
       "      <td>1.059450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36406119</td>\n",
       "      <td>18090557</td>\n",
       "      <td>19574200</td>\n",
       "      <td>1483643</td>\n",
       "      <td>1.082012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16506436</td>\n",
       "      <td>2170797</td>\n",
       "      <td>1172082</td>\n",
       "      <td>-998715</td>\n",
       "      <td>0.539932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16468301</td>\n",
       "      <td>2175640</td>\n",
       "      <td>940054</td>\n",
       "      <td>-1235586</td>\n",
       "      <td>0.432082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>37954389</td>\n",
       "      <td>18906924</td>\n",
       "      <td>19977839</td>\n",
       "      <td>1070915</td>\n",
       "      <td>1.056641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10741019</td>\n",
       "      <td>137678</td>\n",
       "      <td>181739</td>\n",
       "      <td>44061</td>\n",
       "      <td>1.320029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12186687</td>\n",
       "      <td>449473</td>\n",
       "      <td>504357</td>\n",
       "      <td>54884</td>\n",
       "      <td>1.122107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16496937</td>\n",
       "      <td>2171070</td>\n",
       "      <td>893537</td>\n",
       "      <td>-1277533</td>\n",
       "      <td>0.411565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38728839</td>\n",
       "      <td>19278939</td>\n",
       "      <td>19669680</td>\n",
       "      <td>390741</td>\n",
       "      <td>1.020268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16474447</td>\n",
       "      <td>2166590</td>\n",
       "      <td>1035463</td>\n",
       "      <td>-1131127</td>\n",
       "      <td>0.477923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12194928</td>\n",
       "      <td>438847</td>\n",
       "      <td>586145</td>\n",
       "      <td>147298</td>\n",
       "      <td>1.335648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11741001</td>\n",
       "      <td>148710</td>\n",
       "      <td>191378</td>\n",
       "      <td>42668</td>\n",
       "      <td>1.286921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11314243</td>\n",
       "      <td>140222</td>\n",
       "      <td>176361</td>\n",
       "      <td>36139</td>\n",
       "      <td>1.257727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11396940</td>\n",
       "      <td>144989</td>\n",
       "      <td>172589</td>\n",
       "      <td>27600</td>\n",
       "      <td>1.190359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38475578</td>\n",
       "      <td>19082424</td>\n",
       "      <td>19720877</td>\n",
       "      <td>638453</td>\n",
       "      <td>1.033458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12030092</td>\n",
       "      <td>432915</td>\n",
       "      <td>524541</td>\n",
       "      <td>91626</td>\n",
       "      <td>1.211649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10619697</td>\n",
       "      <td>135826</td>\n",
       "      <td>210096</td>\n",
       "      <td>74270</td>\n",
       "      <td>1.546803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10143907</td>\n",
       "      <td>128867</td>\n",
       "      <td>181808</td>\n",
       "      <td>52941</td>\n",
       "      <td>1.410819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16444344</td>\n",
       "      <td>2172475</td>\n",
       "      <td>779689</td>\n",
       "      <td>-1392786</td>\n",
       "      <td>0.358894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12043014</td>\n",
       "      <td>442764</td>\n",
       "      <td>570290</td>\n",
       "      <td>127526</td>\n",
       "      <td>1.288023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12035610</td>\n",
       "      <td>442259</td>\n",
       "      <td>462798</td>\n",
       "      <td>20539</td>\n",
       "      <td>1.046441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12004040</td>\n",
       "      <td>443712</td>\n",
       "      <td>483821</td>\n",
       "      <td>40109</td>\n",
       "      <td>1.090394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>34665004</td>\n",
       "      <td>17164950</td>\n",
       "      <td>19342453</td>\n",
       "      <td>2177503</td>\n",
       "      <td>1.126858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10396750</td>\n",
       "      <td>127575</td>\n",
       "      <td>205626</td>\n",
       "      <td>78051</td>\n",
       "      <td>1.611805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10351629</td>\n",
       "      <td>128114</td>\n",
       "      <td>318986</td>\n",
       "      <td>190872</td>\n",
       "      <td>2.489861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>35483176</td>\n",
       "      <td>17612914</td>\n",
       "      <td>19626935</td>\n",
       "      <td>2014021</td>\n",
       "      <td>1.114349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16425205</td>\n",
       "      <td>2161630</td>\n",
       "      <td>754823</td>\n",
       "      <td>-1406807</td>\n",
       "      <td>0.349192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DESCRIPTION1  RUNTIME_BASE  \\\n",
       "0   TableScan Impl: ColumnBetween l_shipdate BETWE...      12181169   \n",
       "1   TableScan Impl: ColumnVsValue l_shipdate <= '1...      36181844   \n",
       "2   TableScan Impl: ColumnBetween l_shipdate BETWE...      10313214   \n",
       "3   TableScan Impl: ColumnBetween l_shipdate BETWE...      11927000   \n",
       "4   TableScan Impl: ColumnVsValue l_shipdate <= '1...      52224714   \n",
       "5   TableScan Impl: ColumnBetween l_shipdate BETWE...      23031426   \n",
       "10  TableScan Impl: ColumnBetween l_shipdate BETWE...      16812640   \n",
       "12  TableScan Impl: ColumnBetween l_shipdate BETWE...      12127177   \n",
       "14  TableScan Impl: ColumnBetween l_shipdate BETWE...      16936267   \n",
       "16  TableScan Impl: ColumnVsValue l_shipdate <= '1...      38590056   \n",
       "18  TableScan Impl: ColumnBetween l_shipdate BETWE...      16781279   \n",
       "23  TableScan Impl: ColumnBetween l_shipdate BETWE...      12282515   \n",
       "25  TableScan Impl: ColumnBetween l_shipdate BETWE...      16465437   \n",
       "27  TableScan Impl: ColumnBetween l_shipdate BETWE...      10291142   \n",
       "28  TableScan Impl: ColumnVsValue l_shipdate <= '1...      37270390   \n",
       "32  TableScan Impl: ColumnVsValue l_shipdate <= '1...      36406119   \n",
       "34  TableScan Impl: ColumnBetween l_shipdate BETWE...      16506436   \n",
       "37  TableScan Impl: ColumnBetween l_shipdate BETWE...      16468301   \n",
       "39  TableScan Impl: ColumnVsValue l_shipdate <= '1...      37954389   \n",
       "40  TableScan Impl: ColumnBetween l_shipdate BETWE...      10741019   \n",
       "41  TableScan Impl: ColumnBetween l_shipdate BETWE...      12186687   \n",
       "43  TableScan Impl: ColumnBetween l_shipdate BETWE...      16496937   \n",
       "45  TableScan Impl: ColumnVsValue l_shipdate <= '1...      38728839   \n",
       "47  TableScan Impl: ColumnBetween l_shipdate BETWE...      16474447   \n",
       "52  TableScan Impl: ColumnBetween l_shipdate BETWE...      12194928   \n",
       "53  TableScan Impl: ColumnBetween l_shipdate BETWE...      11741001   \n",
       "54  TableScan Impl: ColumnBetween l_shipdate BETWE...      11314243   \n",
       "55  TableScan Impl: ColumnBetween l_shipdate BETWE...      11396940   \n",
       "56  TableScan Impl: ColumnVsValue l_shipdate <= '1...      38475578   \n",
       "57  TableScan Impl: ColumnBetween l_shipdate BETWE...      12030092   \n",
       "58  TableScan Impl: ColumnBetween l_shipdate BETWE...      10619697   \n",
       "59  TableScan Impl: ColumnBetween l_shipdate BETWE...      10143907   \n",
       "64  TableScan Impl: ColumnBetween l_shipdate BETWE...      16444344   \n",
       "66  TableScan Impl: ColumnBetween l_shipdate BETWE...      12043014   \n",
       "67  TableScan Impl: ColumnBetween l_shipdate BETWE...      12035610   \n",
       "68  TableScan Impl: ColumnBetween l_shipdate BETWE...      12004040   \n",
       "69  TableScan Impl: ColumnVsValue l_shipdate <= '1...      34665004   \n",
       "70  TableScan Impl: ColumnBetween l_shipdate BETWE...      10396750   \n",
       "71  TableScan Impl: ColumnBetween l_shipdate BETWE...      10351629   \n",
       "81  TableScan Impl: ColumnVsValue l_shipdate <= '1...      35483176   \n",
       "86  TableScan Impl: ColumnBetween l_shipdate BETWE...      16425205   \n",
       "\n",
       "    RUNTIME_ESTIMATE  RUNTIME_CLUSTERED  TOTAL_ERROR  RELATIVE_ERROR  \n",
       "0             450259             488082        37823        1.084003  \n",
       "1           17999629           19569801      1570172        1.087234  \n",
       "2             129592             173497        43905        1.338794  \n",
       "3             428616             614364       185748        1.433367  \n",
       "4           26029901           28978023      2948122        1.113259  \n",
       "5            3042700           10369509      7326809        3.407996  \n",
       "10           2211067            1154341     -1056726        0.522074  \n",
       "12            448340             592361       144021        1.321232  \n",
       "14           2240600            1000611     -1239989        0.446582  \n",
       "16          19054411           19413835       359424        1.018863  \n",
       "18           2208490            1039654     -1168836        0.470753  \n",
       "23            451729             596204       144475        1.319827  \n",
       "25           2175262            1437007      -738255        0.660613  \n",
       "27            130709             183834        53125        1.406437  \n",
       "28          18422218           19517417      1095199        1.059450  \n",
       "32          18090557           19574200      1483643        1.082012  \n",
       "34           2170797            1172082      -998715        0.539932  \n",
       "37           2175640             940054     -1235586        0.432082  \n",
       "39          18906924           19977839      1070915        1.056641  \n",
       "40            137678             181739        44061        1.320029  \n",
       "41            449473             504357        54884        1.122107  \n",
       "43           2171070             893537     -1277533        0.411565  \n",
       "45          19278939           19669680       390741        1.020268  \n",
       "47           2166590            1035463     -1131127        0.477923  \n",
       "52            438847             586145       147298        1.335648  \n",
       "53            148710             191378        42668        1.286921  \n",
       "54            140222             176361        36139        1.257727  \n",
       "55            144989             172589        27600        1.190359  \n",
       "56          19082424           19720877       638453        1.033458  \n",
       "57            432915             524541        91626        1.211649  \n",
       "58            135826             210096        74270        1.546803  \n",
       "59            128867             181808        52941        1.410819  \n",
       "64           2172475             779689     -1392786        0.358894  \n",
       "66            442764             570290       127526        1.288023  \n",
       "67            442259             462798        20539        1.046441  \n",
       "68            443712             483821        40109        1.090394  \n",
       "69          17164950           19342453      2177503        1.126858  \n",
       "70            127575             205626        78051        1.611805  \n",
       "71            128114             318986       190872        2.489861  \n",
       "81          17612914           19626935      2014021        1.114349  \n",
       "86           2161630             754823     -1406807        0.349192  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_scans(table_name):\n",
    "    m = create_model(table_name, 2)\n",
    "    m.estimate_total_runtime([\"l_shipdate\"], \"l_shipdate\", [100])\n",
    "    assert len(m.scan_estimates[m.scan_estimates['RUNTIME_ESTIMATE'] < 0]) == 0, \"not all runtimes computed\"\n",
    "\n",
    "    CLUSTERED_STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/tpch_shipdate\"\n",
    "    path = f\"{CLUSTERED_STATISTICS_PATH}/table_scans.csv\"\n",
    "    clustered_scans = pd.read_csv(path, sep='|')\n",
    "    clustered_scans = clustered_scans[clustered_scans['TABLE_NAME'] == table_name]\n",
    "    clustered_scans = clustered_scans.sort_values(['QUERY_HASH', 'DESCRIPTION'])\n",
    "    \n",
    "    m.scan_estimates.sort_values(['QUERY_HASH', 'DESCRIPTION'], inplace=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['COLUMN_NAME'] = np.array(clustered_scans['COLUMN_NAME'])\n",
    "    result['DESCRIPTION1'] = np.array(m.scan_estimates['DESCRIPTION'])\n",
    "    result['DESCRIPTION2'] = np.array(clustered_scans['DESCRIPTION'])\n",
    "    result['QUERY_HASH1'] = np.array(m.scan_estimates['QUERY_HASH'])\n",
    "    result['QUERY_HASH2'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['RUNTIME_BASE'] = np.array(m.scan_estimates['RUNTIME_NS'])\n",
    "    result['RUNTIME_ESTIMATE'] = np.array(m.scan_estimates.apply(lambda row: row['RUNTIME_ESTIMATE'] / m.query_frequency(row['QUERY_HASH']), axis=1), dtype=np.int64)\n",
    "    result['RUNTIME_CLUSTERED'] = np.array(clustered_scans['RUNTIME_NS'])\n",
    "    \n",
    "    # make sure we match all operators\n",
    "    matches = result.apply(lambda row: row['DESCRIPTION1'] == row['DESCRIPTION2'] and row['QUERY_HASH1'] == row['QUERY_HASH2'], axis=1)\n",
    "    assert matches.all(), \"not all rows match\"\n",
    "    \n",
    "    result['TOTAL_ERROR'] = result['RUNTIME_CLUSTERED'] - result['RUNTIME_ESTIMATE']\n",
    "    result['RELATIVE_ERROR'] = result['RUNTIME_CLUSTERED'] / result['RUNTIME_ESTIMATE']\n",
    "    \n",
    "    return result\n",
    "    \n",
    "CLUSTERING_COLUMN = \"l_shipdate\"\n",
    "results = eval_scans(\"lineitem\")\n",
    "results = results[CLUSTERING_COLUMN == results['COLUMN_NAME']]\n",
    "#results = results[results['COLUMN_NAME'] != 'l_receiptdate']\n",
    "results[['DESCRIPTION1', 'RUNTIME_BASE', 'RUNTIME_ESTIMATE', 'RUNTIME_CLUSTERED', 'TOTAL_ERROR', 'RELATIVE_ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESCRIPTION1</th>\n",
       "      <th>RUNTIME_BASE</th>\n",
       "      <th>RUNTIME_ESTIMATE</th>\n",
       "      <th>RUNTIME_CLUSTERED</th>\n",
       "      <th>TOTAL_ERROR</th>\n",
       "      <th>RELATIVE_ERROR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12181169</td>\n",
       "      <td>450259</td>\n",
       "      <td>488082</td>\n",
       "      <td>37823</td>\n",
       "      <td>1.084003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36181844</td>\n",
       "      <td>17999629</td>\n",
       "      <td>19569801</td>\n",
       "      <td>1570172</td>\n",
       "      <td>1.087234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10313214</td>\n",
       "      <td>129592</td>\n",
       "      <td>173497</td>\n",
       "      <td>43905</td>\n",
       "      <td>1.338794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11927000</td>\n",
       "      <td>428616</td>\n",
       "      <td>614364</td>\n",
       "      <td>185748</td>\n",
       "      <td>1.433367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>52224714</td>\n",
       "      <td>26029901</td>\n",
       "      <td>28978023</td>\n",
       "      <td>2948122</td>\n",
       "      <td>1.113259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>23031426</td>\n",
       "      <td>3042700</td>\n",
       "      <td>10369509</td>\n",
       "      <td>7326809</td>\n",
       "      <td>3.407996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16812640</td>\n",
       "      <td>2211067</td>\n",
       "      <td>1154341</td>\n",
       "      <td>-1056726</td>\n",
       "      <td>0.522074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12127177</td>\n",
       "      <td>448340</td>\n",
       "      <td>592361</td>\n",
       "      <td>144021</td>\n",
       "      <td>1.321232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16936267</td>\n",
       "      <td>2240600</td>\n",
       "      <td>1000611</td>\n",
       "      <td>-1239989</td>\n",
       "      <td>0.446582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38590056</td>\n",
       "      <td>19054411</td>\n",
       "      <td>19413835</td>\n",
       "      <td>359424</td>\n",
       "      <td>1.018863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16781279</td>\n",
       "      <td>2208490</td>\n",
       "      <td>1039654</td>\n",
       "      <td>-1168836</td>\n",
       "      <td>0.470753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12282515</td>\n",
       "      <td>451729</td>\n",
       "      <td>596204</td>\n",
       "      <td>144475</td>\n",
       "      <td>1.319827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16465437</td>\n",
       "      <td>2175262</td>\n",
       "      <td>1437007</td>\n",
       "      <td>-738255</td>\n",
       "      <td>0.660613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10291142</td>\n",
       "      <td>130709</td>\n",
       "      <td>183834</td>\n",
       "      <td>53125</td>\n",
       "      <td>1.406437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>37270390</td>\n",
       "      <td>18422218</td>\n",
       "      <td>19517417</td>\n",
       "      <td>1095199</td>\n",
       "      <td>1.059450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>36406119</td>\n",
       "      <td>18090557</td>\n",
       "      <td>19574200</td>\n",
       "      <td>1483643</td>\n",
       "      <td>1.082012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16506436</td>\n",
       "      <td>2170797</td>\n",
       "      <td>1172082</td>\n",
       "      <td>-998715</td>\n",
       "      <td>0.539932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16468301</td>\n",
       "      <td>2175640</td>\n",
       "      <td>940054</td>\n",
       "      <td>-1235586</td>\n",
       "      <td>0.432082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>37954389</td>\n",
       "      <td>18906924</td>\n",
       "      <td>19977839</td>\n",
       "      <td>1070915</td>\n",
       "      <td>1.056641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10741019</td>\n",
       "      <td>137678</td>\n",
       "      <td>181739</td>\n",
       "      <td>44061</td>\n",
       "      <td>1.320029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12186687</td>\n",
       "      <td>449473</td>\n",
       "      <td>504357</td>\n",
       "      <td>54884</td>\n",
       "      <td>1.122107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16496937</td>\n",
       "      <td>2171070</td>\n",
       "      <td>893537</td>\n",
       "      <td>-1277533</td>\n",
       "      <td>0.411565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38728839</td>\n",
       "      <td>19278939</td>\n",
       "      <td>19669680</td>\n",
       "      <td>390741</td>\n",
       "      <td>1.020268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16474447</td>\n",
       "      <td>2166590</td>\n",
       "      <td>1035463</td>\n",
       "      <td>-1131127</td>\n",
       "      <td>0.477923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12194928</td>\n",
       "      <td>438847</td>\n",
       "      <td>586145</td>\n",
       "      <td>147298</td>\n",
       "      <td>1.335648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11741001</td>\n",
       "      <td>148710</td>\n",
       "      <td>191378</td>\n",
       "      <td>42668</td>\n",
       "      <td>1.286921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11314243</td>\n",
       "      <td>140222</td>\n",
       "      <td>176361</td>\n",
       "      <td>36139</td>\n",
       "      <td>1.257727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>11396940</td>\n",
       "      <td>144989</td>\n",
       "      <td>172589</td>\n",
       "      <td>27600</td>\n",
       "      <td>1.190359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>38475578</td>\n",
       "      <td>19082424</td>\n",
       "      <td>19720877</td>\n",
       "      <td>638453</td>\n",
       "      <td>1.033458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12030092</td>\n",
       "      <td>432915</td>\n",
       "      <td>524541</td>\n",
       "      <td>91626</td>\n",
       "      <td>1.211649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10619697</td>\n",
       "      <td>135826</td>\n",
       "      <td>210096</td>\n",
       "      <td>74270</td>\n",
       "      <td>1.546803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10143907</td>\n",
       "      <td>128867</td>\n",
       "      <td>181808</td>\n",
       "      <td>52941</td>\n",
       "      <td>1.410819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16444344</td>\n",
       "      <td>2172475</td>\n",
       "      <td>779689</td>\n",
       "      <td>-1392786</td>\n",
       "      <td>0.358894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12043014</td>\n",
       "      <td>442764</td>\n",
       "      <td>570290</td>\n",
       "      <td>127526</td>\n",
       "      <td>1.288023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12035610</td>\n",
       "      <td>442259</td>\n",
       "      <td>462798</td>\n",
       "      <td>20539</td>\n",
       "      <td>1.046441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>12004040</td>\n",
       "      <td>443712</td>\n",
       "      <td>483821</td>\n",
       "      <td>40109</td>\n",
       "      <td>1.090394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>34665004</td>\n",
       "      <td>17164950</td>\n",
       "      <td>19342453</td>\n",
       "      <td>2177503</td>\n",
       "      <td>1.126858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10396750</td>\n",
       "      <td>127575</td>\n",
       "      <td>205626</td>\n",
       "      <td>78051</td>\n",
       "      <td>1.611805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>10351629</td>\n",
       "      <td>128114</td>\n",
       "      <td>318986</td>\n",
       "      <td>190872</td>\n",
       "      <td>2.489861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>TableScan Impl: ColumnVsValue l_shipdate &lt;= '1...</td>\n",
       "      <td>35483176</td>\n",
       "      <td>17612914</td>\n",
       "      <td>19626935</td>\n",
       "      <td>2014021</td>\n",
       "      <td>1.114349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>TableScan Impl: ColumnBetween l_shipdate BETWE...</td>\n",
       "      <td>16425205</td>\n",
       "      <td>2161630</td>\n",
       "      <td>754823</td>\n",
       "      <td>-1406807</td>\n",
       "      <td>0.349192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DESCRIPTION1  RUNTIME_BASE  \\\n",
       "0   TableScan Impl: ColumnBetween l_shipdate BETWE...      12181169   \n",
       "1   TableScan Impl: ColumnVsValue l_shipdate <= '1...      36181844   \n",
       "2   TableScan Impl: ColumnBetween l_shipdate BETWE...      10313214   \n",
       "3   TableScan Impl: ColumnBetween l_shipdate BETWE...      11927000   \n",
       "4   TableScan Impl: ColumnVsValue l_shipdate <= '1...      52224714   \n",
       "5   TableScan Impl: ColumnBetween l_shipdate BETWE...      23031426   \n",
       "10  TableScan Impl: ColumnBetween l_shipdate BETWE...      16812640   \n",
       "12  TableScan Impl: ColumnBetween l_shipdate BETWE...      12127177   \n",
       "14  TableScan Impl: ColumnBetween l_shipdate BETWE...      16936267   \n",
       "16  TableScan Impl: ColumnVsValue l_shipdate <= '1...      38590056   \n",
       "18  TableScan Impl: ColumnBetween l_shipdate BETWE...      16781279   \n",
       "23  TableScan Impl: ColumnBetween l_shipdate BETWE...      12282515   \n",
       "25  TableScan Impl: ColumnBetween l_shipdate BETWE...      16465437   \n",
       "27  TableScan Impl: ColumnBetween l_shipdate BETWE...      10291142   \n",
       "28  TableScan Impl: ColumnVsValue l_shipdate <= '1...      37270390   \n",
       "32  TableScan Impl: ColumnVsValue l_shipdate <= '1...      36406119   \n",
       "34  TableScan Impl: ColumnBetween l_shipdate BETWE...      16506436   \n",
       "37  TableScan Impl: ColumnBetween l_shipdate BETWE...      16468301   \n",
       "39  TableScan Impl: ColumnVsValue l_shipdate <= '1...      37954389   \n",
       "40  TableScan Impl: ColumnBetween l_shipdate BETWE...      10741019   \n",
       "41  TableScan Impl: ColumnBetween l_shipdate BETWE...      12186687   \n",
       "43  TableScan Impl: ColumnBetween l_shipdate BETWE...      16496937   \n",
       "45  TableScan Impl: ColumnVsValue l_shipdate <= '1...      38728839   \n",
       "47  TableScan Impl: ColumnBetween l_shipdate BETWE...      16474447   \n",
       "52  TableScan Impl: ColumnBetween l_shipdate BETWE...      12194928   \n",
       "53  TableScan Impl: ColumnBetween l_shipdate BETWE...      11741001   \n",
       "54  TableScan Impl: ColumnBetween l_shipdate BETWE...      11314243   \n",
       "55  TableScan Impl: ColumnBetween l_shipdate BETWE...      11396940   \n",
       "56  TableScan Impl: ColumnVsValue l_shipdate <= '1...      38475578   \n",
       "57  TableScan Impl: ColumnBetween l_shipdate BETWE...      12030092   \n",
       "58  TableScan Impl: ColumnBetween l_shipdate BETWE...      10619697   \n",
       "59  TableScan Impl: ColumnBetween l_shipdate BETWE...      10143907   \n",
       "64  TableScan Impl: ColumnBetween l_shipdate BETWE...      16444344   \n",
       "66  TableScan Impl: ColumnBetween l_shipdate BETWE...      12043014   \n",
       "67  TableScan Impl: ColumnBetween l_shipdate BETWE...      12035610   \n",
       "68  TableScan Impl: ColumnBetween l_shipdate BETWE...      12004040   \n",
       "69  TableScan Impl: ColumnVsValue l_shipdate <= '1...      34665004   \n",
       "70  TableScan Impl: ColumnBetween l_shipdate BETWE...      10396750   \n",
       "71  TableScan Impl: ColumnBetween l_shipdate BETWE...      10351629   \n",
       "81  TableScan Impl: ColumnVsValue l_shipdate <= '1...      35483176   \n",
       "86  TableScan Impl: ColumnBetween l_shipdate BETWE...      16425205   \n",
       "\n",
       "    RUNTIME_ESTIMATE  RUNTIME_CLUSTERED  TOTAL_ERROR  RELATIVE_ERROR  \n",
       "0             450259             488082        37823        1.084003  \n",
       "1           17999629           19569801      1570172        1.087234  \n",
       "2             129592             173497        43905        1.338794  \n",
       "3             428616             614364       185748        1.433367  \n",
       "4           26029901           28978023      2948122        1.113259  \n",
       "5            3042700           10369509      7326809        3.407996  \n",
       "10           2211067            1154341     -1056726        0.522074  \n",
       "12            448340             592361       144021        1.321232  \n",
       "14           2240600            1000611     -1239989        0.446582  \n",
       "16          19054411           19413835       359424        1.018863  \n",
       "18           2208490            1039654     -1168836        0.470753  \n",
       "23            451729             596204       144475        1.319827  \n",
       "25           2175262            1437007      -738255        0.660613  \n",
       "27            130709             183834        53125        1.406437  \n",
       "28          18422218           19517417      1095199        1.059450  \n",
       "32          18090557           19574200      1483643        1.082012  \n",
       "34           2170797            1172082      -998715        0.539932  \n",
       "37           2175640             940054     -1235586        0.432082  \n",
       "39          18906924           19977839      1070915        1.056641  \n",
       "40            137678             181739        44061        1.320029  \n",
       "41            449473             504357        54884        1.122107  \n",
       "43           2171070             893537     -1277533        0.411565  \n",
       "45          19278939           19669680       390741        1.020268  \n",
       "47           2166590            1035463     -1131127        0.477923  \n",
       "52            438847             586145       147298        1.335648  \n",
       "53            148710             191378        42668        1.286921  \n",
       "54            140222             176361        36139        1.257727  \n",
       "55            144989             172589        27600        1.190359  \n",
       "56          19082424           19720877       638453        1.033458  \n",
       "57            432915             524541        91626        1.211649  \n",
       "58            135826             210096        74270        1.546803  \n",
       "59            128867             181808        52941        1.410819  \n",
       "64           2172475             779689     -1392786        0.358894  \n",
       "66            442764             570290       127526        1.288023  \n",
       "67            442259             462798        20539        1.046441  \n",
       "68            443712             483821        40109        1.090394  \n",
       "69          17164950           19342453      2177503        1.126858  \n",
       "70            127575             205626        78051        1.611805  \n",
       "71            128114             318986       190872        2.489861  \n",
       "81          17612914           19626935      2014021        1.114349  \n",
       "86           2161630             754823     -1406807        0.349192  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_joins(table_name):\n",
    "    m = create_model(table_name, 2)\n",
    "    m.estimate_total_runtime([\"l_shipdate\"], \"l_shipdate\", [100])\n",
    "    assert len(m.join_estimates[m.join_estimates['ESTIMATE_MATERIALIZE_BUILD'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    assert len(m.join_estimates[m.join_estimates['ESTIMATE_MATERIALIZE_PROBE'] < 0]) == 0, \"not all runtimes computed\"\n",
    "\n",
    "    CLUSTERED_STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/tpch_shipdate\"\n",
    "    path = f\"{CLUSTERED_STATISTICS_PATH}/joins.csv\"\n",
    "    clustered_joins = pd.read_csv(path, sep='|')\n",
    "    clustered_joins = clustered_joins[clustered_joins['TABLE_NAME'] == table_name]\n",
    "    clustered_joins = clustered_joins.sort_values(['QUERY_HASH', 'DESCRIPTION'])\n",
    "    \n",
    "    m.join_estimates.sort_values(['QUERY_HASH', 'DESCRIPTION'], inplace=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['COLUMN_NAME'] = np.array(clustered_scans['COLUMN_NAME'])\n",
    "    result['DESCRIPTION1'] = np.array(m.scan_estimates['DESCRIPTION'])\n",
    "    result['DESCRIPTION2'] = np.array(clustered_scans['DESCRIPTION'])\n",
    "    result['QUERY_HASH1'] = np.array(m.scan_estimates['QUERY_HASH'])\n",
    "    result['QUERY_HASH2'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['RUNTIME_BASE'] = np.array(m.scan_estimates['RUNTIME_NS'])\n",
    "    result['RUNTIME_ESTIMATE'] = np.array(m.scan_estimates.apply(lambda row: row['RUNTIME_ESTIMATE'] / m.query_frequency(row['QUERY_HASH']), axis=1), dtype=np.int64)\n",
    "    result['RUNTIME_CLUSTERED'] = np.array(clustered_scans['RUNTIME_NS'])\n",
    "    \n",
    "    # make sure we match all operators\n",
    "    matches = result.apply(lambda row: row['DESCRIPTION1'] == row['DESCRIPTION2'] and row['QUERY_HASH1'] == row['QUERY_HASH2'], axis=1)\n",
    "    assert matches.all(), \"not all rows match\"\n",
    "    \n",
    "    result['TOTAL_ERROR'] = result['RUNTIME_CLUSTERED'] - result['RUNTIME_ESTIMATE']\n",
    "    result['RELATIVE_ERROR'] = result['RUNTIME_CLUSTERED'] / result['RUNTIME_ESTIMATE']\n",
    "    \n",
    "    return result\n",
    "    \n",
    "CLUSTERING_COLUMN = \"l_shipdate\"\n",
    "results = eval_scans(\"lineitem\")\n",
    "results = results[CLUSTERING_COLUMN == results['COLUMN_NAME']]\n",
    "#results = results[results['COLUMN_NAME'] != 'l_receiptdate']\n",
    "results[['DESCRIPTION1', 'RUNTIME_BASE', 'RUNTIME_ESTIMATE', 'RUNTIME_CLUSTERED', 'TOTAL_ERROR', 'RELATIVE_ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 2]],\n",
    "            'orders': [['o_orderdate', 2]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "def create_model(table_name, max_dimensions=2):    \n",
    "    query_frequencies = get_query_frequencies()\n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    start_time_table = datetime.now()\n",
    "    single_table_scans = extract_single_table(scans, table_name)\n",
    "\n",
    "    model = DisjointClustersModel(max_dimensions, query_frequencies, table_name, single_table_scans, table_sizes, distinct_values, CHUNK_SIZE, correlations.get(table_name, {}), joins, sorted_columns_during_creation)\n",
    "    return model\n",
    "\n",
    "model = create_model(\"lineitem\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nosort: 27738011420.957703\n",
      "best: 27738011420.957703\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model.suggest_clustering(20)\n",
    "#model.estimate_distinct_values_per_chunk_at_statistics_time(\"l_orderkey\", \"lineitem\")\n",
    "#print(model.estimate_distinct_values_per_chunk(\"l_orderkey\", [\"l_orderkey\", \"l_partkey\"], \"l_orderkey\", [20,5]))\n",
    "\n",
    "#model.estimate_chunk_count(\"9762c3a887e47469\", 77313, [\"l_orderkey\"], [92])\n",
    "#row = model.joins.iloc[140]\n",
    "#model.get_chunk_count_factor(row, \"BUILD\", [\"l_orderkey\", \"l_shipdate\"], [5, 20])\n",
    "\n",
    "if False or BENCHMARK == \"TPCH\":\n",
    "    time0 = model.estimate_total_runtime([\"l_orderkey\"], \"l_orderkey\", [100])\n",
    "    time1 = model.estimate_total_runtime([\"l_shipdate\", \"l_orderkey\"], \"l_orderkey\", [20, 5])\n",
    "    print(f\"nosort: {time0}\")\n",
    "    print(f\"best: {time1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l_receiptdate', 'l_shipdate', 'l_quantity', 'l_discount', 'l_partkey', 'l_suppkey', 'l_orderkey']\n",
      "Execution time: 0:06:36.114713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[('l_orderkey', 101)], 'l_orderkey', 26370884899.19325],\n",
       " [[('l_partkey', 101)], 'l_orderkey', 26532450915.027252],\n",
       " [[('l_orderkey', 101)], 'l_partkey', 26615562199.038082],\n",
       " [[('l_partkey', 11), ('l_suppkey', 11)], 'l_orderkey', 26699635114.05595],\n",
       " [[('l_partkey', 11), ('l_orderkey', 11)], 'l_orderkey', 26704042761.979164],\n",
       " [[('l_orderkey', 101)], 'l_shipdate', 26935577547.874763],\n",
       " [[('l_shipdate', 18), ('l_discount', 6)], 'l_orderkey', 27072550065.22197],\n",
       " [[('l_shipdate', 101)], 'l_orderkey', 27075918148.742348],\n",
       " [[('l_shipdate', 34), ('l_suppkey', 3)], 'l_orderkey', 27084854319.044548],\n",
       " [[('l_shipdate', 34), ('l_partkey', 3)], 'l_orderkey', 27088142476.69531],\n",
       " [[('l_shipdate', 34), ('l_orderkey', 3)], 'l_orderkey', 27088142476.69531],\n",
       " [[('l_shipdate', 14), ('l_quantity', 8)], 'l_orderkey', 27089905070.863934],\n",
       " [[('l_receiptdate', 11), ('l_shipdate', 11)],\n",
       "  'l_orderkey',\n",
       "  27091244594.047062],\n",
       " [[('l_receiptdate', 18), ('l_discount', 6)],\n",
       "  'l_orderkey',\n",
       "  27136974640.591656],\n",
       " [[('l_receiptdate', 101)], 'l_orderkey', 27145321783.803135],\n",
       " [[('l_receiptdate', 34), ('l_suppkey', 3)], 'l_orderkey', 27152423023.380554],\n",
       " [[('l_receiptdate', 14), ('l_quantity', 8)], 'l_orderkey', 27154567314.11499],\n",
       " [[('l_receiptdate', 34), ('l_partkey', 3)], 'l_orderkey', 27155711181.03132],\n",
       " [[('l_receiptdate', 34), ('l_orderkey', 3)], 'l_orderkey', 27155711181.03132],\n",
       " [[('l_orderkey', 101)], 'l_receiptdate', 27376265918.25877]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo delete\n",
    "start_time = datetime.now()\n",
    "clusterings = model.suggest_clustering(20)\n",
    "end_time = datetime.now()\n",
    "print(f\"Execution time: {end_time - start_time}\")\n",
    "clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering configs for all relevant tables\n",
    "# Currently deactivated, but working\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 2]],\n",
    "            'orders': [['o_orderdate', 2]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "def create_benchmark_configs():    \n",
    "    start_time = datetime.now()\n",
    "    clusterings = {\"default\" : default_benchmark_config()}\n",
    "    query_frequencies = get_query_frequencies()\n",
    "    \n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    print(table_names)\n",
    "    for table_name in table_names:\n",
    "        start_time_table = datetime.now()\n",
    "        single_table_scans = extract_single_table(scans, table_name)\n",
    "        table_size = table_sizes[table_name]\n",
    "        if table_size <= 3 * CHUNK_SIZE:\n",
    "            print(f\"Not computing clustering for {table_name}, as it has only {table_size} rows\")\n",
    "            continue\n",
    "\n",
    "        max_dimensions = 2\n",
    "        model = DisjointClustersModel(max_dimensions, query_frequencies, table_name, single_table_scans, table_sizes, distinct_values, CHUNK_SIZE, correlations.get(table_name, {}), joins, sorted_columns_during_creation)\n",
    "        table_clusterings = model.suggest_clustering(3)\n",
    "        for table_clustering in table_clusterings:\n",
    "            config = default_benchmark_config()\n",
    "            config[table_name] = format_table_clustering(table_clustering)\n",
    "            config_name = get_config_name(config)\n",
    "            clusterings[config_name] = config\n",
    "        end_time_table = datetime.now()\n",
    "        print(f\"Done computing clustering for {table_name} ({end_time_table - start_time_table})\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Computed all clusterings in {end_time - start_time}\")\n",
    "    \n",
    "    return clusterings\n",
    "\n",
    "create_benchmark_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdated code fragments (older model versions) are kept below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"This assertion failure only serves to stop execution here when clicking Cells->Run all. You can safely ignore it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, table_scans, correlations = {}):\n",
    "        super().__init__(table_scans, correlations)        \n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        pairs = itertools.product(interesting_columns, interesting_columns)                \n",
    "        total_runtimes = [self.estimate_total_runtime(self.table_scans, clustering_columns) for clustering_columns in pairs]\n",
    "        total_runtimes.sort(key=lambda x: x[1], reverse=False)\n",
    "        \n",
    "        return total_runtimes[0:first_k]\n",
    "        \n",
    "    \n",
    "    def estimate_total_runtime(self, single_table, clustering_columns):\n",
    "        total_runtime = 0\n",
    "        \n",
    "        pruning_col = clustering_columns[0]\n",
    "        sorted_col = clustering_columns[1]\n",
    "        def compute_runtime(row):\n",
    "            col_name = row['COLUMN_NAME']\n",
    "            if pruning_col == sorted_col:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_log_runtime']\n",
    "                else:\n",
    "                    if col_name in self.correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "\n",
    "            else:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_runtime']\n",
    "                elif col_name == sorted_col:\n",
    "                    # TODO: should this be affected by correlation?\n",
    "                    # we will get less chunks, so a linear scan should be close to optimal_runtime,\n",
    "                    # but log time should beat it anyway\n",
    "                    return row['log_runtime']\n",
    "                else:\n",
    "                    if col_name in self.correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "                    \n",
    "        effective_runtime = single_table.apply(compute_runtime, axis=1)\n",
    "        return [clustering_columns, effective_runtime.sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store additional statistics\n",
    "# TODO keep?\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def round_up_to_chunksize(row):\n",
    "    if row['OUTPUT_ROW_COUNT'] % CHUNK_SIZE == 0:\n",
    "        return row['OUTPUT_ROW_COUNT']\n",
    "    else:\n",
    "        return row['OUTPUT_ROW_COUNT'] + (CHUNK_SIZE - (row['OUTPUT_ROW_COUNT'] % CHUNK_SIZE))\n",
    "\n",
    "scans['pruned_minimum_input_rows'] = scans.apply(round_up_to_chunksize, axis=1)\n",
    "\n",
    "scans['selectivity'] = scans['OUTPUT_ROW_COUNT'] / scans['INPUT_ROW_COUNT']\n",
    "scans['actual_selectivity'] = scans['SINGLE_OUTPUT_ROWS'] / scans['SINGLE_INPUT_ROWS']\n",
    "\n",
    "scans['time_per_ir'] = scans['RUNTIME_NS'] / scans['INPUT_ROW_COUNT']\n",
    "scans['time_per_or'] = scans['RUNTIME_NS'] / scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "# optimal runtime assuming perfect pruning, but not sortedness\n",
    "scans['optimal_runtime'] = scans['time_per_ir'] * scans['pruned_minimum_input_rows']\n",
    "scans['runtime_gain'] = scans['RUNTIME_NS'] - scans['optimal_runtime']\n",
    "\n",
    "\n",
    "# log runtime for sorted columns\n",
    "scans['log_runtime'] = np.log2(scans['RUNTIME_NS'])\n",
    "scans['optimal_log_runtime'] = np.log2(1+scans['optimal_runtime'])\n",
    "scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAIN_COLUMN = 'runtime_gain'\n",
    "\n",
    "scans_groupby_columnname = scans.groupby(['TABLE_NAME', 'COLUMN_NAME'])\n",
    "sum_of_gains = pd.DataFrame(scans_groupby_columnname[GAIN_COLUMN].sum())\n",
    "sum_of_gains.sort_values(by=['TABLE_NAME', GAIN_COLUMN], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "if BENCHMARK == \"TPCH\":\n",
    "    TABLE = \"lineitem\"\n",
    "else:    \n",
    "    TABLE = \"customer_demographics\"\n",
    "\n",
    "import itertools\n",
    "\n",
    "def extract_single_table(table_name):\n",
    "    return scans[scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def extract_interesting_columns(df):\n",
    "    return list(df['COLUMN_NAME'].unique())\n",
    "\n",
    "\n",
    "correlations = {\n",
    "    'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "    'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "    'l_commitdate': ['l_receiptdate', 'l_shipdate']\n",
    "}\n",
    "#correlations = {}\n",
    "def table_sorting_options(table_name):\n",
    "    single_table = extract_single_table(table_name)\n",
    "    interesting_cols = extract_interesting_columns(single_table)\n",
    "    pairs = itertools.product(interesting_cols, interesting_cols)\n",
    "    \n",
    "    total_times = []\n",
    "    for pair in pairs:\n",
    "        pruning_col = pair[0]\n",
    "        sorted_col = pair[1]\n",
    "\n",
    "        def compute_runtime(row):\n",
    "            col_name = row['COLUMN_NAME']\n",
    "            if pruning_col == sorted_col:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_log_runtime']\n",
    "                else:\n",
    "                    if col_name in correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "\n",
    "            else:\n",
    "                if col_name == pruning_col:\n",
    "                    return row['optimal_runtime']\n",
    "                elif col_name == sorted_col:\n",
    "                    # TODO: should this be affected by correlation?\n",
    "                    # we will get less chunks, so a linear scan should be close to optimal_runtime,\n",
    "                    # but log time should beat it anyway\n",
    "                    return row['log_runtime']\n",
    "                else:\n",
    "                    if col_name in correlations.get(pruning_col, []):\n",
    "                        # correlated to pruning column -> a lot of pruning, no sortedness\n",
    "                        # TODO: better measure correlation\n",
    "                        return 1.2 * row['optimal_runtime']\n",
    "                    else:\n",
    "                        return row['RUNTIME_NS']\n",
    "\n",
    "        effective_runtime = single_table.apply(compute_runtime, axis=1)\n",
    "        total_times.append([pair, effective_runtime.sum()])    \n",
    "    total_times = pd.DataFrame(total_times, columns=['columns', 'time'])    \n",
    "    return total_times\n",
    "\n",
    "options = table_sorting_options(TABLE)\n",
    "options.sort_values(by=['time'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = pd.read_csv(f\"{STATISTICS_PATH}/aggregates.csv\", sep=',')\n",
    "\n",
    "# it looks like column names are mixed up.\n",
    "# COLUMN_NAME -> actually GROUP_BY_COLUMN_COUNT\n",
    "# GROUP_BY_COLUMN_COUNT -> actually AGGREGATE_COLUMN_COUNT\n",
    "# AGGREGATE_COLUMN_COUNT -> actually COLUMN_NAME\n",
    "\n",
    "COL_NAME = 'AGGREGATE_COLUMN_COUNT'\n",
    "GROUPBY_COL = 'COLUMN_NAME'\n",
    "AGG_COL = 'GROUP_BY_COLUMN_COUNT'\n",
    "\n",
    "# All aggregates have to read the entire table, so we cannot skip chunks.\n",
    "# But getting all groups consecutive could provide a speedup\n",
    "# As a result, we care only about aggregates with group by columns\n",
    "\n",
    "interesting_aggregates = aggregates[aggregates[GROUPBY_COL] > 0]\n",
    "stats = interesting_aggregates.groupby(['TABLE_NAME', COL_NAME])\n",
    "out_columns = pd.DataFrame(stats['OUTPUT_ROW_COUNT'].max())\n",
    "out_columns.sort_values(by=['TABLE_NAME', 'OUTPUT_ROW_COUNT'], ascending=[True, False])\n",
    "aggregates[aggregates['COLUMN_TYPE'] == 'DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_time_per_column = scans.groupby(['COLUMN_NAME'])\n",
    "accumulated_scan_times = pd.DataFrame(scan_time_per_column['RUNTIME_NS'].sum())\n",
    "total_scan_runtime = accumulated_scan_times['RUNTIME_NS'].sum()\n",
    "assert total_scan_runtime == scans['RUNTIME_NS'].sum(), f\"{total_scan_runtime}, {scans['RUNTIME_NS'].sum()}\"\n",
    "print(f\"total scan runtime: {total_scan_runtime}\")\n",
    "\n",
    "scan_time_per_column_prunable = scans[scans['useful_for_pruning']].groupby(['COLUMN_NAME'])\n",
    "accumulated_prunable_scan_times = pd.DataFrame(scan_time_per_column_prunable['RUNTIME_NS'].sum())\n",
    "total_prunable_scan_runtime = accumulated_prunable_scan_times['RUNTIME_NS'].sum()\n",
    "print(f\"total prunable scan runtime: {total_prunable_scan_runtime}\")\n",
    "print(f\"{100*total_prunable_scan_runtime/total_scan_runtime}% of scan runtime amount to prunable scans\")\n",
    "\n",
    "accumulated_scan_times.sort_values(['RUNTIME_NS'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "joins = load_join_statistics()\n",
    "\n",
    "print(joins['PROBE_COLUMN'].unique())\n",
    "\n",
    "join_time_per_column = joins.groupby(['PROBE_COLUMN'])\n",
    "\n",
    "accumulated_join_times = pd.DataFrame(join_time_per_column['RUNTIME_NS'].sum())\n",
    "print(len(accumulated_join_times))\n",
    "total_join_runtime = accumulated_join_times['RUNTIME_NS'].sum()\n",
    "#assert total_join_runtime == joins['RUNTIME_NS'].sum(), f\"{total_join_runtime},{joins['RUNTIME_NS'].sum()}\"\n",
    "print(f\"total join runtime: {total_join_runtime}\")\n",
    "\n",
    "joins[joins.apply(lambda x : x['PROBE_COLUMN'] not in ['o_custkey' ,'n_nationkey' ,'s_nationkey' ,'l_suppkey', 's_suppkey',\n",
    " 'l_orderkey', 'o_orderkey', 'p_partkey' ,'l_partkey' ,'ps_suppkey',\n",
    " 'c_nationkey' ,'r_regionkey' ,'c_custkey' ,'ps_partkey'] ,axis=1)]\n",
    "\n",
    "print(f\"for {BENCHMARK}, joins take about {total_join_runtime / total_scan_runtime} times longer than table scans\")\n",
    "accumulated_join_times.sort_values(['RUNTIME_NS'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
